{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "## Table of contents\n1. [Load the data](#1.-Load-the-data) <br>\n    1.1 Load song table<br>\n    1.2 Load user table <br>\n    1.3 Load user play frequency table<br>\n2. [Spark performance basics](#2.-Spark-performance-basics)<br>\n3. [Explore the data with Spark APIs](#3.-Explore-the-data-with-Spark-APIs)<br>\n    3.1 [Clean song table](#3.1-Clean-song-table) <br>\n    3.2 [Clean play table and user table](#3.2-Clean-play-table-and-user-table) <br>\n4. [Visualize the data](#4.-Visualize-the-data)<br>\n5. [Build the recommender system](#5.-Build-the-recommender-system)<br>\n    5.1 Setup training and test set<br>\n    5.2 Train collaborative filtering<br>\n    5.3 Tune parameters <br>\n    5.4 Evaluate recommendation results<br>\n6. [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>\n    6.1 Item based recommender<br>\n    6.2 Integrate<br>\n    \n7. Summary and next steps", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, count, struct, row_number, when, isnan, log,lit\nfrom pyspark.sql.functions import round as cround\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 1
        }, 
        {
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 1. Load the data", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "\nspark = SparkSession.builder.getOrCreate()\nuser_table_raw = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'valid_user_highfreq.csv'))\n\nuser_table_raw.take(5)\n\nsong_table= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'song_table.csv'))\nprint(song_table.take(5))\n\nsong_freq = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'user_song_freq.csv'))\nprint(song_freq.take(5))\n\ndownload_table = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'clean_download.csv'))\nprint (download_table.take(5))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[Row(uid='751824', device='ar', song_id='6483029', freq='385'), Row(uid='168156920', device='ip', song_id='6792060', freq='5'), Row(uid='497685', device='ar', song_id='7207401', freq='26'), Row(uid='1062806', device='ar', song_id='6841262', freq='50'), Row(uid='168195436', device='ar', song_id='12808784', freq='22')]\n[Row(_c0='0', uid=None, device='ip', song_id='6945370.0', song_name=None, paid_flag=None), Row(_c0='1', uid='1685126.0', device='ar', song_id='170455.0', song_name='\u987a\u6d41\u3001\u9006\u6d41', paid_flag=None), Row(_c0='2', uid='736305.0', device='ar', song_id='23380344.0', song_name='\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)', paid_flag=None), Row(_c0='3', uid='168042561.0', device='ar', song_id='6292506.0', song_name='\u5e1d\u90fd', paid_flag=None), Row(_c0='4', uid='1749320.0', device='ar', song_id='21473237.0', song_name='\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1', paid_flag=None)]\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "source": "user_db = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', '3_1uid.csv'))\nprint (user_db.select('uid').distinct().count())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "264715\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 144
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2. Spark performance basics", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "print (sc.defaultParallelism)\nprint (sc.getConf().toDebugString())\nprint (\"Number of partitions for the song_freq DataFrame: \" + str(song_freq.rdd.getNumPartitions()))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "2\nhive.metastore.warehouse.dir=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/work/spark-warehouse\nspark.app.id=app-20171016230717-0048-126850fe-0ee5-441f-8108-3064dadf856e\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.host=10.143.133.18\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.driver.port=38597\nspark.eventLog.dir=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.id=driver\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.history.fs.logDirectory=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0019:7089\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v65/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.catalogImplementation=hive\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\n", 
                    "name": "stdout"
                }, 
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-3-a20ffd7eb996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of partitions for the song_freq DataFrame: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0m: name 'song_freq' is not defined"
                    ], 
                    "evalue": "name 'song_freq' is not defined", 
                    "ename": "NameError", 
                    "output_type": "error"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 3. Explore the data with Spark APIs", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "user_table_raw.show(truncate=False)\nprint (\"Number of users: \", user_table_raw.count())\nprint (\"Number of different users: \" + str(user_table_raw.select('uid').distinct().count()))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---+----------+---------+\n|_c0|Unnamed: 0|uid      |\n+---+----------+---------+\n|1  |1         |154563989|\n|2  |2         |154806874|\n|3  |3         |154777984|\n|4  |4         |154801899|\n|5  |5         |154522980|\n|6  |6         |154466362|\n|7  |7         |154467953|\n|8  |8         |158752252|\n|9  |9         |154559964|\n|10 |10        |154542883|\n|11 |11        |154828695|\n|12 |12        |154723056|\n|13 |13        |154751052|\n|14 |14        |154630129|\n|15 |15        |154684841|\n|16 |16        |154799108|\n|17 |17        |154786598|\n|18 |18        |154561771|\n|19 |19        |154508382|\n|20 |20        |154710857|\n+---+----------+---------+\nonly showing top 20 rows\n\nNumber of users:  264714\nNumber of different users: 264714\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 36
        }, 
        {
            "cell_type": "code", 
            "source": "song_table.show(truncate=False)\nprint (\"Number of songs: \", song_table.count())\nprint (\"Number of different songs: \" + str(song_table.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------+---------+-------------------------------+------+-----------+\n|song_id |song_type|song_name                      |singer|song_length|\n+--------+---------+-------------------------------+------+-----------+\n|602239  |null     |\u859b\u51ef\u742a                            |0     |null       |\n|160911  |null     |\u8521\u4f9d\u6797&\u5468\u6770\u4f26                        |0     |null       |\n|1033156 |null     |\u6c6a\u82cf\u6cf7                            |0     |null       |\n|294622  |null     |DJ\u821e\u66f2                           |0     |null       |\n|517174  |null     |\u68a6\u9e3d                             |0     |null       |\n|6606144 |null     |\u6768\u5c0f\u66fc&\u51b7\u6f20                         |0     |null       |\n|6432663 |null     |\u5c0f\u4e54                             |0     |null       |\n|6587633 |null     |\u97e9\u5b87                             |0     |null       |\n|6587662 |null     |\u97e9\u5b87                             |0     |null       |\n|158182  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|1037626 |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|995380  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|529630  |null     |\u8bd5\u97f3\u789f                            |0     |null       |\n|3006068 |null     |\u7f51\u7edc\u6b4c\u624b                           |0     |null       |\n|4111186 |null     |\u5e7d\u5e7d                             |0     |null       |\n|1488120 |null     |Declan Masterson               |0     |null       |\n|21461735|null     |Lil Uzi Vert&Quavo&Travis Scott|0     |null       |\n|22399766|null     |\u5f20\u6770                             |0     |null       |\n|60057   |null     |\u7518\u840d                             |0     |null       |\n|116273  |null     |null                           |0     |null       |\n+--------+---------+-------------------------------+------+-----------+\nonly showing top 20 rows\n\nNumber of songs:  5047315\nNumber of different songs: 1559990\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 21
        }, 
        {
            "cell_type": "code", 
            "source": "song_table.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- song_length: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 3.1 Clean song table \n- remove invalid song_id \n- get single entry for each song_id (most common song)\n- drop song_length column because 1) large variance 2) not very relevant", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# song_table.createOrReplaceTempView('song_table')\nsong_table.createOrReplaceTempView('song_table')\nsong_table_valid = spark.sql(\"select *  from song_table where song_id > 0 and song_id is not null\")\n\nprint (\"Number of songs: \", song_table_valid.count())\nprint (\"Number of different songs: \" + str(song_table_valid.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of songs:  3230980\nNumber of different songs: 1559987\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "source": "# get most common non-zero song_type\ntype_counts = song_table_valid.groupBy(['song_id', 'song_type'])\\\n    .count().alias('cnt')\\\n    .where(col('song_type') != '0')\n\nmax_type = (type_counts\n    .groupBy('song_id')\n    .agg(F.max(struct(col('count'), col('song_type'))).alias('max'))\n    .select(col('song_id'), col('max.song_type')))\n\n# get most common not null song_name \nname_counts = song_table_valid.groupBy(['song_id', 'song_name'])\\\n    .count().alias('cnt')\\\n    .where(col('song_name').isNotNull())\n    \nmax_name = (name_counts.groupBy('song_id')\n            .agg(F.max(struct(col('count'), col('song_name'))).alias('max'))\n            .select(col('song_id'),col('max.song_name')))\n\n\n# get most common not null singer \nsinger_counts = song_table_valid.groupBy(['song_id', 'singer']).count().alias('cnt').where(col('singer').isNotNull())\nw = Window().partitionBy('song_id').orderBy(col('count').desc())\nmax_singer = (singer_counts\n              .withColumn('rn', row_number().over(w))\n              .where(col('rn')==1)\n              .select('song_id', 'singer'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 64
        }, 
        {
            "cell_type": "markdown", 
            "source": "note: tried pandas - takes a long time for me; both Window or struct should work ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "print (max_type.select('song_id').distinct().count())\nprint (max_name.select('song_id').distinct().count())\nprint (max_singer.select('song_id').distinct().count())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "231216\n1559532\n1547516\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 67
        }, 
        {
            "cell_type": "code", 
            "source": "songs = song_table_valid.select('song_id').distinct().alias('songs')\nsong_unique = songs\\\n    .join(max_type, 'song_id','left')\\\n    .join(max_name, 'song_id','left')\\\n    .join(max_singer, 'song_id','left')\\\n    .select('song_id', 'song_type','song_name', 'singer')\\\n\nsong_unique = song_unique.na.fill('0', subset=['song_type'])", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 103
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.show()\nprint (song_unique.select('song_id').distinct().count())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------+---------+--------------------+----------------+\n| song_id|song_type|           song_name|          singer|\n+--------+---------+--------------------+----------------+\n|  100140|        0|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|\n|10015022|        0|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab|\n| 1003644|        0|Save The One, Sav...|  T.M.Revolution|\n| 1004266|        0|        Broken heart|             \u9ec4\u4e49\u8fbe|\n| 1006370|        0|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|\n| 1006422|        0|                  \u6d63\u7eb1|              \u6731\u6d01|\n|10065669|        1|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|\n|10071852|        0|Dirt Road Anthem ...|  Country Nation|\n|10087323|        0|Helden sterben ei...| Michael Wendler|\n| 1009129|        1|    Dietro L'Incanto|Ludovico Einaudi|\n|  100964|        1|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77|\n| 1010103|        0|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|\n|10101536|        0|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|\n|  101021|        0|           I Believe|             Era|\n|10102878|        0|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|\n|10107689|        0|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|\n|  101122|        0|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|\n|  101272|        0|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|\n|10139662|        1|                  \u843d\u82b1|             \u674e\u5c0f\u7490|\n|10141277|        0|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|\n+--------+---------+--------------------+----------------+\nonly showing top 20 rows\n\n1559987\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 22
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.groupBy('song_type').count().sort(col('count').desc()).show()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+-------+\n|song_type|  count|\n+---------+-------+\n|        0|1328771|\n|        1| 155797|\n|        2|  65042|\n|        3|  10053|\n|       73|     22|\n|       90|     18|\n|       91|     12|\n|       89|     12|\n|       48|      7|\n|       66|      6|\n|       88|      6|\n|       26|      6|\n|       30|      6|\n|       43|      6|\n|       60|      6|\n|       33|      5|\n|       27|      5|\n|       41|      5|\n|       82|      5|\n|       32|      4|\n+---------+-------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 108
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_song.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 121
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 3.2 Clean play table and user table\n- remove extremely high frequency (0.9999 quantile at approximately 1000 play frequency)\n- remove play history without uid \n\nNumber of valid users:  264708 <br>\nNumber of invalid users:  186", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq = song_freq.withColumn('freq', song_freq['freq'].cast('integer'))\nsong_freq = song_freq.filter((col('uid').isNotNull()) & (col('uid') > '0') &\n                             (col('song_id').isNotNull()) & (col('song_id')>'0'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.sort(col('freq').desc()).show()", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+------+--------+-----+\n|      uid|device| song_id| freq|\n+---------+------+--------+-----+\n|167982849|    ar| 4554016|73609|\n|  1791497|    ar| 3401476|51858|\n|   751824|    ar| 9950164|46970|\n|  1685126|    ar|15249349|41265|\n|  1685126|    ar| 9950164|39207|\n| 37025504|    ar| 9950164|35198|\n|  1791497|    ar|15198178|30975|\n| 37025504|    ar|15249349|29830|\n|  1791497|    ar| 6468891|26765|\n|  1791497|    ar|  442265|23907|\n|  1685126|    ar| 5237384|22949|\n|  1791497|    ar| 9950164|22849|\n| 22730453|    ar| 7005106|22294|\n| 22730453|    ar| 5965686|22289|\n|   497685|    ar| 9950164|21574|\n|  1062806|    ar| 9950164|20929|\n|  1791497|    ar| 5245130|19813|\n| 37025504|    ar| 5237384|19799|\n|  1685126|    ar| 6468891|19709|\n|  1791497|    ar|  125802|19697|\n+---------+------+--------+-----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "source": "# # super slow; may not be working \n# import pandas as pd\n# user_song_freq = song_freq.toPandas().head()\n# cutoff_freq = user_song_freq['freq'].quantile(0.999)\n# valid_freq = user_song_freq.loc[(user_song_freq['freq']<cutoff_freq) & (user_song_freq['song_id'] > 0)]\n# extreme_freq = user_song_freq.loc[(user_song_freq['freq']>=cutoff_freq)]", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "cutoff_freq = song_freq.approxQuantile('freq', [0.99], 0.005)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "cutoff_freq = 1000", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "source": "extreme_freq = song_freq.filter(col('freq') >= cutoff_freq)\n# get valid song played frequency\nvalid_freq = song_freq.filter(col('freq') < cutoff_freq)\n\n# get valid users \noutlier_user = extreme_freq.select('uid').distinct()\noutlier_user.createOrReplaceTempView('filter_view')\nvalid_user = user_table_raw.where('uid not in (select uid from filter_view)')", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "cell_type": "code", 
            "source": "print (\"Number of valid users: \", valid_user.select('uid').distinct().count())\nprint (\"Number of invalid users: \", outlier_user.select('uid').distinct().count())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of valid users:  264708\nNumber of invalid users:  186\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "source": "# song_length = song_table.groupBy(['song_id', 'length']).avg(col('song_length')).alias('avg_length').where((col('song_length').isNotNull()) & (col('song_length') > '200') & (col('song_length') <'720'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "##### Needs to clean song_table:\n1. remove duplicate entries; only for single ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "download_table.show(truncate=False)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---+-----------+------+----------+------------------------------+---------+\n|_c0|uid        |device|song_id   |song_name                     |paid_flag|\n+---+-----------+------+----------+------------------------------+---------+\n|0  |null       |ip    |6945370.0 |null                          |null     |\n|1  |1685126.0  |ar    |170455.0  |\u987a\u6d41\u3001\u9006\u6d41                         |null     |\n|2  |736305.0   |ar    |23380344.0|\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)                   |null     |\n|3  |168042561.0|ar    |6292506.0 |\u5e1d\u90fd                            |null     |\n|4  |1749320.0  |ar    |21473237.0|\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1                      |null     |\n|5  |155948236.0|ar    |93388.0   |\u4e9a\u62c9\u4f2f\u8df3\u821e\u5973\u90ce                       |null     |\n|6  |167794453.0|ar    |497722.0  |\u85d5\u65ad\u4e1d\u8fde                          |null     |\n|7  |168505311.0|ip    |4188142.0 |null                          |null     |\n|8  |168031064.0|ar    |4243838.0 |\u7231\u60c5\u7801\u5934(2651,cn \u5929\u5730\u4eba\u97f3\u4e50\u7f51)          |null     |\n|9  |167626177.0|ar    |1080516.0 |\u65e0\u6cd5\u539f\u8c05(\u7535\u89c6\u5267\u300a\u56de\u5bb6\u7684\u8bf1\u60d1\u300b\u4e3b\u9898\u66f2)           |null     |\n|10 |736305.0   |ar    |344885.0  |Because Of You                |null     |\n|11 |736305.0   |ar    |119699.0  |\u522b\u519b\u8425                           |null     |\n|12 |1685126.0  |ar    |3042675.0 |\u65f6\u5149\u673a\u524d\u594f-\u4e94\u6708\u5929_\u4e94\u6708\u5929(\u94c3\u58f0)             |null     |\n|13 |167691874.0|ar    |3287563.0 |\u5176\u5b9e\u90fd\u6ca1\u6709                         |null     |\n|14 |736305.0   |ar    |6792401.0 |Dream It Possible             |null     |\n|15 |167973828.0|ar    |10901925.0|Overtime (Vicetone Remix Edit)|null     |\n|16 |736305.0   |ar    |9925786.0 |Dream It Possible(34\u79d2\u94c3\u58f0\u7248)     |null     |\n|17 |167784467.0|ar    |8050943.0 |\u8036\u5229\u4e9a\u5973\u90ce                         |null     |\n|18 |168007277.0|ip    |4373829.0 |null                          |null     |\n|19 |167782564.0|ar    |325517.0  |\u68a6\u9a7c\u94c3                           |null     |\n+---+-----------+------+----------+------------------------------+---------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 18
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.show(truncate=False)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+------+--------+----+\n|uid      |device|song_id |freq|\n+---------+------+--------+----+\n|751824   |ar    |6483029 |385 |\n|168156920|ip    |6792060 |5   |\n|497685   |ar    |7207401 |26  |\n|1062806  |ar    |6841262 |50  |\n|168195436|ar    |12808784|22  |\n|1685126  |ar    |59582   |26  |\n|168286187|ar    |4188404 |2   |\n|37025504 |ar    |481552  |733 |\n|168478031|ar    |9822502 |4   |\n|168406030|ar    |909773  |7   |\n|168410987|ar    |5425869 |10  |\n|168511270|ar    |6817428 |68  |\n|168115240|ar    |23665227|2   |\n|168396372|ar    |4276822 |10  |\n|168417737|ar    |5383328 |19  |\n|1062806  |ar    |20870989|73  |\n|168373631|ar    |7202991 |60  |\n|168335848|ip    |4112638 |15  |\n|37025504 |ar    |1108956 |76  |\n|168453430|ar    |1705363 |1   |\n+---------+------+--------+----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 11
        }, 
        {
            "cell_type": "code", 
            "source": "download_table.select('uid').distinct().count()", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 143, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "242243"
                    }
                }
            ], 
            "execution_count": 143
        }, 
        {
            "cell_type": "code", 
            "source": "valid_user.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    \nvalid_freq.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_freq.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 4. Visualize the data\n###  TODO\nUsing Seaborn and matplotlib", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install seaborn", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): seaborn in /usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages\r\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 164
        }, 
        {
            "cell_type": "code", 
            "source": "import seaborn as sns\n%matplotlib inline", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "import pandas as pd\nfreqPandas = valid_freq.toPandas()\ndlPandas = download_table.toPandas()\nsns.lmplot(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "plot matrix for song played by user, with frequency as the hue ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "min(freqPandas['uid'])", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "sns.palplot(sns.diverging_palette(10, 133, sep=80, n=10))\n# user_id: max 100047599, min 99983627\n# song_id: max 2147483647, min 2\nsns.(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 5. Build the recommender system\n(Cited from IBM bluemix Data Science Experience (DSX) document) <br>\n\n\"\nCollaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who have similar preference on the same item will also have similar opinions on items that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users versus all songs. Most cells in the matrix will be empty. An empty cell means the user hasn't played the song yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for song factors and solving for user factors.\n\"\n\n\nChallenge in this recommender system: <br>\n1. Small number of play history that could be shown by sparsity of the utility matrix\n2. Limited features for songs - mixed language\n\nSolution: <br>\n1. Hybrid", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# restarting kernel\nreload = True", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "source": "# load saved files if starting from middle\nif reload: \n    song_unique = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_song.csv'))\n    song_unique.take(5)\n    \n    valid_freq = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_freq.csv'))\n    valid_freq.take(5)\n    \n    valid_user = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    valid_user.take(5)\n    \n    download_table = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'clean_download.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "cell_type": "code", 
            "source": "valid_freq = valid_freq.withColumn('uid', valid_freq['uid'].cast('integer'))\nvalid_freq = valid_freq.withColumn('song_id', valid_freq['song_id'].cast('integer'))\nvalid_freq = valid_freq.withColumn('freq', valid_freq['freq'].cast('double'))\n\nvalid_user = valid_user.withColumn('uid', valid_user['uid'].cast('integer'))\n\nsong_unique = song_unique.withColumn('song_type', song_unique['song_type'].cast('integer'))\nsong_unique = song_unique.withColumn('song_id', song_unique['song_id'].cast('integer'))\n\nvalid_download = download_table.withColumn('uid', download_table['uid'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\n\nvalid_freq = valid_freq.withColumn('label',log(10.0, valid_freq.freq))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "cell_type": "code", 
            "source": "valid_freq.printSchema()\nvalid_user.printSchema()\nsong_unique.printSchema()\nvalid_download.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n\n# check sparsity \ncounts_freq = valid_freq.count()\nprint ('Number of song frequency entried: ', counts_freq)\n# songs played/( songs x users)\npercentage = (counts_freq)*1.0/1559987/264708\nprint ('Percentage of song played: ', percentage)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of song frequency entried:  32801295\nPercentage of song played:  7.943336195316835e-05\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 18
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n\nmoreThanOnce = valid_freq.groupBy('song_id').count().alias('cnt').where(col('count') > 1).select('song_id').count()\nprint ('Number of songs played more than once: ', moreThanOnce)\n# significantly less, only recommend songs played more than once\n# utility matrix will be less sparse\n# number of song frequency entries will be reduce by 686993(only played once) -32114102, which is 2%", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of songs played more than once:  872994\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 28
        }, 
        {
            "cell_type": "code", 
            "source": "print ('Percentage of filled matrix: ', round((32801295-686993)*100.0/(872994*264708),4))\n# vs. ~0.0073% filled ", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Percentage of filled matrix:  0.0139\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 42
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.createOrReplaceTempView('song_unique')\nvalid_freq.createOrReplaceTempView('valid_freq')\nsong_freq = spark.sql(\"\"\"\n    select s.song_type, s.song_id, s.song_name, s.singer, COALESCE(f.cnt,0) as freq\n    from song_unique as s\n    left join\n        (select song_id, count(*) as cnt from valid_freq group by song_id) as f\n        on f.song_id = s.song_id\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- freq: long (nullable = false)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n\nprint (song_freq.select('song_id').count())\nprint (song_freq.select('song_id').where(col('freq')>1).count())\nsong_freq.show()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "1559987\n872994\n+---------+--------+--------------------+----------------+----+\n|song_type| song_id|           song_name|          singer|freq|\n+---------+--------+--------------------+----------------+----+\n|        0|  100140|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|   2|\n|        0|10015022|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab| 193|\n|        0| 1003644|Save The One, Sav...|  T.M.Revolution|   1|\n|        0| 1004266|        Broken heart|             \u9ec4\u4e49\u8fbe|   5|\n|        0| 1006370|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|  30|\n|        0| 1006422|                  \u6d63\u7eb1|              \u6731\u6d01|   5|\n|        1|10065669|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|  17|\n|        0|10071852|Dirt Road Anthem ...|  Country Nation|   1|\n|        0|10087323|Helden sterben ei...| Michael Wendler|   1|\n|        1| 1009129|    Dietro L'Incanto|Ludovico Einaudi|  16|\n|        1|  100964|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77| 110|\n|        0| 1010103|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|   1|\n|        0|10101536|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|   2|\n|        0|  101021|           I Believe|             Era|   2|\n|        0|10102878|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|  18|\n|        0|10107689|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|   3|\n|        0|  101122|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|  13|\n|        0|  101272|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|   2|\n|        1|10139662|                  \u843d\u82b1|             \u674e\u5c0f\u7490|  73|\n|        0|10141277|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|   4|\n+---------+--------+--------------------+----------------+----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.1 Setup training and test set\n\nsetup: 80% training and 20% test set ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# take log of the frequency and scale to -1 to 100 representing degree of preference \n\nvalid_freq2 = valid_freq.withColumn('label',cround(log(10.0, valid_freq.freq)/3.0*10,0))\nvalid_freq2 = valid_freq2.replace(0, -1, subset=['label'])\n# should be None \nvalid_freq2 = valid_freq2.na.fill(0, subset=['label'])\n\n# print(valid_freq2.groupBy().max('label').show())", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "cell_type": "code", 
            "source": "# combine with download table\n\nvalid_download = valid_download.withColumn('download', lit(1))\nvalid_download = valid_download.drop('device')\nvalid_download = valid_download.drop('song_name')\n\nvalid_score = valid_freq2.join(valid_download, ['uid','song_id'], 'left_outer')\nvalid_score = valid_score.na.fill(0, subset=['download'])\n\nvalid_score = valid_score.withColumn('label', when(valid_score.download==1, lit(10)).otherwise(valid_score.label))\n\nvalid_score = valid_score.replace(-0.5, -1, subset=['label'])", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "source": "(trainingFreq, testFreq) = valid_score.randomSplit([80.0, 20.0])\n\ntrainingFreq.printSchema()\n# utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns]).show()\ntestFreq.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.2 Setup collaborative filtering model\n\nAccoding to DSX document again:\n\"\nA NaN result is due to [SPARK-14489](https://issues.apache.org/jira/browse/SPARK-14489) and because the model can't predict values for users for which there's no data.\"", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "model = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\").fit(trainingFreq)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "cell_type": "code", 
            "source": "predictions = model.transform(testFreq)\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "The root mean squared error for our model is: 2.917168466920176\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.3 Tune parameters\n\nCross validation and grid search to tune for hyperparameters\n\nALS algorithm :\n```python\n    class pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )\n```\n\nThe ALS hyperparameters are:\n- `rank` = the number of latent factors in the model\n- `maxIter` = the maximum number of iterations \n- `regParam` = the regularization parameter", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "(trainingScore, validationScore) = valid_score.randomSplit([90.0, 10.0])\n\nals = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n\n# paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 3, 5, 7]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,2,3]).addGrid(als.regParam, [0.05, 0.1]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = crossval.fit(trainingScore)\npredictions = cvModel.transform(validationScore)\n\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.drop())))", 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-16-633e8eca8475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcrossval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossValidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparamGrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m                 \u001b[1;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3168.fit.\n: org.apache.spark.SparkException: Job 141 cancelled as part of cancellation of all jobs\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1382)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:694)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:464)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:363)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n"
                    ], 
                    "evalue": "An error occurred while calling o3168.fit.\n: org.apache.spark.SparkException: Job 141 cancelled as part of cancellation of all jobs\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1382)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:694)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:464)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:363)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                    "ename": "Py4JJavaError", 
                    "output_type": "error"
                }
            ], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "print ('Best rank is: ', cvModel.bestModel.rank)\nprint ('Best regularizer is: ', cvModel.bestModel.params)\n# evaluate with 0 for null prediction\n# print (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))\n# The root mean squared error for our model is: 3.1921429126212857", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Best rank is:  1\nBest regularizer is:  []\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 136
        }, 
        {
            "cell_type": "code", 
            "source": "valid_score.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 20
        }, 
        {
            "cell_type": "code", 
            "source": "predictions.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'predictions_CF_1015.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.4 Evaluate recommendation results\n\n- MSE is low -> recommender seems to be pretty good\n- However, looking at recommended songs for individual users, it seems to recommending weird songs\n- Therefore, need alternative for inactive users for a more explainable model\n\nDetailed evaluation as following:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "utility_matrix_small = valid_user.crossJoin(song_freq.select('song_id')).select('uid','song_id')\nutility_matrix_small = utility_matrix_small.join(valid_score, ['uid', 'song_id'], 'left_outer').select('uid', 'song_id', 'label')\n\nutility_matrix = utility_matrix_small.na.fill('0', subset=['label'])\n\n# Replace predicted NaN values with the average frequency and evaluate the model\n# avgScore = utility_matrix.select('label').groupBy().avg().first()[0]\n# print (\"The average score in the dataset is: \" + str(avgScore))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 27
        }, 
        {
            "cell_type": "code", 
            "source": "predictions.createOrReplaceTempView('pred_subset')\npred_first20 = spark.sql('select uid, song_id, label,download, prediction from pred_subset order by uid limit 100').show()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+-----+--------+-----+--------+-----------+\n|  uid| song_id|label|download| prediction|\n+-----+--------+-----+--------+-----------+\n|12333|21596231|  1.0|       0|  2.4781432|\n|12333|   55219|  3.0|       0|  1.4091263|\n|12333|  708667|  4.0|       0|  3.9749548|\n|12333| 2725093|  3.0|       0|  4.3128133|\n|12333| 5114569|  6.0|       0|   4.895308|\n|36816| 6906526| -1.0|       0|   4.813818|\n|60183| 3627946| -1.0|       0|-0.13519855|\n|60183|  116329| -1.0|       0| -0.2599472|\n|60183|23610522| -1.0|       0| -0.7154152|\n|60183|20866010|  2.0|       0| 0.11216657|\n|60183| 1242385|  1.0|       0|-0.47428238|\n|60183|  134967|  1.0|       0| 0.13887726|\n|60183|21234365| -1.0|       0|-0.37378824|\n|60183| 6227103| -1.0|       0| 0.34997967|\n|60183|  710442| -1.0|       0|-0.09907829|\n|60183| 6128890| -1.0|       0|-0.11929984|\n|60183|  223973|  1.0|       0| 0.30912238|\n|60183|  981246| -1.0|       0| -0.6802237|\n|60183|23641903| -1.0|       0| 0.11194147|\n|60183|  706554| -1.0|       0| 0.29457986|\n+-----+--------+-----+--------+-----------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 11
        }, 
        {
            "cell_type": "code", 
            "source": "def recommendSongsAlt(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "cell_type": "code", 
            "source": "print (\"Recommendations for user 169262317:\")\nrecommendSongsAlt(model, 169262317, 10)\n# print \"Recommendations for user 471:\"\n# recommendSongs(cvModel, 471, 10)\n# print \"Recommendations for user 496:\"\n# recommendSongs(cvModel, 496, 10)\n\nprint (\"Recommendations for user 169262317:\")\nrecommendSongsAlt(model, 169262317, 10)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Recommendations for user 169262317:\n+--------+-----------------------------+-------+----------+\n|song_id |song_name                    |singer |prediction|\n+--------+-----------------------------+-------+----------+\n|22857125|\u6211\u8981\u4f60\u7684\u7231                        |\u845b\u5170     |7.101551  |\n|4631704 |\u524d\u4e16\u4eca\u751f \u94a2\u7434\u7248                     |\u7f51\u7edc\u6b4c\u624b   |6.32294   |\n|5319753 |\u6d2a\u6e56\u6c34\u6d6a\u6253\u6d6a\u00b7\u5c0f\u66f2\u597d\u5531\u53e3\u96be\u5f00               |\u7d22\u5b9d\u8389    |6.194232  |\n|12153162|\u90a3\u4e9b\u52a0\u4e86\u7279\u6280\u7684\u8f7b\u97f3\u4e50(\u8fdd\u548c\u611f\u541b\u5df2\u4e0b\u7ebf)          |\u5de5\u4f5c\u5b66\u4e60\u542c\u4ec0\u4e48|6.1369357 |\n|651007  |\u5988\u5988\u7684\u5a1c\u9c81\u5a03(\u539f\u4f4f\u6c11\u7248)                 |\u5df4\u7b71\u827e    |6.111135  |\n|5911939 |Main Title(28\u79d2\u94c3\u58f0\u7248)           |\u7535\u89c6\u539f\u58f0   |5.7174525 |\n|3460032 |\u9093\u4e3d\u541b-\u6708\u4eae\u4ee3\u8868\u6211\u7684\u5fc3(2009\u5408\u6210Club Wendy)|\u4e2d\u6587\u6162\u6447   |5.612504  |\n|3473601 |The Space Track              |Bobina |5.607437  |\n|6095814 |\u8f6e\u56de\u4e4b\u5883(27\u79d2\u94c3\u58f0\u7248)                 |Critty |5.585169  |\n|4805192 |\u5fd8\u60c5\u6851\u5df4\u821e                        |\u8349\u8722     |5.555094  |\n+--------+-----------------------------+-------+----------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 15
        }, 
        {
            "cell_type": "code", 
            "source": "# total user:264708 \n# select 12333\nselect_valid = valid_score.createOrReplaceTempView('select_valid')\nselected = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_valid\n    where uid = 12333\n    group by uid\n    order by 2 asc \n\"\"\")\n# inactive_user.count()\n# treshold at 1: 68268 -> around a quarter \n# treshold at 5: 154568 -> more than half \n# treshold at 2: 98849 -> around one third", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 40, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "98849"
                    }
                }
            ], 
            "execution_count": 40
        }, 
        {
            "cell_type": "code", 
            "source": "selected.show()\n\nprint (\"Recommendations for user 12333:\")\nrecommendSongsAlt(model, 12333, 10)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "inactive_user.show()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+---+\n|      uid|cnt|\n+---------+---+\n|168984483|  1|\n|168275548|  1|\n|168572441|  1|\n|168532375|  1|\n|168510789|  1|\n|168713236|  1|\n|168686527|  1|\n|168683346|  1|\n|167801928|  1|\n|168173268|  1|\n|168889990|  1|\n|168779601|  1|\n|169042674|  1|\n|168266657|  1|\n|167640755|  1|\n|168029402|  1|\n|168767795|  1|\n|167579971|  1|\n|168902557|  1|\n|168975045|  1|\n+---------+---+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 37
        }, 
        {
            "cell_type": "code", 
            "source": "print (\"Recommendations for user 167579971:\")\nrecommendSongsAlt(model, 167579971, 10)\n\nprint (\"Recommendations for user 168975045:\")\nrecommendSongsAlt(model, 168975045, 10)\n\n# 169042674 0\n# 167579971 10\n# 169042674 0\n# 167640755 10\n\n\n# valid_score.select().where(col('uid')==169042674).show()\n# problem is it's not in the user list", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Recommendations for user 167579971:\n+--------+-----------------+----------------+----------+\n|song_id |song_name        |singer          |prediction|\n+--------+-----------------+----------------+----------+\n|11109921|\u514b\u535c\u52d2 (Cover)      |\u90d1\u5c0f\u5b87             |3.387147  |\n|5009810 |\u8302\u540d\u8fd9\u573a\u96e8            |\u79e6\u9f50              |3.3330908 |\n|22399046|\u9f13\u4e0e\u82b1              |\u8427\u5fc6\u60c5Alex         |3.2971313 |\n|7190353 |\u5ff5\u5ff5\u4e0d\u5fd8\u7684\u59d1\u5a18(1\u520602\u79d2\u94c3\u58f0\u7248)|\u963f\u6743              |3.035895  |\n|413840  |Ich verzeih' Dir |Veronika Fischer|2.9503236 |\n|6110029 |\u660e\u5929(37\u79d2\u94c3\u58f0\u7248)       |\u8427\u4e9a\u8f69             |2.6847723 |\n|22827582|Come Together    |Michael Jackson |2.626977  |\n|2697950 |\u5728\u96e8\u4e2d\u6f2b\u6b65            |\u4ed8\u5a1c              |2.5599701 |\n|366974  |\u96ea\u67d3\u7684\u98ce\u91c7            |\u9648\u8bfa              |2.257006  |\n|6922682 |\u8bf7\u4f60\u50cf\u6211\u8fd9\u6837\u505a-\u8ddf\u6211\u6765-(\u7ea2\u679c\u679c)|\u513f\u7ae5\u6545\u4e8b            |2.2517805 |\n+--------+-----------------+----------------+----------+\n\nRecommendations for user 168975045:\n+--------+-------------------------+----------------+----------+\n|song_id |song_name                |singer          |prediction|\n+--------+-------------------------+----------------+----------+\n|81330   |\u5411\u65e5\u8475\u7684\u82b1\u5b63                   |\u4fdd\u5251\u950b             |12.676041 |\n|1240655 |Go In, Go Hard           |Wretch 32       |10.050616 |\n|1240642 |Bright Lights            |\u7535\u5f71\u539f\u58f0            |10.050616 |\n|15145086|\u5fae\u7b11\u7684\u5411\u65e5\u8475(\u4f34\u594f\u7248)              |BEJ48           |9.34867   |\n|13165006|??? ???                  |\u88f4\u79c0\u667a             |9.190221  |\n|13967998|And...???                |\u767d\u667a\u82f1             |9.190221  |\n|859831  |free storm               |\u88f4\u6da9\u742a             |9.002642  |\n|413840  |Ich verzeih' Dir         |Veronika Fischer|8.934969  |\n|6199318 |\u8d24\u826f(39\u79d2\u94c3\u58f0\u7248)               |\u82cf\u9633              |8.810483  |\n|5725764 |\u571f\u8c6a\u624b\u673a\u94c3\u58f0 \u5de7\u5999\u5236\u4f5c Www.Y2002.Com|\u7f51\u7edc\u6b4c\u624b            |8.637566  |\n+--------+-------------------------+----------------+----------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 39
        }, 
        {
            "cell_type": "code", 
            "source": "dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n# Create a Spark DataFrame with the movies that have already been rated by this user\nsongsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n# Apply the recommender system to the data set without the already rated movies to predict ratings\npredictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n# Join with the movies DataFrame to get the movies titles and genres\nrecommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction)\n\nrecommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 6. Hybrid recommender system\n\nsupplemnt the collaborative filtering with item-item recommender <br>\nfor user with only 1 song played history (or two):<br>\n- recommend songs based on cosine similarity to the song/songs played by the user \n- (not implemented) for user with 0 songs player: recommend top k songs  ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "from pyspark.mllib.linalg import Vectors\n# Package for distributed linear algebra DIMSUM\n# Dimension Independent Matrix Square using MapReduce\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.types \\\nimport ArrayType, StringType, DoubleType, StructType, StructField\n\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.mllib.linalg.distributed \\\nimport IndexedRowMatrix,IndexedRow, RowMatrix, BlockMatrix,CoordinateMatrix\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 13
        }, 
        {
            "cell_type": "code", 
            "source": "# convert to Dense vector in dataframe \n# song_unique.createOrReplaceTempView('song20')\nsong_unique = song_unique.na.fill('0', subset=['singer','song_name'])\n\nsong_str2arr = song_unique.withColumn('singer_arr', split(col(\"singer\"), \" \").cast(ArrayType(StringType())).alias(\"singer_arr\"))\nw2v= Word2Vec(vectorSize=3, minCount=0, inputCol=\"singer_arr\", outputCol=\"singer_vec\")\nsong_vec = w2v.fit(song_str2arr).transform(song_str2arr)\n\nsong_vec = song_vec.withColumn('name_arr', split(col(\"song_name\"), \" \").cast(ArrayType(StringType())).alias(\"name_arr\"))\nw2v = Word2Vec(vectorSize=5, minCount=0, inputCol=\"name_arr\", outputCol=\"name_vec\")\nsong_vec = w2v.fit(song_vec).transform(song_vec)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "cell_type": "code", 
            "source": "# convert to VectorAssembler \nfrom pyspark.ml.feature import VectorAssembler\n\ndef extract(row):\n    return (row.song_type, ) + tuple(row.name_vec.toArray().tolist()) + tuple(row.singer_vec.toArray().tolist())\n\nsong_vec_temp = song_vec.select('song_type','name_vec','singer_vec')\n# save as dataframe \nsong_vec_df = song_vec_temp.rdd.map(extract).toDF()\nsong_vec_df= song_vec_df.withColumn(\"id\", monotonically_increasing_id())\nsong_vec_df = song_vec_df.withColumn('_1', song_vec_df['_1'].cast('double'))\n\n# create feature vectros for each row \n# assembler = VectorAssembler(inputCols=song_vec_temp.columns, outputCol=\"features\")\n# song_vec_feat = assembler.transform(song_vec_temp).select('features')\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "cell_type": "code", 
            "source": "song_vec_df.printSchema()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: double (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- id: long (nullable = false)\n\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 18
        }, 
        {
            "cell_type": "code", 
            "source": "# manuely transpose the dataframe - very slow 20mins\nsong_vec_rdd = song_vec_df.rdd\ndata = []\nfor i in range(9):\n    data.append(song_vec_rdd.map(lambda row: row[i]).collect())", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "# convert to matrix \n\n# mat = IndexedRowMatrix(data.map(lambda row:IndexedRow(monotonically_increasing_id(), Vectors(list(row)))\n# song_mat_temp = IndexedRowMatrix(song_vec_mod.rdd.map(lambda row: IndexedRow(row['id'], Vectors.dense(row[:9]))))", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 17
        }, 
        {
            "cell_type": "code", 
            "source": "# manurally construct index row\nindexedT = IndexedRow(0, Vectors.dense(data[0]))\n\ntoIndexedRows = [IndexedRow(i, data[0]) for i in range(9)]\nindexedRows = sc.parallelize(toIndexedRows)\nmat = IndexedRowMatrix(indexedRows)\n\nm = mat.numRows() \nn = mat.numCols()  ", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 22
        }, 
        {
            "cell_type": "code", 
            "source": "rowMat = mat.toRowMatrix()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 28
        }, 
        {
            "cell_type": "code", 
            "source": "rows_rdd = rowMat.rows", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 42
        }, 
        {
            "cell_type": "code", 
            "source": "print(rows_rdd.count())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "9\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 44
        }, 
        {
            "cell_type": "code", 
            "source": "simThres = rowMat.columnSimilarities(0.1)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 29
        }, 
        {
            "cell_type": "code", 
            "source": "print(simThres.numCols(), simThres.numRows())", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "1559987 1559987\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 33
        }, 
        {
            "cell_type": "code", 
            "source": "simTresidm = simThres.toIndexedRowMatrix()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 40
        }, 
        {
            "cell_type": "code", 
            "source": "simThredRdd = simThres.entries", 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o650.getMatrixEntries.\n: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:981)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\t... 16 more\nCaused by: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveExternalCatalog':\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:169)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\t... 21 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\t... 29 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\t... 34 more\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\t... 42 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 43 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 49 more\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 54 more\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\n\tat sun.reflect.GeneratedConstructorAccessor79.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\t... 83 more\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t... 95 more\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2dd590a1, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[1;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-45-a8238024506c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimThredRdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimThres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mentries\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# DataFrame on the Scala/Java side. Then we map each Row in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;31m# the DataFrame back to a MatrixEntry on this side.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mentries_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"getMatrixEntries\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentries_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMatrixEntry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mIllegalArgumentException\u001b[0m: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
                    ], 
                    "evalue": "\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"", 
                    "ename": "IllegalArgumentException", 
                    "output_type": "error"
                }
            ], 
            "execution_count": 45
        }, 
        {
            "cell_type": "code", 
            "source": "# it will return an rdd of indexed row, need to extract \ndef extract(row):\n    return (row.song_type, ) + tuple(row.name_vec.toArray().tolist()) + tuple(row.singer_vec.toArray().tolist())\n\nsong_vec_temp = song_vec.select('song_type','name_vec','singer_vec')\n# save as dataframe \nsong_vec_df = song_vec_temp.rdd.map(extract).toDF()", 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o492.getMatrixEntries.\n: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:981)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\t... 16 more\nCaused by: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveExternalCatalog':\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:169)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\t... 21 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\t... 29 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\t... 34 more\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\t... 42 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 43 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 49 more\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 54 more\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)\n\tat org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:255)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.getMatrixEntries(PythonMLLibAPI.scala:1203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n------\r\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:88)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:57)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:436)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\t... 83 more\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:675)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:219)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t... 95 more\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@86f0b782, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 111 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/jupyter-rt/kernel-cc51d222-5ae8-46c9-99a5-3d643a78fde2-20171019_192249/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(AccessController.java:650)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 108 more\n", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[1;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-34-10458ab5ab3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# it will return an rdd of indexed row, need to extract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msimTredrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimThres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mentries\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# DataFrame on the Scala/Java side. Then we map each Row in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;31m# the DataFrame back to a MatrixEntry on this side.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mentries_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"getMatrixEntries\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentries_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMatrixEntry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mIllegalArgumentException\u001b[0m: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
                    ], 
                    "evalue": "\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"", 
                    "ename": "IllegalArgumentException", 
                    "output_type": "error"
                }
            ], 
            "execution_count": 34
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "# TODO:\n- upon recommended songs, generate 100 songs per user - keep the song_id \n- try classification methods for fine tune to get top 20 \n- take into considerations of more features: song_name, singer, song_type\n- predict the class: 0 - 5 score (training with normalized frequency 1-5 and download automatically 5)\n- evalute both model's top 20 recommendation, rms error \n\nFor collaborative filter<br>\n- visualize the frequency distribution - try normalization \n- ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark21", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "pygments_lexer": "ipython3", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 1
}