{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "## Table of contents\n1. [Load the data](#1.-Load-the-data) <br>\n    1.1 Load song table<br>\n    1.2 Load user table <br>\n    1.3 Load user play frequency table<br>\n2. [Spark performance basics](#2.-Spark-performance-basics)<br>\n3. [Explore the data with Spark APIs](#3.-Explore-the-data-with-Spark-APIs)<br>\n    3.1 [Clean song table](#3.1-Clean-song-table) <br>\n    3.2 [Clean play table and user table](#3.2-Clean-play-table-and-user-table) <br>\n4. [Visualize the data](#4.-Visualize-the-data)<br>\n5. [Build the recommender system](#5.-Build-the-recommender-system)<br>\n    5.1 Collaborative filtering<br>\n    5.2 Tune parameters<br>\n    5.3 Evaluation<br>\n6. [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>\n    6.1 Item based recommender<br>\n    6.2 Integrate<br>\n    \n7. Summary and next steps", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, count, struct, row_number, when, isnan, log,lit\nfrom pyspark.sql.functions import round as cround\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 1. Load the data", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\nspark = SparkSession.builder.getOrCreate()\nuser_table_raw = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'valid_user_highfreq.csv'))\n\nuser_table_raw.take(5)\n\nsong_table= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'song_table.csv'))\nprint(song_table.take(5))\n\nsong_freq = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'user_song_freq.csv'))\nprint(song_freq.take(5))\n\ndownload_table = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'clean_download.csv'))\nprint (download_table.take(5))", 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[Row(uid='751824', device='ar', song_id='6483029', freq='385'), Row(uid='168156920', device='ip', song_id='6792060', freq='5'), Row(uid='497685', device='ar', song_id='7207401', freq='26'), Row(uid='1062806', device='ar', song_id='6841262', freq='50'), Row(uid='168195436', device='ar', song_id='12808784', freq='22')]\n[Row(_c0='0', uid=None, device='ip', song_id='6945370.0', song_name=None, paid_flag=None), Row(_c0='1', uid='1685126.0', device='ar', song_id='170455.0', song_name='\u987a\u6d41\u3001\u9006\u6d41', paid_flag=None), Row(_c0='2', uid='736305.0', device='ar', song_id='23380344.0', song_name='\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)', paid_flag=None), Row(_c0='3', uid='168042561.0', device='ar', song_id='6292506.0', song_name='\u5e1d\u90fd', paid_flag=None), Row(_c0='4', uid='1749320.0', device='ar', song_id='21473237.0', song_name='\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1', paid_flag=None)]\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "user_db = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', '3_1uid.csv'))\nprint (user_db.select('uid').distinct().count())", 
            "execution_count": 144, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "264715\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 2. Spark performance basics", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "print (sc.defaultParallelism)\nprint (sc.getConf().toDebugString())\nprint (\"Number of partitions for the song_freq DataFrame: \" + str(song_freq.rdd.getNumPartitions()))", 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "2\nhive.metastore.warehouse.dir=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/work/spark-warehouse\nspark.app.id=app-20171016230717-0048-126850fe-0ee5-441f-8108-3064dadf856e\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.host=10.143.133.18\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.driver.port=38597\nspark.eventLog.dir=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.id=driver\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.history.fs.logDirectory=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0019:7089\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v65/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.catalogImplementation=hive\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\n"
                }, 
                {
                    "ename": "NameError", 
                    "output_type": "error", 
                    "evalue": "name 'song_freq' is not defined", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-3-a20ffd7eb996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of partitions for the song_freq DataFrame: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0m: name 'song_freq' is not defined"
                    ]
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 3. Explore the data with Spark APIs", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "user_table_raw.show(truncate=False)\nprint (\"Number of users: \", user_table_raw.count())\nprint (\"Number of different users: \" + str(user_table_raw.select('uid').distinct().count()))", 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+----------+---------+\n|_c0|Unnamed: 0|uid      |\n+---+----------+---------+\n|1  |1         |154563989|\n|2  |2         |154806874|\n|3  |3         |154777984|\n|4  |4         |154801899|\n|5  |5         |154522980|\n|6  |6         |154466362|\n|7  |7         |154467953|\n|8  |8         |158752252|\n|9  |9         |154559964|\n|10 |10        |154542883|\n|11 |11        |154828695|\n|12 |12        |154723056|\n|13 |13        |154751052|\n|14 |14        |154630129|\n|15 |15        |154684841|\n|16 |16        |154799108|\n|17 |17        |154786598|\n|18 |18        |154561771|\n|19 |19        |154508382|\n|20 |20        |154710857|\n+---+----------+---------+\nonly showing top 20 rows\n\nNumber of users:  264714\nNumber of different users: 264714\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "song_table.show(truncate=False)\nprint (\"Number of songs: \", song_table.count())\nprint (\"Number of different songs: \" + str(song_table.select('song_id').distinct().count()))", 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+-------------------------------+------+-----------+\n|song_id |song_type|song_name                      |singer|song_length|\n+--------+---------+-------------------------------+------+-----------+\n|602239  |null     |\u859b\u51ef\u742a                            |0     |null       |\n|160911  |null     |\u8521\u4f9d\u6797&\u5468\u6770\u4f26                        |0     |null       |\n|1033156 |null     |\u6c6a\u82cf\u6cf7                            |0     |null       |\n|294622  |null     |DJ\u821e\u66f2                           |0     |null       |\n|517174  |null     |\u68a6\u9e3d                             |0     |null       |\n|6606144 |null     |\u6768\u5c0f\u66fc&\u51b7\u6f20                         |0     |null       |\n|6432663 |null     |\u5c0f\u4e54                             |0     |null       |\n|6587633 |null     |\u97e9\u5b87                             |0     |null       |\n|6587662 |null     |\u97e9\u5b87                             |0     |null       |\n|158182  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|1037626 |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|995380  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|529630  |null     |\u8bd5\u97f3\u789f                            |0     |null       |\n|3006068 |null     |\u7f51\u7edc\u6b4c\u624b                           |0     |null       |\n|4111186 |null     |\u5e7d\u5e7d                             |0     |null       |\n|1488120 |null     |Declan Masterson               |0     |null       |\n|21461735|null     |Lil Uzi Vert&Quavo&Travis Scott|0     |null       |\n|22399766|null     |\u5f20\u6770                             |0     |null       |\n|60057   |null     |\u7518\u840d                             |0     |null       |\n|116273  |null     |null                           |0     |null       |\n+--------+---------+-------------------------------+------+-----------+\nonly showing top 20 rows\n\nNumber of songs:  5047315\nNumber of different songs: 1559990\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "song_table.printSchema()", 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- song_length: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 3.1 Clean song table \n- remove invalid song_id \n- get single entry for each song_id (most common song)\n- drop song_length column because 1) large variance 2) not very relevant", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# song_table.createOrReplaceTempView('song_table')\nsong_table.createOrReplaceTempView('song_table')\nsong_table_valid = spark.sql(\"select *  from song_table where song_id > 0 and song_id is not null\")\n\nprint (\"Number of songs: \", song_table_valid.count())\nprint (\"Number of different songs: \" + str(song_table_valid.select('song_id').distinct().count()))", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs:  3230980\nNumber of different songs: 1559987\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# get most common non-zero song_type\ntype_counts = song_table_valid.groupBy(['song_id', 'song_type'])\\\n    .count().alias('cnt')\\\n    .where(col('song_type') != '0')\n\nmax_type = (type_counts\n    .groupBy('song_id')\n    .agg(F.max(struct(col('count'), col('song_type'))).alias('max'))\n    .select(col('song_id'), col('max.song_type')))\n\n# get most common not null song_name \nname_counts = song_table_valid.groupBy(['song_id', 'song_name'])\\\n    .count().alias('cnt')\\\n    .where(col('song_name').isNotNull())\n    \nmax_name = (name_counts.groupBy('song_id')\n            .agg(F.max(struct(col('count'), col('song_name'))).alias('max'))\n            .select(col('song_id'),col('max.song_name')))\n\n\n# get most common not null singer \nsinger_counts = song_table_valid.groupBy(['song_id', 'singer']).count().alias('cnt').where(col('singer').isNotNull())\nw = Window().partitionBy('song_id').orderBy(col('count').desc())\nmax_singer = (singer_counts\n              .withColumn('rn', row_number().over(w))\n              .where(col('rn')==1)\n              .select('song_id', 'singer'))", 
            "execution_count": 64, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "note: tried pandas - takes a long time for me; both Window or struct should work ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "print (max_type.select('song_id').distinct().count())\nprint (max_name.select('song_id').distinct().count())\nprint (max_singer.select('song_id').distinct().count())", 
            "execution_count": 67, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "231216\n1559532\n1547516\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "songs = song_table_valid.select('song_id').distinct().alias('songs')\nsong_unique = songs\\\n    .join(max_type, 'song_id','left')\\\n    .join(max_name, 'song_id','left')\\\n    .join(max_singer, 'song_id','left')\\\n    .select('song_id', 'song_type','song_name', 'singer')\\\n\nsong_unique = song_unique.na.fill('0', subset=['song_type'])", 
            "execution_count": 103, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "song_unique.show()\nprint (song_unique.select('song_id').distinct().count())", 
            "execution_count": 22, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+--------------------+----------------+\n| song_id|song_type|           song_name|          singer|\n+--------+---------+--------------------+----------------+\n|  100140|        0|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|\n|10015022|        0|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab|\n| 1003644|        0|Save The One, Sav...|  T.M.Revolution|\n| 1004266|        0|        Broken heart|             \u9ec4\u4e49\u8fbe|\n| 1006370|        0|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|\n| 1006422|        0|                  \u6d63\u7eb1|              \u6731\u6d01|\n|10065669|        1|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|\n|10071852|        0|Dirt Road Anthem ...|  Country Nation|\n|10087323|        0|Helden sterben ei...| Michael Wendler|\n| 1009129|        1|    Dietro L'Incanto|Ludovico Einaudi|\n|  100964|        1|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77|\n| 1010103|        0|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|\n|10101536|        0|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|\n|  101021|        0|           I Believe|             Era|\n|10102878|        0|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|\n|10107689|        0|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|\n|  101122|        0|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|\n|  101272|        0|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|\n|10139662|        1|                  \u843d\u82b1|             \u674e\u5c0f\u7490|\n|10141277|        0|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|\n+--------+---------+--------------------+----------------+\nonly showing top 20 rows\n\n1559987\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "song_unique.groupBy('song_type').count().sort(col('count').desc()).show()", 
            "execution_count": 108, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+-------+\n|song_type|  count|\n+---------+-------+\n|        0|1328771|\n|        1| 155797|\n|        2|  65042|\n|        3|  10053|\n|       73|     22|\n|       90|     18|\n|       91|     12|\n|       89|     12|\n|       48|      7|\n|       66|      6|\n|       88|      6|\n|       26|      6|\n|       30|      6|\n|       43|      6|\n|       60|      6|\n|       33|      5|\n|       27|      5|\n|       41|      5|\n|       82|      5|\n|       32|      4|\n+---------+-------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "song_unique.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_song.csv'))", 
            "execution_count": 121, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 3.2 Clean play table and user table\n- remove extremely high frequency (0.9999 quantile at approximately 1000 play frequency)\n- remove play history without uid \n\nNumber of valid users:  264708 <br>\nNumber of invalid users:  186", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "song_freq = song_freq.withColumn('freq', song_freq['freq'].cast('integer'))\nsong_freq = song_freq.filter((col('uid').isNotNull()) & (col('uid') > '0') &\n                             (col('song_id').isNotNull()) & (col('song_id')>'0'))", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "source": "song_freq.sort(col('freq').desc()).show()", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+-----+\n|      uid|device| song_id| freq|\n+---------+------+--------+-----+\n|167982849|    ar| 4554016|73609|\n|  1791497|    ar| 3401476|51858|\n|   751824|    ar| 9950164|46970|\n|  1685126|    ar|15249349|41265|\n|  1685126|    ar| 9950164|39207|\n| 37025504|    ar| 9950164|35198|\n|  1791497|    ar|15198178|30975|\n| 37025504|    ar|15249349|29830|\n|  1791497|    ar| 6468891|26765|\n|  1791497|    ar|  442265|23907|\n|  1685126|    ar| 5237384|22949|\n|  1791497|    ar| 9950164|22849|\n| 22730453|    ar| 7005106|22294|\n| 22730453|    ar| 5965686|22289|\n|   497685|    ar| 9950164|21574|\n|  1062806|    ar| 9950164|20929|\n|  1791497|    ar| 5245130|19813|\n| 37025504|    ar| 5237384|19799|\n|  1685126|    ar| 6468891|19709|\n|  1791497|    ar|  125802|19697|\n+---------+------+--------+-----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# # super slow; may not be working \n# import pandas as pd\n# user_song_freq = song_freq.toPandas().head()\n# cutoff_freq = user_song_freq['freq'].quantile(0.999)\n# valid_freq = user_song_freq.loc[(user_song_freq['freq']<cutoff_freq) & (user_song_freq['song_id'] > 0)]\n# extreme_freq = user_song_freq.loc[(user_song_freq['freq']>=cutoff_freq)]", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "cutoff_freq = song_freq.approxQuantile('freq', [0.99], 0.005)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "cutoff_freq = 1000", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "extreme_freq = song_freq.filter(col('freq') >= cutoff_freq)\n# get valid song played frequency\nvalid_freq = song_freq.filter(col('freq') < cutoff_freq)\n\n# get valid users \noutlier_user = extreme_freq.select('uid').distinct()\noutlier_user.createOrReplaceTempView('filter_view')\nvalid_user = user_table_raw.where('uid not in (select uid from filter_view)')", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "print (\"Number of valid users: \", valid_user.select('uid').distinct().count())\nprint (\"Number of invalid users: \", outlier_user.select('uid').distinct().count())", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of valid users:  264708\nNumber of invalid users:  186\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# song_length = song_table.groupBy(['song_id', 'length']).avg(col('song_length')).alias('avg_length').where((col('song_length').isNotNull()) & (col('song_length') > '200') & (col('song_length') <'720'))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "##### Needs to clean song_table:\n1. remove duplicate entries; only for single ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "download_table.show(truncate=False)", 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+-----------+------+----------+------------------------------+---------+\n|_c0|uid        |device|song_id   |song_name                     |paid_flag|\n+---+-----------+------+----------+------------------------------+---------+\n|0  |null       |ip    |6945370.0 |null                          |null     |\n|1  |1685126.0  |ar    |170455.0  |\u987a\u6d41\u3001\u9006\u6d41                         |null     |\n|2  |736305.0   |ar    |23380344.0|\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)                   |null     |\n|3  |168042561.0|ar    |6292506.0 |\u5e1d\u90fd                            |null     |\n|4  |1749320.0  |ar    |21473237.0|\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1                      |null     |\n|5  |155948236.0|ar    |93388.0   |\u4e9a\u62c9\u4f2f\u8df3\u821e\u5973\u90ce                       |null     |\n|6  |167794453.0|ar    |497722.0  |\u85d5\u65ad\u4e1d\u8fde                          |null     |\n|7  |168505311.0|ip    |4188142.0 |null                          |null     |\n|8  |168031064.0|ar    |4243838.0 |\u7231\u60c5\u7801\u5934(2651,cn \u5929\u5730\u4eba\u97f3\u4e50\u7f51)          |null     |\n|9  |167626177.0|ar    |1080516.0 |\u65e0\u6cd5\u539f\u8c05(\u7535\u89c6\u5267\u300a\u56de\u5bb6\u7684\u8bf1\u60d1\u300b\u4e3b\u9898\u66f2)           |null     |\n|10 |736305.0   |ar    |344885.0  |Because Of You                |null     |\n|11 |736305.0   |ar    |119699.0  |\u522b\u519b\u8425                           |null     |\n|12 |1685126.0  |ar    |3042675.0 |\u65f6\u5149\u673a\u524d\u594f-\u4e94\u6708\u5929_\u4e94\u6708\u5929(\u94c3\u58f0)             |null     |\n|13 |167691874.0|ar    |3287563.0 |\u5176\u5b9e\u90fd\u6ca1\u6709                         |null     |\n|14 |736305.0   |ar    |6792401.0 |Dream It Possible             |null     |\n|15 |167973828.0|ar    |10901925.0|Overtime (Vicetone Remix Edit)|null     |\n|16 |736305.0   |ar    |9925786.0 |Dream It Possible(34\u79d2\u94c3\u58f0\u7248)     |null     |\n|17 |167784467.0|ar    |8050943.0 |\u8036\u5229\u4e9a\u5973\u90ce                         |null     |\n|18 |168007277.0|ip    |4373829.0 |null                          |null     |\n|19 |167782564.0|ar    |325517.0  |\u68a6\u9a7c\u94c3                           |null     |\n+---+-----------+------+----------+------------------------------+---------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "song_freq.show(truncate=False)", 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+----+\n|uid      |device|song_id |freq|\n+---------+------+--------+----+\n|751824   |ar    |6483029 |385 |\n|168156920|ip    |6792060 |5   |\n|497685   |ar    |7207401 |26  |\n|1062806  |ar    |6841262 |50  |\n|168195436|ar    |12808784|22  |\n|1685126  |ar    |59582   |26  |\n|168286187|ar    |4188404 |2   |\n|37025504 |ar    |481552  |733 |\n|168478031|ar    |9822502 |4   |\n|168406030|ar    |909773  |7   |\n|168410987|ar    |5425869 |10  |\n|168511270|ar    |6817428 |68  |\n|168115240|ar    |23665227|2   |\n|168396372|ar    |4276822 |10  |\n|168417737|ar    |5383328 |19  |\n|1062806  |ar    |20870989|73  |\n|168373631|ar    |7202991 |60  |\n|168335848|ip    |4112638 |15  |\n|37025504 |ar    |1108956 |76  |\n|168453430|ar    |1705363 |1   |\n+---------+------+--------+----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "download_table.select('uid').distinct().count()", 
            "execution_count": 143, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "242243"
                    }, 
                    "metadata": {}, 
                    "execution_count": 143, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "valid_user.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    \nvalid_freq.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_freq.csv'))", 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 4. Visualize the data\n###  TODO\nUsing Seaborn and matplotlib", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "!pip install seaborn", 
            "execution_count": 164, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): seaborn in /usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages\r\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import seaborn as sns\n%matplotlib inline", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import pandas as pd\nfreqPandas = valid_freq.toPandas()\ndlPandas = download_table.toPandas()\nsns.lmplot(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "plot matrix for song played by user, with frequency as the hue ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "min(freqPandas['uid'])", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "sns.palplot(sns.diverging_palette(10, 133, sep=80, n=10))\n# user_id: max 100047599, min 99983627\n# song_id: max 2147483647, min 2\nsns.(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 5. Build the recommender system\n(Cited from IBM bluemix Data Science Experience (DSX) document) <br>\n\n\"\nCollaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who have similar preference on the same item will also have similar opinions on items that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users versus all songs. Most cells in the matrix will be empty. An empty cell means the user hasn't played the song yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for song factors and solving for user factors.\n\"\n\n\nChallenge in this recommender system: <br>\n1. Small number of play history that could be shown by sparsity of the utility matrix\n2. Limited features for songs - mixed language\n\nSolution: <br>\n1. Hybrid", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# restarting kernel\nreload = True", 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# load saved files if starting from middle\nif reload: \n    song_unique = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_song.csv'))\n    song_unique.take(5)\n    \n    valid_freq = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_freq.csv'))\n    valid_freq.take(5)\n    \n    valid_user = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    valid_user.take(5)\n    \n    download_table = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'clean_download.csv'))", 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "valid_freq = valid_freq.withColumn('uid', valid_freq['uid'].cast('integer'))\nvalid_freq = valid_freq.withColumn('song_id', valid_freq['song_id'].cast('integer'))\nvalid_freq = valid_freq.withColumn('freq', valid_freq['freq'].cast('double'))\n\nvalid_user = valid_user.withColumn('uid', valid_user['uid'].cast('integer'))\n\nsong_unique = song_unique.withColumn('song_type', song_unique['song_type'].cast('integer'))\nsong_unique = song_unique.withColumn('song_id', song_unique['song_id'].cast('integer'))\n\nvalid_download = download_table.withColumn('uid', download_table['uid'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\n\nvalid_freq = valid_freq.withColumn('label',log(10.0, valid_freq.freq))", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "valid_freq.printSchema()\nvalid_user.printSchema()\nsong_unique.printSchema()\nvalid_download.printSchema()", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "##### checking\n\n# check sparsity \ncounts_freq = valid_freq.count()\nprint ('Number of song frequency entried: ', counts_freq)\n# songs played/( songs x users)\npercentage = (counts_freq)*1.0/1559987/264708\nprint ('Percentage of song played: ', percentage)", 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of song frequency entried:  32801295\nPercentage of song played:  7.943336195316835e-05\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "##### checking\n\nmoreThanOnce = valid_freq.groupBy('song_id').count().alias('cnt').where(col('count') > 1).select('song_id').count()\nprint ('Number of songs played more than once: ', moreThanOnce)\n# significantly less, only recommend songs played more than once\n# utility matrix will be less sparse\n# number of song frequency entries will be reduce by 686993(only played once) -32114102, which is 2%", 
            "execution_count": 28, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs played more than once:  872994\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "print ('Percentage of filled matrix: ', round((32801295-686993)*100.0/(872994*264708),4))\n# vs. ~0.0073% filled ", 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Percentage of filled matrix:  0.0139\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "song_unique.createOrReplaceTempView('song_unique')\nvalid_freq.createOrReplaceTempView('valid_freq')\nsong_freq = spark.sql(\"\"\"\n    select s.song_type, s.song_id, s.song_name, s.singer, COALESCE(f.cnt,0) as freq\n    from song_unique as s\n    left join\n        (select song_id, count(*) as cnt from valid_freq group by song_id) as f\n        on f.song_id = s.song_id\n\"\"\")", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "song_freq.printSchema()", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- freq: long (nullable = false)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "##### checking\n\nprint (song_freq.select('song_id').count())\nprint (song_freq.select('song_id').where(col('freq')>1).count())\nsong_freq.show()", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "1559987\n872994\n+---------+--------+--------------------+----------------+----+\n|song_type| song_id|           song_name|          singer|freq|\n+---------+--------+--------------------+----------------+----+\n|        0|  100140|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|   2|\n|        0|10015022|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab| 193|\n|        0| 1003644|Save The One, Sav...|  T.M.Revolution|   1|\n|        0| 1004266|        Broken heart|             \u9ec4\u4e49\u8fbe|   5|\n|        0| 1006370|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|  30|\n|        0| 1006422|                  \u6d63\u7eb1|              \u6731\u6d01|   5|\n|        1|10065669|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|  17|\n|        0|10071852|Dirt Road Anthem ...|  Country Nation|   1|\n|        0|10087323|Helden sterben ei...| Michael Wendler|   1|\n|        1| 1009129|    Dietro L'Incanto|Ludovico Einaudi|  16|\n|        1|  100964|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77| 110|\n|        0| 1010103|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|   1|\n|        0|10101536|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|   2|\n|        0|  101021|           I Believe|             Era|   2|\n|        0|10102878|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|  18|\n|        0|10107689|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|   3|\n|        0|  101122|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|  13|\n|        0|  101272|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|   2|\n|        1|10139662|                  \u843d\u82b1|             \u674e\u5c0f\u7490|  73|\n|        0|10141277|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|   4|\n+---------+--------+--------------------+----------------+----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "### checking \ntest = utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns])", 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# very slow\n# 872994 * 264713\nutility_matrix.printSchema()", 
            "execution_count": 24, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- label: double (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.1 Collaborative filtering\n\nsetup: 80% training and 20% test set ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# take log of the frequency and scale to -1 to 100 representing degree of preference \n\nvalid_freq2 = valid_freq.withColumn('label',cround(log(10.0, valid_freq.freq)/3.0*10,0))\nvalid_freq2 = valid_freq2.replace(0, -1, subset=['label'])\n# should be None \nvalid_freq2 = valid_freq2.na.fill(0, subset=['label'])\n\n# print(valid_freq2.groupBy().max('label').show())", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# combine with download table\n\nvalid_download = valid_download.withColumn('download', lit(1))\nvalid_download = valid_download.drop('device')\nvalid_download = valid_download.drop('song_name')\n\nvalid_score = valid_freq2.join(valid_download, ['uid','song_id'], 'left_outer')\nvalid_score = valid_score.na.fill(0, subset=['download'])\n\nvalid_score = valid_score.withColumn('label', when(valid_score.download==1, lit(10)).otherwise(valid_score.label))\n\nvalid_score = valid_score.replace(-0.5, -1, subset=['label'])", 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "(trainingFreq, testFreq) = valid_score.randomSplit([80.0, 20.0])\n\ntrainingFreq.printSchema()\n# utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns]).show()\ntestFreq.printSchema()", 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "model = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\").fit(trainingFreq)", 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "KeyboardInterrupt", 
                    "output_type": "error", 
                    "evalue": "", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-13-f416e09da5db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"song_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratingCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "#### Evaluation for single model\n\nAccoding to DSX document again:\n\"\nA NaN result is due to [SPARK-14489](https://issues.apache.org/jira/browse/SPARK-14489) and because the model can't predict values for users for which there's no data.\"", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "predictions = model.transform(testFreq)\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))", 
            "execution_count": 125, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The root mean squared error for our model is: 2.9157391903149836\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "predictions.createOrReplaceTempView('pred_subset')\npred_first20 = spark.sql('select * from pred_subset order by uid limit 100').show()", 
            "execution_count": 126, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------+------+----+-----+----+---------+--------+------------+\n|  uid| song_id|device|freq|label| _c0|paid_flag|download|  prediction|\n+-----+--------+------+----+-----+----+---------+--------+------------+\n|12333| 5237384|    ip|52.0|  6.0|null|     null|       0|   4.3707337|\n|12333|  708528|    ip|11.0|  3.0|null|     null|       0|   3.6194046|\n|12333|  875447|    ip|54.0|  6.0|null|     null|       0|   1.6091484|\n|12333|  706155|    ip|14.0|  4.0|null|     null|       0| -0.08946617|\n|12333|   90519|    ip| 6.0|  3.0|null|     null|       0|   2.2461584|\n|12333|21596231|    ip| 2.0|  1.0|null|     null|       0|   5.9548955|\n|12333|  703693|    ip|13.0|  4.0|null|     null|       0|   3.8563719|\n|51923| 6842853|    ar| 1.0| -1.0|null|     null|       0|         NaN|\n|60183|  127430|    ar| 2.0|  1.0|null|     null|       0| -0.33274028|\n|60183|  445491|    ar| 1.0| -1.0|null|     null|       0| -0.23748893|\n|60183| 6094076|    ar| 2.0|  1.0|null|     null|       0|   0.8730478|\n|60183|  130997|    ar| 2.0|  1.0|null|     null|       0|  -0.2845996|\n|60183| 4392706|    ar| 1.0| -1.0|null|     null|       0|-0.012946829|\n|60183| 1117264|    ar| 2.0|  1.0|null|     null|       0|  0.06794316|\n|60183| 4803341|    ar| 1.0| -1.0|null|     null|       0| -0.34041137|\n|60183|  164300|    ar| 1.0| -1.0|null|     null|       0|  -0.0396643|\n|60183| 6696820|    ar| 1.0| -1.0|null|     null|       0| -0.61629385|\n|60183|  537739|    ar| 1.0| -1.0|null|     null|       0| -0.15378176|\n|60183|  116329|    ar| 1.0| -1.0|null|     null|       0|  0.26834732|\n|60183|  500820|    ar| 1.0| -1.0|null|     null|       0| -0.08018987|\n+-----+--------+------+----+-----+----+---------+--------+------------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.2 Tune parameters\n\nCross validation and grid search to tune for hyperparameters\n\nALS algorithm :\n```python\n    class pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )\n```\n\nThe ALS hyperparameters are:\n- `rank` = the number of latent factors in the model\n- `maxIter` = the maximum number of iterations \n- `regParam` = the regularization parameter", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "(trainingScore, validationScore) = valid_score.randomSplit([90.0, 10.0])\n\nals = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n\n# paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 3, 5, 7]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,2,3]).addGrid(als.regParam, [0.05, 0.1]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = crossval.fit(trainingScore)\npredictions = cvModel.transform(validationScore)\n\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.drop())))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "print ('Best rank is: ', cvModel.bestModel.rank)\nprint ('Best regularizer is: ', cvModel.bestModel.params)\n# evaluate with 0 for null prediction\n# print (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))\n# The root mean squared error for our model is: 3.1921429126212857", 
            "execution_count": 136, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Best rank is:  1\nBest regularizer is:  []\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "valid_score.printSchema()", 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "predictions.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'predictions_CF_1015.csv'))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 5.5 Recommend songs\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "utility_matrix_small = valid_user.crossJoin(song_freq.select('song_id')).select('uid','song_id')\nutility_matrix_small = utility_matrix_small.join(valid_score, ['uid', 'song_id'], 'left_outer').select('uid', 'song_id', 'label')\n\nutility_matrix = utility_matrix_small.na.fill('0', subset=['label'])\n\n# Replace predicted NaN values with the average frequency and evaluate the model\n# avgScore = utility_matrix.select('label').groupBy().avg().first()[0]\n# print (\"The average score in the dataset is: \" + str(avgScore))", 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "def recommendSongs(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = utility_matrix.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = utility_matrix.filter(utility_matrix.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction)\n\n    recommendations.show(truncate=False)", 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "def recommendSongsAlt(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction)\n\n    recommendations.show(truncate=False)", 
            "execution_count": 35, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "print (\"Recommendations for user 169262317:\")\nrecommendSongsAlt(cvModel2, 169262317, 10)\n# print \"Recommendations for user 471:\"\n# recommendSongs(cvModel, 471, 10)\n# print \"Recommendations for user 496:\"\n# recommendSongs(cvModel, 496, 10)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Recommendations for user 169262317:\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 6. Hybrid recommender system", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.mllib.linalg import Vectors\n# Package for distributed linear algebra DIMSUM\n# Dimension Independent Matrix Square using MapReduce\nfrom pyspark.mllib.linalg.distributed import MatrixEntry, RowMatrix\n\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.types import ArrayType, StringType", 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# convert to Dense vector in dataframe \n# song_unique.createOrReplaceTempView('song20')\nsong_unique = song_unique.na.fill('0', subset=['singer','song_name'])\n\nsong_str2arr = song_unique.withColumn('singer_arr', split(col(\"singer\"), \" \").cast(ArrayType(StringType())).alias(\"singer_arr\"))\nw2v= Word2Vec(vectorSize=3, minCount=0, inputCol=\"singer_arr\", outputCol=\"singer_vec\")\nsong_vec = w2v.fit(song_str2arr).transform(song_str2arr)\n\nsong_vec = song_vec.withColumn('name_arr', split(col(\"song_name\"), \" \").cast(ArrayType(StringType())).alias(\"name_arr\"))\nw2v = Word2Vec(vectorSize=5, minCount=0, inputCol=\"name_arr\", outputCol=\"name_vec\")\nsong_vec = w2v.fit(song_vec).transform(song_vec)", 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "song_vec.count()", 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "1559987"
                    }, 
                    "metadata": {}, 
                    "execution_count": 15, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# TODO:\n- upon recommended songs, generate 100 songs per user - keep the song_id \n- try classification methods for fine tune to get top 20 \n- take into considerations of more features: song_name, singer, song_type\n- predict the class: 0 - 5 score (training with normalized frequency 1-5 and download automatically 5)\n- evalute both model's top 20 recommendation, rms error \n\nFor collaborative filter<br>\n- visualize the frequency distribution - try normalization \n- ", 
            "cell_type": "markdown"
        }
    ], 
    "nbformat_minor": 1
}