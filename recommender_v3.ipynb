{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "## Table of contents\n1. [Load the data](#1.-Load-the-data) <br>\n    1.1 Load song table<br>\n    1.2 Load user table <br>\n    1.3 Load user play frequency table<br>\n2. [Spark performance basics](#2.-Spark-performance-basics)<br>\n3. [Explore the data with Spark APIs](#3.-Explore-the-data-with-Spark-APIs)<br>\n    3.1 [Clean song table](#3.1-Clean-song-table) <br>\n    3.2 [Clean play table and user table](#3.2-Clean-play-table-and-user-table) <br>\n4. [Visualize the data](#4.-Visualize-the-data)<br>\n5. [Build the recommender system](#5.-Build-the-recommender-system)<br>\n    5.1 Setup training and test set<br>\n    5.2 Train collaborative filtering<br>\n    5.3 Tune parameters <br>\n    5.4 Evaluate recommendation results<br>\n6. [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>\n    6.1 Setup vectors<br>\n    6.2 Compute similarity matrix<br>\n    6.3 Recommend for Selected user <br>\n    \n7. Summary and next steps", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, count, struct, row_number, when, isnan, log,lit\nfrom pyspark.sql.functions import round as cround\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 1. Load the data\n\nOnly for the first time. <br>\nCleaned data will be saved and could be imported in future. <<br>\n\nJump to other sections: \n- [Build the recommender system](#5.-Build-the-recommender-system)<br>\n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "cell_type": "markdown"
        }, 
        {
            "source": "spark = SparkSession.builder.getOrCreate()\nuser_table_raw = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'valid_user_highfreq.csv'))\n\nuser_table_raw.take(5)\n\nsong_table= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'song_table.csv'))\nprint(song_table.take(5))\n\nsong_freq = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'user_song_freq.csv'))\nprint(song_freq.take(5))\n\ndownload_table = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'clean_download.csv'))\nprint (download_table.take(5))", 
            "metadata": {}, 
            "execution_count": 131, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "KeyboardInterrupt", 
                    "output_type": "error", 
                    "evalue": "", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-131-c302e545493b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muser_table_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.execution.datasources.csv.CSVFileFormat'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbmos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'musicrecommendation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid_user_highfreq.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muser_table_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ]
        }, 
        {
            "source": "user_db = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', '3_1uid.csv'))\nprint (user_db.select('uid').distinct().count())", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "264715\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 2. Spark performance basics", 
            "cell_type": "markdown"
        }, 
        {
            "source": "print (sc.defaultParallelism)\nprint (sc.getConf().toDebugString())\nprint (\"Number of partitions for the song_freq DataFrame: \" + str(song_freq.rdd.getNumPartitions()))", 
            "metadata": {}, 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "4\nhive.metastore.warehouse.dir=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/work/spark-warehouse\nspark.app.id=app-20171021174259-0155-332c25ad-3f8b-4fff-be46-e709b7342dff\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.host=10.143.133.71\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.driver.port=35869\nspark.eventLog.dir=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.id=driver\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.history.fs.logDirectory=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0022:7089\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v65/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.catalogImplementation=hive\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\nNumber of partitions for the song_freq DataFrame: 8\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 3. Explore the data with Spark APIs", 
            "cell_type": "markdown"
        }, 
        {
            "source": "user_table_raw.show(truncate=False)\nprint (\"Number of users: \", user_table_raw.count())\nprint (\"Number of different users: \" + str(user_table_raw.select('uid').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+----------+---------+\n|_c0|Unnamed: 0|uid      |\n+---+----------+---------+\n|1  |1         |154563989|\n|2  |2         |154806874|\n|3  |3         |154777984|\n|4  |4         |154801899|\n|5  |5         |154522980|\n|6  |6         |154466362|\n|7  |7         |154467953|\n|8  |8         |158752252|\n|9  |9         |154559964|\n|10 |10        |154542883|\n|11 |11        |154828695|\n|12 |12        |154723056|\n|13 |13        |154751052|\n|14 |14        |154630129|\n|15 |15        |154684841|\n|16 |16        |154799108|\n|17 |17        |154786598|\n|18 |18        |154561771|\n|19 |19        |154508382|\n|20 |20        |154710857|\n+---+----------+---------+\nonly showing top 20 rows\n\nNumber of users:  264714\nNumber of different users: 264714\n"
                }
            ]
        }, 
        {
            "source": "song_table.show(truncate=False)\nprint (\"Number of songs: \", song_table.count())\nprint (\"Number of different songs: \" + str(song_table.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+-------------------------------+------+-----------+\n|song_id |song_type|song_name                      |singer|song_length|\n+--------+---------+-------------------------------+------+-----------+\n|602239  |null     |\u859b\u51ef\u742a                            |0     |null       |\n|160911  |null     |\u8521\u4f9d\u6797&\u5468\u6770\u4f26                        |0     |null       |\n|1033156 |null     |\u6c6a\u82cf\u6cf7                            |0     |null       |\n|294622  |null     |DJ\u821e\u66f2                           |0     |null       |\n|517174  |null     |\u68a6\u9e3d                             |0     |null       |\n|6606144 |null     |\u6768\u5c0f\u66fc&\u51b7\u6f20                         |0     |null       |\n|6432663 |null     |\u5c0f\u4e54                             |0     |null       |\n|6587633 |null     |\u97e9\u5b87                             |0     |null       |\n|6587662 |null     |\u97e9\u5b87                             |0     |null       |\n|158182  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|1037626 |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|995380  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|529630  |null     |\u8bd5\u97f3\u789f                            |0     |null       |\n|3006068 |null     |\u7f51\u7edc\u6b4c\u624b                           |0     |null       |\n|4111186 |null     |\u5e7d\u5e7d                             |0     |null       |\n|1488120 |null     |Declan Masterson               |0     |null       |\n|21461735|null     |Lil Uzi Vert&Quavo&Travis Scott|0     |null       |\n|22399766|null     |\u5f20\u6770                             |0     |null       |\n|60057   |null     |\u7518\u840d                             |0     |null       |\n|116273  |null     |null                           |0     |null       |\n+--------+---------+-------------------------------+------+-----------+\nonly showing top 20 rows\n\nNumber of songs:  5047315\nNumber of different songs: 1559990\n"
                }
            ]
        }, 
        {
            "source": "song_table.printSchema()", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- song_length: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 3.1 Clean song table \n##### Need to clean song_table:\n- remove invalid song_id \n- get single entry for each song_id (most common song)\n- drop song_length column because 1) large variance 2) not very relevant", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# song_table.createOrReplaceTempView('song_table')\nsong_table.createOrReplaceTempView('song_table')\nsong_table_valid = spark.sql(\"select *  from song_table where song_id > 0 and song_id is not null\")\n\nprint (\"Number of songs: \", song_table_valid.count())\nprint (\"Number of different songs: \" + str(song_table_valid.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs:  3230980\nNumber of different songs: 1559987\n"
                }
            ]
        }, 
        {
            "source": "# get most common non-zero song_type\ntype_counts = song_table_valid.groupBy(['song_id', 'song_type'])\\\n    .count().alias('cnt')\\\n    .where(col('song_type') != '0')\n\nmax_type = (type_counts\n    .groupBy('song_id')\n    .agg(F.max(struct(col('count'), col('song_type'))).alias('max'))\n    .select(col('song_id'), col('max.song_type')))\n\n# get most common not null song_name \nname_counts = song_table_valid.groupBy(['song_id', 'song_name'])\\\n    .count().alias('cnt')\\\n    .where(col('song_name').isNotNull())\n    \nmax_name = (name_counts.groupBy('song_id')\n            .agg(F.max(struct(col('count'), col('song_name'))).alias('max'))\n            .select(col('song_id'),col('max.song_name')))\n\n\n# get most common not null singer \nsinger_counts = song_table_valid.groupBy(['song_id', 'singer']).count().alias('cnt').where(col('singer').isNotNull())\nw = Window().partitionBy('song_id').orderBy(col('count').desc())\nmax_singer = (singer_counts\n              .withColumn('rn', row_number().over(w))\n              .where(col('rn')==1)\n              .select('song_id', 'singer'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "note: tried pandas - takes a long time for me; both Window or struct should work ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "print (max_type.select('song_id').distinct().count())\nprint (max_name.select('song_id').distinct().count())\nprint (max_singer.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "231216\n1559532\n1547516\n"
                }
            ]
        }, 
        {
            "source": "songs = song_table_valid.select('song_id').distinct().alias('songs')\nsong_unique = songs\\\n    .join(max_type, 'song_id','left')\\\n    .join(max_name, 'song_id','left')\\\n    .join(max_singer, 'song_id','left')\\\n    .select('song_id', 'song_type','song_name', 'singer')\\\n\nsong_unique = song_unique.na.fill('0', subset=['song_type'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.show()\nprint (song_unique.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 22, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+--------------------+----------------+\n| song_id|song_type|           song_name|          singer|\n+--------+---------+--------------------+----------------+\n|  100140|        0|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|\n|10015022|        0|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab|\n| 1003644|        0|Save The One, Sav...|  T.M.Revolution|\n| 1004266|        0|        Broken heart|             \u9ec4\u4e49\u8fbe|\n| 1006370|        0|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|\n| 1006422|        0|                  \u6d63\u7eb1|              \u6731\u6d01|\n|10065669|        1|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|\n|10071852|        0|Dirt Road Anthem ...|  Country Nation|\n|10087323|        0|Helden sterben ei...| Michael Wendler|\n| 1009129|        1|    Dietro L'Incanto|Ludovico Einaudi|\n|  100964|        1|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77|\n| 1010103|        0|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|\n|10101536|        0|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|\n|  101021|        0|           I Believe|             Era|\n|10102878|        0|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|\n|10107689|        0|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|\n|  101122|        0|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|\n|  101272|        0|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|\n|10139662|        1|                  \u843d\u82b1|             \u674e\u5c0f\u7490|\n|10141277|        0|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|\n+--------+---------+--------------------+----------------+\nonly showing top 20 rows\n\n1559987\n"
                }
            ]
        }, 
        {
            "source": "song_unique.groupBy('song_type').count().sort(col('count').desc()).show()", 
            "metadata": {}, 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+-------+\n|song_type|  count|\n+---------+-------+\n|        0|1328771|\n|        1| 155797|\n|        2|  65042|\n|        3|  10053|\n|       73|     22|\n|       90|     18|\n|       89|     12|\n|       91|     12|\n|       48|      7|\n|       88|      6|\n|       43|      6|\n|       66|      6|\n|       60|      6|\n|       26|      6|\n|       30|      6|\n|       27|      5|\n|       41|      5|\n|       33|      5|\n|       82|      5|\n|       32|      4|\n+---------+-------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "song_unique.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_song.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 3.2 Clean play table and user table\n- remove extremely high frequency (0.9999 quantile at approximately 1000 play frequency)\n- remove play history without uid \n\nNumber of valid users:  264708 <br>\nNumber of invalid users:  186", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_freq = song_freq.withColumn('freq', song_freq['freq'].cast('integer'))\nsong_freq = song_freq.filter((col('uid').isNotNull()) & (col('uid') > '0') &\n                             (col('song_id').isNotNull()) & (col('song_id')>'0'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_freq.sort(col('freq').desc()).show()", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+-----+\n|      uid|device| song_id| freq|\n+---------+------+--------+-----+\n|167982849|    ar| 4554016|73609|\n|  1791497|    ar| 3401476|51858|\n|   751824|    ar| 9950164|46970|\n|  1685126|    ar|15249349|41265|\n|  1685126|    ar| 9950164|39207|\n| 37025504|    ar| 9950164|35198|\n|  1791497|    ar|15198178|30975|\n| 37025504|    ar|15249349|29830|\n|  1791497|    ar| 6468891|26765|\n|  1791497|    ar|  442265|23907|\n|  1685126|    ar| 5237384|22949|\n|  1791497|    ar| 9950164|22849|\n| 22730453|    ar| 7005106|22294|\n| 22730453|    ar| 5965686|22289|\n|   497685|    ar| 9950164|21574|\n|  1062806|    ar| 9950164|20929|\n|  1791497|    ar| 5245130|19813|\n| 37025504|    ar| 5237384|19799|\n|  1685126|    ar| 6468891|19709|\n|  1791497|    ar|  125802|19697|\n+---------+------+--------+-----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "cutoff_freq = song_freq.approxQuantile('freq', [0.99], 0.005)\nprint (cutoff_freq)", 
            "metadata": {}, 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[54.0]\n"
                }
            ]
        }, 
        {
            "source": "cutoff_freq = 1000", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "extreme_freq = song_freq.filter(col('freq') >= cutoff_freq)\n# get valid song played frequency\nvalid_freq = song_freq.filter(col('freq') < cutoff_freq)\n\n# get valid users \noutlier_user = extreme_freq.select('uid').distinct()\noutlier_user.createOrReplaceTempView('filter_view')\nvalid_user = user_table_raw.where('uid not in (select uid from filter_view)')", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print (\"Number of valid users: \", valid_user.select('uid').distinct().count())\nprint (\"Number of invalid users: \", outlier_user.select('uid').distinct().count())\n# Number of valid users:  264708\n# Number of invalid users:  186", 
            "metadata": {}, 
            "execution_count": 19, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of valid users:  264708\nNumber of invalid users:  186\n"
                }
            ]
        }, 
        {
            "source": "# too confusing to add this feature\n# song_length = song_table.groupBy(['song_id', 'length']).avg(col('song_length')).alias('avg_length').where((col('song_length').isNotNull()) & (col('song_length') > '200') & (col('song_length') <'720'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "download_table.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+-----------+------+----------+------------------------------+---------+\n|_c0|uid        |device|song_id   |song_name                     |paid_flag|\n+---+-----------+------+----------+------------------------------+---------+\n|0  |null       |ip    |6945370.0 |null                          |null     |\n|1  |1685126.0  |ar    |170455.0  |\u987a\u6d41\u3001\u9006\u6d41                         |null     |\n|2  |736305.0   |ar    |23380344.0|\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)                   |null     |\n|3  |168042561.0|ar    |6292506.0 |\u5e1d\u90fd                            |null     |\n|4  |1749320.0  |ar    |21473237.0|\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1                      |null     |\n|5  |155948236.0|ar    |93388.0   |\u4e9a\u62c9\u4f2f\u8df3\u821e\u5973\u90ce                       |null     |\n|6  |167794453.0|ar    |497722.0  |\u85d5\u65ad\u4e1d\u8fde                          |null     |\n|7  |168505311.0|ip    |4188142.0 |null                          |null     |\n|8  |168031064.0|ar    |4243838.0 |\u7231\u60c5\u7801\u5934(2651,cn \u5929\u5730\u4eba\u97f3\u4e50\u7f51)          |null     |\n|9  |167626177.0|ar    |1080516.0 |\u65e0\u6cd5\u539f\u8c05(\u7535\u89c6\u5267\u300a\u56de\u5bb6\u7684\u8bf1\u60d1\u300b\u4e3b\u9898\u66f2)           |null     |\n|10 |736305.0   |ar    |344885.0  |Because Of You                |null     |\n|11 |736305.0   |ar    |119699.0  |\u522b\u519b\u8425                           |null     |\n|12 |1685126.0  |ar    |3042675.0 |\u65f6\u5149\u673a\u524d\u594f-\u4e94\u6708\u5929_\u4e94\u6708\u5929(\u94c3\u58f0)             |null     |\n|13 |167691874.0|ar    |3287563.0 |\u5176\u5b9e\u90fd\u6ca1\u6709                         |null     |\n|14 |736305.0   |ar    |6792401.0 |Dream It Possible             |null     |\n|15 |167973828.0|ar    |10901925.0|Overtime (Vicetone Remix Edit)|null     |\n|16 |736305.0   |ar    |9925786.0 |Dream It Possible(34\u79d2\u94c3\u58f0\u7248)     |null     |\n|17 |167784467.0|ar    |8050943.0 |\u8036\u5229\u4e9a\u5973\u90ce                         |null     |\n|18 |168007277.0|ip    |4373829.0 |null                          |null     |\n|19 |167782564.0|ar    |325517.0  |\u68a6\u9a7c\u94c3                           |null     |\n+---+-----------+------+----------+------------------------------+---------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "song_freq.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+----+\n|uid      |device|song_id |freq|\n+---------+------+--------+----+\n|751824   |ar    |6483029 |385 |\n|168156920|ip    |6792060 |5   |\n|497685   |ar    |7207401 |26  |\n|1062806  |ar    |6841262 |50  |\n|168195436|ar    |12808784|22  |\n|1685126  |ar    |59582   |26  |\n|168286187|ar    |4188404 |2   |\n|37025504 |ar    |481552  |733 |\n|168478031|ar    |9822502 |4   |\n|168406030|ar    |909773  |7   |\n|168410987|ar    |5425869 |10  |\n|168511270|ar    |6817428 |68  |\n|168115240|ar    |23665227|2   |\n|168396372|ar    |4276822 |10  |\n|168417737|ar    |5383328 |19  |\n|1062806  |ar    |20870989|73  |\n|168373631|ar    |7202991 |60  |\n|168335848|ip    |4112638 |15  |\n|37025504 |ar    |1108956 |76  |\n|168453430|ar    |1705363 |1   |\n+---------+------+--------+----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "download_table.select('uid').distinct().count()", 
            "metadata": {}, 
            "execution_count": 143, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "242243"
                    }, 
                    "metadata": {}, 
                    "execution_count": 143, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "valid_user.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    \nvalid_freq.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_freq.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 4. Visualize the data\n###  TODO\nUsing Seaborn and matplotlib", 
            "cell_type": "markdown"
        }, 
        {
            "source": "!pip install seaborn", 
            "metadata": {}, 
            "execution_count": 164, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): seaborn in /usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages\r\n"
                }
            ]
        }, 
        {
            "source": "import seaborn as sns\n%matplotlib inline", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "import pandas as pd\nfreqPandas = valid_freq.toPandas()\ndlPandas = download_table.toPandas()\nsns.lmplot(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "plot matrix for song played by user, with frequency as the hue ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "min(freqPandas['uid'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sns.palplot(sns.diverging_palette(10, 133, sep=80, n=10))\n# user_id: max 100047599, min 99983627\n# song_id: max 2147483647, min 2\nsns.(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 5. Build the recommender system\n(Cited from IBM bluemix Data Science Experience (DSX) document) <br>\n\n\"\nCollaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who have similar preference on the same item will also have similar opinions on items that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users versus all songs. Most cells in the matrix will be empty. An empty cell means the user hasn't played the song yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for song factors and solving for user factors.\n\"\n\n\nChallenge in this recommender system: <br>\n1. Small number of play history that could be shown by sparsity of the utility matrix\n2. Limited features for songs - mixed language\n\nSolution: <br>\n1. Hybrid", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# restarting kernel\nreload = True", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# load saved files if starting from middle\nif reload: \n    song_unique = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_song.csv'))\n    song_unique.take(5)\n    \n    valid_freq = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_freq.csv'))\n    valid_freq.take(5)\n    \n    valid_user = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    valid_user.take(5)\n    \n    download_table = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'clean_download.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.printSchema()\nsong_unique.count()", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\n"
                }, 
                {
                    "data": {
                        "text/plain": "1559987"
                    }, 
                    "metadata": {}, 
                    "execution_count": 5, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "valid_freq = valid_freq.withColumn('uid', valid_freq['uid'].cast('integer'))\nvalid_freq = valid_freq.withColumn('song_id', valid_freq['song_id'].cast('integer'))\nvalid_freq = valid_freq.withColumn('freq', valid_freq['freq'].cast('double'))\n\nvalid_user = valid_user.withColumn('uid', valid_user['uid'].cast('integer'))\n\nsong_unique = song_unique.withColumn('song_type', song_unique['song_type'].cast('integer'))\nsong_unique = song_unique.withColumn('song_id', song_unique['song_id'].cast('integer'))\n\nvalid_download = download_table.withColumn('uid', download_table['uid'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\n\n# valid_freq = valid_freq.withColumn('label',log(10.0, valid_freq.freq))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.createOrReplaceTempView('song_unique')\nvalid_freq.createOrReplaceTempView('valid_freq')\nsong_freq = spark.sql(\"\"\"\n    select s.song_type, s.song_id, s.song_name, s.singer, COALESCE(f.cnt,0) as freq\n    from song_unique as s\n    left join\n        (select song_id, count(*) as cnt from valid_freq group by song_id) as f\n        on f.song_id = s.song_id\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# take log of the frequency and scale to -1 to 100 representing degree of preference \n\nvalid_freq2 = valid_freq.withColumn('label',cround(log(10.0, valid_freq.freq)/3.0*10,0))\nvalid_freq2 = valid_freq2.replace(0, -1, subset=['label'])\n# should be None \nvalid_freq2 = valid_freq2.na.fill(0, subset=['label'])\n\n# print(valid_freq2.groupBy().max('label').show())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# combine with download table\n\nvalid_download = valid_download.withColumn('download', lit(1))\nvalid_download = valid_download.drop('device')\nvalid_download = valid_download.drop('song_name')\n\nvalid_score = valid_freq2.join(valid_download, ['uid','song_id'], 'left_outer')\nvalid_score = valid_score.na.fill(0, subset=['download'])\n\nvalid_score = valid_score.withColumn('label', when(valid_score.download==1, lit(10)).otherwise(valid_score.label))\n\nvalid_score = valid_score.replace(-0.5, -1, subset=['label'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Jump to other sections: \n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_unique.printSchema()\nvalid_user.printSchema()\nvalid_freq.printSchema()\ndownload_table.printSchema()", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: string (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "valid_freq.printSchema()\nvalid_user.printSchema()\nsong_unique.printSchema()\nvalid_download.printSchema()", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "##### checking\n# check sparsity \ncounts_freq = valid_freq.count()\nprint ('Number of song frequency entried: ', counts_freq)\n# songs played/( songs x users)\npercentage = (counts_freq)*1.0/1559987/264708\nprint ('Percentage of song played: ', percentage)", 
            "metadata": {}, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of song frequency entried:  32801295\nPercentage of song played:  7.943336195316835e-05\n"
                }
            ]
        }, 
        {
            "source": "##### checking\n\nmoreThanOnce = valid_freq.groupBy('song_id').count().alias('cnt').where(col('count') > 1).select('song_id').count()\nprint ('Number of songs played more than once: ', moreThanOnce)\n# significantly less, only recommend songs played more than once\n# utility matrix will be less sparse\n# number of song frequency entries will be reduce by 686993(only played once) -32114102, which is 2%", 
            "metadata": {}, 
            "execution_count": 28, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs played more than once:  872994\n"
                }
            ]
        }, 
        {
            "source": "print ('Percentage of filled matrix: ', round((32801295-686993)*100.0/(872994*264708),4))\n# vs. ~0.0073% filled ", 
            "metadata": {}, 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Percentage of filled matrix:  0.0139\n"
                }
            ]
        }, 
        {
            "source": "song_freq.printSchema()\nsong_freq.count()", 
            "metadata": {}, 
            "execution_count": 26, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- freq: long (nullable = false)\n\n"
                }, 
                {
                    "data": {
                        "text/plain": "1559987"
                    }, 
                    "metadata": {}, 
                    "execution_count": 26, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "##### checking\n\nprint (song_freq.select('song_id').count())\nprint (song_freq.select('song_id').where(col('freq')>1).count())\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "1559987\n872994\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.1 Setup training and test set\n\nsetup: 80% training and 20% test set ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "(trainingFreq, testFreq) = valid_score.randomSplit([80.0, 20.0])\n\ntrainingFreq.printSchema()\n# utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns]).show()\ntestFreq.printSchema()", 
            "metadata": {}, 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.2 Setup collaborative filtering model\n\nAccoding to DSX document again:\n\"\nA NaN result is due to [SPARK-14489](https://issues.apache.org/jira/browse/SPARK-14489) and because the model can't predict values for users for which there's no data.\"", 
            "cell_type": "markdown"
        }, 
        {
            "source": "model = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\").fit(trainingFreq)", 
            "metadata": {}, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "KeyboardInterrupt", 
                    "output_type": "error", 
                    "evalue": "", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-17-f416e09da5db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"song_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratingCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ]
        }, 
        {
            "source": "predictions = model.transform(testFreq)\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))", 
            "metadata": {}, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The root mean squared error for our model is: 2.917168466920176\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.3 Tune parameters\n\nCross validation and grid search to tune for hyperparameters\n\nALS algorithm :\n```python\n    class pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )\n```\n\nThe ALS hyperparameters are:\n- `rank` = the number of latent factors in the model\n- `maxIter` = the maximum number of iterations \n- `regParam` = the regularization parameter", 
            "cell_type": "markdown"
        }, 
        {
            "source": "(trainingScore, validationScore) = valid_score.randomSplit([90.0, 10.0])\n\nals = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n\n# paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 3, 5, 7]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,2,3]).addGrid(als.regParam, [0.05, 0.1]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = crossval.fit(trainingScore)\npredictions = cvModel.transform(validationScore)\n\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.drop())))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "Py4JJavaError", 
                    "output_type": "error", 
                    "evalue": "An error occurred while calling o3168.fit.\n: org.apache.spark.SparkException: Job 141 cancelled as part of cancellation of all jobs\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1382)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:694)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:464)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:363)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-16-633e8eca8475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcrossval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossValidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparamGrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m                 \u001b[1;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3168.fit.\n: org.apache.spark.SparkException: Job 141 cancelled as part of cancellation of all jobs\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1382)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:722)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:722)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:694)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:464)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:363)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n"
                    ]
                }
            ]
        }, 
        {
            "source": "print ('Best rank is: ', cvModel.bestModel.rank)\nprint ('Best regularizer is: ', cvModel.bestModel.params)\n# evaluate with 0 for null prediction\n# print (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))\n# The root mean squared error for our model is: 3.1921429126212857", 
            "metadata": {}, 
            "execution_count": 136, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Best rank is:  1\nBest regularizer is:  []\n"
                }
            ]
        }, 
        {
            "source": "valid_score.printSchema()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "predictions.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'predictions_CF_1015.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 5.4 Evaluate recommendation results\n\n- MSE is low -> recommender seems to be pretty good\n- However, looking at recommended songs for individual users, it seems to recommending weird songs\n- Therefore, need alternative for inactive users for a more explainable model\n\nDetailed evaluation as following:", 
            "cell_type": "markdown"
        }, 
        {
            "source": "utility_matrix_small = valid_user.crossJoin(song_freq.select('song_id')).select('uid','song_id')\nutility_matrix_small = utility_matrix_small.join(valid_score, ['uid', 'song_id'], 'left_outer').select('uid', 'song_id', 'label')\n\nutility_matrix = utility_matrix_small.na.fill('0', subset=['label'])\n\n# Replace predicted NaN values with the average frequency and evaluate the model\n# avgScore = utility_matrix.select('label').groupBy().avg().first()[0]\n# print (\"The average score in the dataset is: \" + str(avgScore))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "predictions.createOrReplaceTempView('pred_subset')\npred_first20 = spark.sql('select uid, song_id, label,download, prediction from pred_subset order by uid limit 100').show()", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------+-----+--------+-----------+\n|  uid| song_id|label|download| prediction|\n+-----+--------+-----+--------+-----------+\n|12333|21596231|  1.0|       0|  2.4781432|\n|12333|   55219|  3.0|       0|  1.4091263|\n|12333|  708667|  4.0|       0|  3.9749548|\n|12333| 2725093|  3.0|       0|  4.3128133|\n|12333| 5114569|  6.0|       0|   4.895308|\n|36816| 6906526| -1.0|       0|   4.813818|\n|60183| 3627946| -1.0|       0|-0.13519855|\n|60183|  116329| -1.0|       0| -0.2599472|\n|60183|23610522| -1.0|       0| -0.7154152|\n|60183|20866010|  2.0|       0| 0.11216657|\n|60183| 1242385|  1.0|       0|-0.47428238|\n|60183|  134967|  1.0|       0| 0.13887726|\n|60183|21234365| -1.0|       0|-0.37378824|\n|60183| 6227103| -1.0|       0| 0.34997967|\n|60183|  710442| -1.0|       0|-0.09907829|\n|60183| 6128890| -1.0|       0|-0.11929984|\n|60183|  223973|  1.0|       0| 0.30912238|\n|60183|  981246| -1.0|       0| -0.6802237|\n|60183|23641903| -1.0|       0| 0.11194147|\n|60183|  706554| -1.0|       0| 0.29457986|\n+-----+--------+-----+--------+-----------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 44, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print (\"Recommendations for user 169262317:\")\n# recommendCF(model, 169262317, 10)\n# print \"Recommendations for user 471:\"\n# recommendCF(cvModel, 471, 10)\n# print \"Recommendations for user 496:\"\n# recommendCF(cvModel, 496, 10)\n\nprint (\"Recommendations for user 12333:\")\nrecommendCF(model, 12333, 10)", 
            "metadata": {}, 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Recommendations for user 169262317:\nRecommendations for user 12333:\n"
                }, 
                {
                    "ename": "NameError", 
                    "output_type": "error", 
                    "evalue": "name 'model' is not defined", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-14-722855a59612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Recommendations for user 12333:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mrecommendSongsAlt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12333\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
                    ]
                }
            ]
        }, 
        {
            "source": "# total user:264708 \n# select 12333\nselect_valid = valid_score.createOrReplaceTempView('select_valid')\nselected = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_valid\n    where uid = 12333\n    group by uid\n    order by 2 asc \n\"\"\")\n# inactive_user.count()\n# treshold at 1: 68268 -> around a quarter \n# treshold at 5: 154568 -> more than half \n# treshold at 2: 98849 -> around one third", 
            "metadata": {}, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "98849"
                    }, 
                    "metadata": {}, 
                    "execution_count": 40, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "selected.show()\n\nprint (\"Recommendations for user 12333:\")\nrecommendCF(model, 12333, 10)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "inactive_user.show()", 
            "metadata": {}, 
            "execution_count": 37, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+---+\n|      uid|cnt|\n+---------+---+\n|168984483|  1|\n|168275548|  1|\n|168572441|  1|\n|168532375|  1|\n|168510789|  1|\n|168713236|  1|\n|168686527|  1|\n|168683346|  1|\n|167801928|  1|\n|168173268|  1|\n|168889990|  1|\n|168779601|  1|\n|169042674|  1|\n|168266657|  1|\n|167640755|  1|\n|168029402|  1|\n|168767795|  1|\n|167579971|  1|\n|168902557|  1|\n|168975045|  1|\n+---------+---+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "print (\"Recommendations for user 167579971:\")\nrecommendCF(model, 167579971, 10)\n\nprint (\"Recommendations for user 168975045:\")\nrecommendCF(model, 168975045, 10)\n\n# 169042674 0\n# 167579971 10\n# 169042674 0\n# 167640755 10\n\n\n# valid_score.select().where(col('uid')==169042674).show()\n# problem is it's not in the user list", 
            "metadata": {}, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Recommendations for user 167579971:\n+--------+-----------------+----------------+----------+\n|song_id |song_name        |singer          |prediction|\n+--------+-----------------+----------------+----------+\n|11109921|\u514b\u535c\u52d2 (Cover)      |\u90d1\u5c0f\u5b87             |3.387147  |\n|5009810 |\u8302\u540d\u8fd9\u573a\u96e8            |\u79e6\u9f50              |3.3330908 |\n|22399046|\u9f13\u4e0e\u82b1              |\u8427\u5fc6\u60c5Alex         |3.2971313 |\n|7190353 |\u5ff5\u5ff5\u4e0d\u5fd8\u7684\u59d1\u5a18(1\u520602\u79d2\u94c3\u58f0\u7248)|\u963f\u6743              |3.035895  |\n|413840  |Ich verzeih' Dir |Veronika Fischer|2.9503236 |\n|6110029 |\u660e\u5929(37\u79d2\u94c3\u58f0\u7248)       |\u8427\u4e9a\u8f69             |2.6847723 |\n|22827582|Come Together    |Michael Jackson |2.626977  |\n|2697950 |\u5728\u96e8\u4e2d\u6f2b\u6b65            |\u4ed8\u5a1c              |2.5599701 |\n|366974  |\u96ea\u67d3\u7684\u98ce\u91c7            |\u9648\u8bfa              |2.257006  |\n|6922682 |\u8bf7\u4f60\u50cf\u6211\u8fd9\u6837\u505a-\u8ddf\u6211\u6765-(\u7ea2\u679c\u679c)|\u513f\u7ae5\u6545\u4e8b            |2.2517805 |\n+--------+-----------------+----------------+----------+\n\nRecommendations for user 168975045:\n+--------+-------------------------+----------------+----------+\n|song_id |song_name                |singer          |prediction|\n+--------+-------------------------+----------------+----------+\n|81330   |\u5411\u65e5\u8475\u7684\u82b1\u5b63                   |\u4fdd\u5251\u950b             |12.676041 |\n|1240655 |Go In, Go Hard           |Wretch 32       |10.050616 |\n|1240642 |Bright Lights            |\u7535\u5f71\u539f\u58f0            |10.050616 |\n|15145086|\u5fae\u7b11\u7684\u5411\u65e5\u8475(\u4f34\u594f\u7248)              |BEJ48           |9.34867   |\n|13165006|??? ???                  |\u88f4\u79c0\u667a             |9.190221  |\n|13967998|And...???                |\u767d\u667a\u82f1             |9.190221  |\n|859831  |free storm               |\u88f4\u6da9\u742a             |9.002642  |\n|413840  |Ich verzeih' Dir         |Veronika Fischer|8.934969  |\n|6199318 |\u8d24\u826f(39\u79d2\u94c3\u58f0\u7248)               |\u82cf\u9633              |8.810483  |\n|5725764 |\u571f\u8c6a\u624b\u673a\u94c3\u58f0 \u5de7\u5999\u5236\u4f5c Www.Y2002.Com|\u7f51\u7edc\u6b4c\u624b            |8.637566  |\n+--------+-------------------------+----------------+----------+\n\n"
                }
            ]
        }, 
        {
            "source": "dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n# Create a Spark DataFrame with the movies that have already been rated by this user\nsongsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n# Apply the recommender system to the data set without the already rated movies to predict ratings\npredictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n# Join with the movies DataFrame to get the movies titles and genres\nrecommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction)\n\nrecommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 6. Hybrid recommender system\n\nsupplemnt the collaborative filtering with item-item recommender <br>\nfor user with only 1 song played history (or two):<br>\n- recommend songs based on cosine similarity to the song/songs played by the user \n- (not implemented) for user with 0 songs player: recommend top k songs  \n\n- tries implementing on all 800k songs, failed after countless trial\n- eventually turning to subsample ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from pyspark.mllib.linalg import Vectors\n# Package for distributed linear algebra DIMSUM\n# Dimension Independent Matrix Square using MapReduce\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.types \\\nimport ArrayType, StringType, DoubleType, StructType, StructField\n\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.mllib.linalg.distributed \\\nimport IndexedRowMatrix,IndexedRow, RowMatrix, BlockMatrix,CoordinateMatrix\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 6.1 Setup feature vector\n\nGet feature vectors for top 500 songs and save it for future use; it's okay to run ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_freq = song_freq.where(col('freq')>1).orderBy('freq', ascending=False)\n\ntop_songs = song_freq.orderBy('freq', ascending =False).limit(500)\ntop_songs = top_songs.na.fill('0', subset=['singer','song_name'])\n\ntop_songs = top_songs.withColumn('id', monotonically_increasing_id())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "if not reload:\n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# convert to VectorAssembler \nfrom pyspark.ml.feature import VectorAssembler\n\ndef extract(row):\n    return (row.song_id, row.song_type, ) + tuple(row.name_vec.toArray().tolist()) + tuple(row.singer_vec.toArray().tolist())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_freq.count()", 
            "metadata": {}, 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "872994"
                    }, 
                    "metadata": {}, 
                    "execution_count": 16, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "reload", 
            "metadata": {}, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "True"
                    }, 
                    "metadata": {}, 
                    "execution_count": 17, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# get a dataframe of vector\nif not reload:\n    \n    song_str2arr = top_songs.withColumn('singer_arr', split(col(\"singer\"), \" \").cast(ArrayType(StringType())).alias(\"singer_arr\"))\n    w2v= Word2Vec(vectorSize=3, minCount=0, inputCol=\"singer_arr\", outputCol=\"singer_vec\")\n    top_song_vec = w2v.fit(song_str2arr).transform(song_str2arr)\n\n    top_song_vec = top_song_vec.withColumn('name_arr', split(col(\"song_name\"), \" \").cast(ArrayType(StringType())).alias(\"name_arr\"))\n    w2v = Word2Vec(vectorSize=5, minCount=0, inputCol=\"name_arr\", outputCol=\"name_vec\")\n    top_song_vec = w2v.fit(top_song_vec).transform(top_song_vec)\n    \n    top_vec_temp = top_song_vec.select('song_type','name_vec','singer_vec', 'song_id')\n    # save as dataframe \n    top_vec_df = top_vec_temp.rdd.map(extract).toDF()\n    top_vec_df = top_vec_df.withColumn(\"id\", monotonically_increasing_id())\n    top_vec_df = top_vec_df.withColumn('_1', top_vec_df['_1'].cast('double'))\n    \n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500.csv'))\n\n    top_vec_df.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500_vec.csv'))\n    \n\nelse:\n    top_vec_df = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_vec.csv'))    \n    \n    sim_df = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_simi.csv'))    \n    ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sim_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferschema\", \"true\")\\\n  .load(bmos.url('musicrecommendation', 'top_500_simi.csv')) ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Jump to other sections: \n- [Recommend for selected users](#6.3.-Recommend for selected users)", 
            "cell_type": "markdown"
        }, 
        {
            "source": "top_vec_df.printSchema()", 
            "metadata": {}, 
            "execution_count": 25, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: integer (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- _10: double (nullable = true)\n |-- id: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "top_songs.show()", 
            "metadata": {}, 
            "execution_count": 19, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+--------+--------------------+-----------+------+\n|song_type| song_id|           song_name|     singer|  freq|\n+---------+--------+--------------------+-----------+------+\n|        2|15249349|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|    \u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|        2| 9950164|               \u521a\u597d\u9047\u89c1\u4f60|        \u674e\u7389\u521a|102218|\n|        2| 5237384|                \u9006\u6d41\u6210\u6cb3|        \u91d1\u5357\u73b2| 71834|\n|        2| 6468891|                  \u6f14\u5458|        \u859b\u4e4b\u8c26| 68042|\n|        2|15807836|\u4e09\u751f\u4e09\u4e16-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843...|         \u5f20\u6770| 58433|\n|        2| 5114569|          \u6ca1\u6709\u4f60\u966a\u4f34\u771f\u7684\u597d\u5b64\u5355|         \u68a6\u7136| 53102|\n|        2| 3287564|                 \u5c0f\u65f6\u5019|        \u82cf\u6253\u7eff| 52403|\n|        2|16400733|\u601d\u6155-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|        \u90c1\u53ef\u552f| 48343|\n|        2| 6657692|             \u8d70\u7740\u8d70\u7740\u5c31\u6563\u4e86|        \u5e84\u5fc3\u598d| 47438|\n|        2| 3620537|              \u4f60\u8fd8\u8981\u6211\u600e\u6837|        \u859b\u4e4b\u8c26| 43961|\n|        2| 7149583|                \u544a\u767d\u6c14\u7403|        \u5468\u6770\u4f26| 41887|\n|        2| 6749207|               Faded|Alan Walker| 38523|\n|        2| 3971731|               \u4ee5\u540e\u7684\u4ee5\u540e|        \u5e84\u5fc3\u598d| 38138|\n|        1|23498554|                \u52a8\u7269\u4e16\u754c|        \u859b\u4e4b\u8c26| 37840|\n|        2|23082492|                  \u9ad8\u5c1a|        \u859b\u4e4b\u8c26| 37762|\n|        2| 4147754|  \u4e11\u516b\u602a-(\u7535\u89c6\u5267\u300a\u5982\u679c\u6211\u7231\u4f60\u300b\u63d2\u66f2)|        \u859b\u4e4b\u8c26| 34504|\n|        2|13273544|                \u52c9\u4e3a\u5176\u96be|         \u738b\u5195| 33689|\n|        1|16827761|                  \u6210\u90fd|         \u8d75\u96f7| 33348|\n|        2| 6449273|                  \u7ec5\u58eb|        \u859b\u4e4b\u8c26| 32905|\n|        1| 7176392|                  \u6210\u90fd|         \u8d75\u96f7| 31211|\n+---------+--------+--------------------+-----------+------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "top_songs.filter(col('id') == 315).show()", 
            "metadata": {}, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+-------+---------+------+----+---+\n|song_type|song_id|song_name|singer|freq| id|\n+---------+-------+---------+------+----+---+\n|        2| 203139|      \u84dd\u83b2\u82b1|    \u8bb8\u5dcd|7257|315|\n+---------+-------+---------+------+----+---+\n\n"
                }
            ]
        }, 
        {
            "source": "top_vec_df.printSchema()", 
            "metadata": {}, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: double (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- id: long (nullable = false)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 6.2 Compute similarity matrix\n#### using pyspark's distributed matrix\n\nunderstanding the goal: is to get single row of similarities when needed <br>\nproblem: \n\np1: columnSimilarities: need to transpose the matrix -> using toBlockMatrix().transpose() results in absurdly large matrix<br>\nSolved: manually transpose at DataFrame<br>\n\np2: distributed container does not allow random access to computed similarity matrix <br>\nTrial and Error:\n- to RDD? takes a long time, and very likely, does not have enough memory for a local matrix\n- do matrix multiplication for BlockMatrix? takes a long time\n- broadcast to different workers, couldn't do patten match from csr_matrix \n- reduced to 872994", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# manuely transpose the dataframe - 20s\ntop_vec_rdd = top_vec_df.rdd\ndata = []\nfor i in range(9):\n    data.append(top_vec_rdd.map(lambda row: row[i]).collect())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# manurally construct IndexedRow\n# indexedT = IndexedRow(0, Vectors.dense(data[0]))\n\ntoIndexedRows = [IndexedRow(i, data[i]) for i in range(9)]\nindexedRows = sc.parallelize(toIndexedRows)\nmat = IndexedRowMatrix(indexedRows)\n\nrowMat = mat.toRowMatrix()\n\n# usualy takes long, but when u'r luck: 40s\nsimThres = rowMat.columnSimilarities(0.05)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "top_song_vec.printSchema()", 
            "metadata": {}, 
            "execution_count": 45, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = false)\n |-- singer: string (nullable = false)\n |-- freq: long (nullable = false)\n |-- id: long (nullable = false)\n |-- singer_arr: array (nullable = false)\n |    |-- element: string (containsNull = true)\n |-- singer_vec: vector (nullable = true)\n |-- name_arr: array (nullable = false)\n |    |-- element: string (containsNull = true)\n |-- name_vec: vector (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "# convert to matrix \n\n# mat = IndexedRowMatrix(data.map(lambda row:IndexedRow(monotonically_increasing_id(), Vectors(list(row)))\n# song_mat_temp = IndexedRowMatrix(song_vec_mod.rdd.map(lambda row: IndexedRow(row['id'], Vectors.dense(row[:9]))))\n\n# it will return an rdd of indexed row, need to extract \ndef extractRows(row):\n#     count = count+1\n    return (row.index, ) + tuple(row.vector.toArray().tolist())\n\n# save as dataframe \n# count = 0\n\ncolumnName = [str(i+1) for i in range(500)]\ncolumnName = ['id'] + columnName\nsim_df = simRdd.map(extractRows).toDF(columnName)", 
            "metadata": {}, 
            "execution_count": 48, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "ValueError", 
                    "output_type": "error", 
                    "evalue": "RDD is empty", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-48-6540777b41bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcolumnName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcolumnName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcolumnName\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msim_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextractRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RDD is empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misEmpty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mValueError\u001b[0m: RDD is empty"
                    ]
                }
            ]
        }, 
        {
            "source": "print(simThres.numCols(), simThres.numRows())", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 47, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "500 500\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 6.3 Recommend for selected users", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# filter users: listened to only one song before, but more than once\n\nselect_score= valid_score.createOrReplaceTempView('select_score')\ninactive_user = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_score\n    group by uid\n    having cnt = 1\n    order by 2 asc \n\"\"\")\n\ninactive_user = inactive_user.createOrReplaceTempView('inactive_user')\n\nselect_pos = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label > -1\n\"\"\")\n\nselect_neg = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label = -1\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 54, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "select_pos.join(song_freq, 'song_id').orderBy(song_freq.freq, ascending = False).show()", 
            "metadata": {}, 
            "execution_count": 134, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+----+-----+---------+--------------------+-------+------+\n| song_id|      uid|freq|label|song_type|           song_name| singer|  freq|\n+--------+---------+----+-----+---------+--------------------+-------+------+\n|15249349|168965732| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168848103| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168597540| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168028168| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168826351|10.0| 10.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168258482| 4.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168916514| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167764401| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167639000| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168149928| 0.0|  0.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168139018|24.0|  5.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167896511| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168930543| 2.0| 10.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168204433| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168335651| 4.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167900079| 9.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168364536| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167972265| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168914581| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168906776|14.0|  4.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n+--------+---------+----+-----+---------+--------------------+-------+------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "select_pos.printSchema()", 
            "metadata": {}, 
            "execution_count": 57, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n\n"
                }
            ]
        }, 
        {
            "source": "select_pos.count()\nselect_neg.count()", 
            "metadata": {}, 
            "execution_count": 55, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "49455"
                    }, 
                    "metadata": {}, 
                    "execution_count": 55, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "user =  168965732\nnbRecommendations = 10\n\npos_user = select_pos.filter(col('uid')==user)\nsong = pos_user.select('song_id').collect()[0].song_id\n\nsong = 15249349\nsong_detail = song_freq.filter(col('song_id')==song)\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 81, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "pos_user.show()", 
            "metadata": {}, 
            "execution_count": 132, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+-------+----+-----+\n|     uid|song_id|freq|label|\n+--------+-------+----+-----+\n|60683192|6694268| 6.0|  3.0|\n+--------+-------+----+-----+\n\n"
                }
            ]
        }, 
        {
            "source": "# song_freq.take(1)\n# [Row(song_type=2, song_id=15249349, song_name='\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b\u7247\u5c3e\u66f2)', singer='\u5f20\u78a7\u6668&\u6768\u5b97\u7eac', freq=109798)]\ntop_songs.take(1)", 
            "metadata": {}, 
            "execution_count": 99, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(song_type=2, song_id=15249349, song_name='\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b\u7247\u5c3e\u66f2)', singer='\u5f20\u78a7\u6668&\u6768\u5b97\u7eac', freq=109798, id=0)]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 99, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "type(pos_user)", 
            "metadata": {}, 
            "execution_count": 84, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "metadata": {}, 
                    "execution_count": 84, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "song = 15249349\nfind_song = top_songs.filter(col('song_id')==song)\nsong_index = find_song.select('id').collect()[0].id", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 101, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_index", 
            "metadata": {}, 
            "execution_count": 135, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "0"
                    }, 
                    "metadata": {}, 
                    "execution_count": 135, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "song_sim = sim_df.select(str(song_index+1), 'id')\nrecommendations = song_sim.join(top_songs, song_sim.id == top_songs.id).select(top_songs.song_id, top_songs.song_name, top_songs.singer, str(song_index+1)).orderBy(str(song_index+1), ascending=False).limit(nbRecommendations)\nrecommendations.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 136, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------------------------+---------+---+\n|song_id |song_name                  |singer   |1  |\n+--------+---------------------------+---------+---+\n|203139  |\u84dd\u83b2\u82b1                        |\u8bb8\u5dcd       |0.0|\n|451678  |Booty Music                |Deep Side|0.0|\n|385834  |\u85d5\u65ad\u4e1d\u8fde                       |\u9648\u745e       |0.0|\n|6576370 |Seve (Radio Edit)          |Tez cadey|0.0|\n|7186125 |\u4e3a\u4f60\u5199\u8bd7                       |\u5434\u514b\u7fa4&\u62d6\u978b   |0.0|\n|3972100 |\u9047\u89c1                         |\u5b59\u71d5\u59ff      |0.0|\n|7175354 |\u5982\u679c\u6ca1\u9047\u89c1\u4f60                     |\u9f99\u6885\u5b50      |0.0|\n|23362650|\u6211\u662f\u4f60\u7684\u98ce-(\u52a8\u753b\u7535\u5f71\u300a\u51b0\u96ea\u5927\u4f5c\u6218\u300b\u4e2d\u6587\u5ba3\u4f20\u66f2)   |\u5b89\u5a1c&\u8bfa\u4e00&\u5218\u9713\u5a1c|0.0|\n|3970170 |\u9189\u76f8\u601d                        |\u7941\u9686       |0.0|\n|7175427 |\u4e3a\u4ec0\u4e48\u6211\u597d\u60f3\u544a\u8bc9\u4ed6\u6211\u662f\u8c01-(\u7535\u5f71\u300a\u8c0e\u8a00\u897f\u897f\u91cc\u300b\u4e3b\u9898\u66f2)|\u5f20\u78a7\u6668      |0.0|\n+--------+---------------------------+---------+---+\n\n"
                }
            ]
        }, 
        {
            "source": "import pandas as pd\nsim_pd = sim_df.toPandas()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 189, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "test = sim_pd['4']\ntest", 
            "metadata": {}, 
            "execution_count": 190, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "0      0.000000\n1      0.000000\n2      0.000000\n3      0.000000\n4      0.000000\n5      0.000000\n6      0.000000\n7      0.000000\n8      0.000000\n9      0.000000\n10     0.000000\n11     0.000000\n12     0.000000\n13     0.000000\n14     0.000000\n15     0.000000\n16     0.000000\n17     0.000000\n18     0.000000\n19     0.000000\n20     0.000000\n21     0.000000\n22     0.000000\n23     0.000000\n24     0.000000\n25     0.000000\n26     0.000000\n27     0.977922\n28     0.000000\n29     0.000000\n         ...   \n470    0.000000\n471    0.000000\n472    0.000000\n473    0.000000\n474    0.000000\n475    0.000000\n476    0.000000\n477    0.000000\n478    0.000000\n479    0.000000\n480    0.000000\n481    0.000000\n482    0.000000\n483    0.000000\n484    0.000000\n485    0.000000\n486    0.000000\n487    0.000000\n488    0.000000\n489    0.000000\n490    0.000000\n491    0.000000\n492    0.000000\n493    0.000000\n494    0.000000\n495    0.000000\n496    0.000000\n497    0.000000\n498    0.000000\n499    0.000000\nName: 4, dtype: float64"
                    }, 
                    "metadata": {}, 
                    "execution_count": 190, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "indx = str(song_index+1)\n\ntest_vec = [max(sim_pd[indx][i], sim_pd[str(i+1)][song_index]) for i in range(500)]", 
            "metadata": {}, 
            "execution_count": 193, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "test_vec", 
            "metadata": {}, 
            "execution_count": 194, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.97792185490407757,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.98834370800715821,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.9917360354405117,\n 0.99679019804929458,\n 0.99130301653685038,\n 0.97766744423101937,\n 0.9915212225603478,\n 0.99092596571087044,\n 0.98039886587527714,\n 0.99037037369337133,\n 0.99446124306319994,\n 0.99552551516918375,\n 0.99024251759904791,\n 0.99757164893673966,\n 0.9944187776410397,\n 0.95829034029983595,\n 0.99511736534458439,\n 0.9922439801672247,\n 0.98841299472189903,\n 0.98351302805293161,\n 0.98569337464533835,\n 0.98555938152297617,\n 0.99862385984090407,\n 0.99461691663347829,\n 0.98750029522074922,\n 0.99464552730174904,\n 0.96027127175880467,\n 0.96344461527960845,\n 0.99361949380053793,\n 0.99051097510751385,\n 0.98609520076411039,\n 0.99316768285807999,\n 0.99864133038893055,\n 0.9898759651159269,\n 0.97728350839221401,\n 0.99400758105519194,\n 0.99105592411786958,\n 0.96312602167808326,\n 0.99168273831290144,\n 0.97390930248569063,\n 0.97339615988171901,\n 0.9865287348242131,\n 0.99725749652029771,\n 0.98330531103665064,\n 0.98402416888064204,\n 0.97529145526225758,\n 0.99505883224201797,\n 0.98236053653593169,\n 0.9840166454896967,\n 0.99726487069197634,\n 0.97945410484566842,\n 0.99179516505724596,\n 0.99236451460977293,\n 0.99523148083901436,\n 0.98881053393654272,\n 0.98960613894292315,\n 0.97563155407706315,\n 0.99303454957912551,\n 0.98598991267351599,\n 0.99143611949777499,\n 0.99287474754842009,\n 0.98979864106636983,\n 0.96550384997651262,\n 0.96754802200142742,\n 0.99168468692191969,\n 0.98422855245981444,\n 0.99135538680058288,\n 0.9540446687407993,\n 0.99464397808482674,\n 0.98550330428973665,\n 0.99598545120724102,\n 0.99361932975888734,\n 0.99028011860161336,\n 0.99683327773212993,\n 0.99484770583472149,\n 0.98690255812986771,\n 0.99425591442858907,\n 0.9889376499937429,\n 0.96313271771696995,\n 0.96601335483107131,\n 0.98583964931208445,\n 0.98483961085834104,\n 0.98964872003336091,\n 0.99135157846230304,\n 0.98036821787035389,\n 0.99278146554830571,\n 0.9922356216393009,\n 0.98719773631578933,\n 0.98930440053257063,\n 0.98831151931858141,\n 0.99731505579587998,\n 0.98601081553352243,\n 0.98881170043985767,\n 0.99402646647470994,\n 0.99223532872151055,\n 0.98645622428490187,\n 0.99689585264351144,\n 0.97163045983154805,\n 0.99459834485076348,\n 0.98918750745736694,\n 0.99613655208645979,\n 0.99059055466976265,\n 0.96887895796785239,\n 0.98297008161916266,\n 0.99249454141374682,\n 0.98754534375700853,\n 0.99232468340130153,\n 0.99320243374724604,\n 0.98869241351704606,\n 0.96560115317502682,\n 0.99790465347501556,\n 0.99038024607409625,\n 0.98905463668372007,\n 0.99176833464079006,\n 0.99129078824464456,\n 0.99017532122739949,\n 0.97630990701582032,\n 0.9711367191336272,\n 0.98745041889028795,\n 0.99248323653272008,\n 0.99371118944009107,\n 0.96840627387954326,\n 0.99387058765684699,\n 0.99634240959366493,\n 0.9946704703386462,\n 0.99023425875764226,\n 0.99431954203873363,\n 0.98324130249587272,\n 0.98679723187417301,\n 0.98276494380387636,\n 0.98803954789151238,\n 0.99146135257295287,\n 0.96622506599130842,\n 0.98446018402391855,\n 0.98833557365408364,\n 0.98849242836841478]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 194, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "song_index = 3\nsim_df.select(str(song_index+1), 'id').show()", 
            "metadata": {}, 
            "execution_count": 192, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+---+\n|  4| id|\n+---+---+\n|0.0|315|\n|0.0|386|\n|0.0|454|\n|0.0|365|\n|0.0|451|\n|0.0|384|\n|0.0|324|\n|0.0|180|\n|0.0|320|\n|0.0|373|\n|0.0|369|\n|0.0|408|\n|0.0|307|\n|0.0|428|\n|0.0|464|\n|0.0| 11|\n|0.0| 14|\n|0.0|466|\n|0.0|346|\n|0.0| 24|\n+---+---+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "song_sim2.transpose()[0][1:]", 
            "metadata": {}, 
            "execution_count": 218, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "1      0.000000\n2      0.000000\n3      0.000000\n4      0.000000\n5      0.000000\n6      0.991990\n7      0.996428\n8      0.990109\n9      0.991140\n10     0.983688\n11     0.989649\n12     0.996449\n13     0.992721\n14     0.961659\n15     0.985563\n16     0.981806\n17     0.990653\n18     0.956802\n19     0.984632\n20     0.956802\n21     0.970663\n22     0.997403\n23     0.984302\n24     0.990659\n25     0.981547\n26     0.990576\n27     0.982761\n28     0.978021\n29     0.983118\n30     0.978452\n         ...   \n471    0.995807\n472    0.994865\n473    0.988145\n474    0.959287\n475    0.993485\n476    0.978655\n477    0.988010\n478    0.979081\n479    0.981779\n480    0.993745\n481    0.952978\n482    0.986275\n483    0.985170\n484    0.982584\n485    0.990998\n486    0.966997\n487    0.990085\n488    0.987956\n489    0.986200\n490    0.989347\n491    0.981748\n492    0.975722\n493    0.989297\n494    0.968739\n495    0.971601\n496    0.994996\n497    0.978166\n498    0.984932\n499    0.997022\n500    0.988839\nName: 0, dtype: float64"
                    }, 
                    "metadata": {}, 
                    "execution_count": 218, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "song_sim1 = sim_df.select(str(song_index+1), 'id').toPandas()\nsong_sim2 = sim_df.filter(col('id')==str(song_index+1)).toPandas()\n\n\nprint(song1_sort)\nprint(song_sim2.transpose())\n# print(song_sim2[str(song_index)][1:])\n\nsong1_sort = song_sim1.sort('id')\nsong2_trans= song_sim2.transpose()[0][1:]\n\nfrom copy import deepcopy\ntest_vec = [max(song1_sort[str(song_index+1)][i], song2_trans[i]) for i in range(500)]\n\ntest_df = DataFrame{}", 
            "metadata": {}, 
            "execution_count": 220, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "            4   id\n247  0.988344    0\n27   0.977922    1\n444  0.985840    2\n186  0.000000    3\n138  0.000000    4\n443  0.000000    5\n431  0.000000    6\n128  0.000000    7\n129  0.000000    8\n95   0.000000    9\n236  0.000000   10\n15   0.000000   11\n33   0.000000   12\n446  0.000000   13\n16   0.000000   14\n353  0.000000   15\n207  0.000000   16\n31   0.000000   17\n404  0.000000   18\n269  0.000000   19\n405  0.000000   20\n44   0.000000   21\n54   0.000000   22\n181  0.000000   23\n19   0.000000   24\n419  0.000000   25\n196  0.000000   26\n246  0.000000   27\n325  0.000000   28\n326  0.000000   29\n..        ...  ...\n107  0.000000  470\n426  0.000000  471\n336  0.000000  472\n281  0.000000  473\n142  0.000000  474\n449  0.000000  475\n324  0.000000  476\n208  0.000000  477\n496  0.000000  478\n361  0.000000  479\n282  0.000000  480\n164  0.000000  481\n297  0.000000  482\n170  0.000000  483\n262  0.000000  484\n482  0.000000  485\n479  0.000000  486\n263  0.000000  487\n260  0.000000  488\n63   0.000000  489\n245  0.000000  490\n137  0.000000  491\n43   0.000000  492\n495  0.000000  493\n98   0.000000  494\n399  0.000000  495\n189  0.000000  496\n407  0.000000  497\n205  0.000000  498\n133  0.000000  499\n\n[500 rows x 2 columns]\n            0\nid   4.000000\n1    0.000000\n2    0.000000\n3    0.000000\n4    0.000000\n5    0.000000\n6    0.991990\n7    0.996428\n8    0.990109\n9    0.991140\n10   0.983688\n11   0.989649\n12   0.996449\n13   0.992721\n14   0.961659\n15   0.985563\n16   0.981806\n17   0.990653\n18   0.956802\n19   0.984632\n20   0.956802\n21   0.970663\n22   0.997403\n23   0.984302\n24   0.990659\n25   0.981547\n26   0.990576\n27   0.982761\n28   0.978021\n29   0.983118\n..        ...\n471  0.995807\n472  0.994865\n473  0.988145\n474  0.959287\n475  0.993485\n476  0.978655\n477  0.988010\n478  0.979081\n479  0.981779\n480  0.993745\n481  0.952978\n482  0.986275\n483  0.985170\n484  0.982584\n485  0.990998\n486  0.966997\n487  0.990085\n488  0.987956\n489  0.986200\n490  0.989347\n491  0.981748\n492  0.975722\n493  0.989297\n494  0.968739\n495  0.971601\n496  0.994996\n497  0.978166\n498  0.984932\n499  0.997022\n500  0.988839\n\n[501 rows x 1 columns]\n"
                }, 
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/site-packages/ipykernel/__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
                }, 
                {
                    "data": {
                        "text/plain": "[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.99198997386480814,\n 0.9964278596487901,\n 0.99010948892525541,\n 0.99113990310855771,\n 0.98368780838840431,\n 0.98964865686002002,\n 0.99644935456142036,\n 0.99272102123567141,\n 0.96165929021539831,\n 0.98556328178031705,\n 0.98180584332954313,\n 0.99065257175277033,\n 0.95680216798534679,\n 0.98463234612377193,\n 0.95680216798534679,\n 0.97066307601829105,\n 0.99740321586796177,\n 0.98430176373800871,\n 0.99065916304318746,\n 0.98154730187504247,\n 0.99057639566980082,\n 0.98276138458323015,\n 0.9780214032687049,\n 0.98311844684147542,\n 0.97845155118145422,\n 0.99316823148381594,\n 0.98795632520002552,\n 0.98638222055462044,\n 0.98327304239592239,\n 0.98910242106077084,\n 0.97959870047435815,\n 0.99050502034124843,\n 0.99585543428337808,\n 0.95401118145592945,\n 0.96927390722104201,\n 0.99252610291541632,\n 0.9931702913020094,\n 0.99670360646664102,\n 0.98173599326593097,\n 0.99493103516052084,\n 0.99305659551043712,\n 0.95680216798534679,\n 0.96877647049537785,\n 0.9887336674250552,\n 0.99233066817799187,\n 0.99238926434550889,\n 0.98532127518961599,\n 0.98465751916739308,\n 0.98865491992231125,\n 0.97990019600663758,\n 0.98455177913767289,\n 0.98131892095886253,\n 0.99408203123711114,\n 0.99530178960032922,\n 0.98992624601912749,\n 0.98481550472695212,\n 0.98714971852361733,\n 0.98725151867158578,\n 0.99049536788297521,\n 0.98379273011112045,\n 0.98747058083019457,\n 0.98584853796426142,\n 0.98766437844513766,\n 0.98446851961984105,\n 0.9915559249950483,\n 0.98493912825925056,\n 0.99011216680663117,\n 0.9857028095884911,\n 0.99961961594361415,\n 0.98783415238274153,\n 0.97761099463612788,\n 0.99801862839977162,\n 0.98753563580401871,\n 0.9816616182637794,\n 0.95745310252049176,\n 0.96927824069802693,\n 0.9918166213010059,\n 0.99295975421440785,\n 0.98079916045562598,\n 0.99056519436170654,\n 0.98615061609771881,\n 0.99139981472392902,\n 0.96438270321646324,\n 0.99545087794489973,\n 0.98626887525720131,\n 0.99600898082083744,\n 0.9940149924851136,\n 0.99356511086590682,\n 0.98472206738001922,\n 0.99574316842208599,\n 0.98335990154980613,\n 0.99588596422956532,\n 0.9807431454112594,\n 0.99136460684284233,\n 0.99013997129311382,\n 0.99371286352310995,\n 0.95496697119820972,\n 0.98749756234041641,\n 0.99208678352119883,\n 0.99515208938387834,\n 0.9877388299434775,\n 0.9628897673837552,\n 0.98207144039268013,\n 0.98986205636955382,\n 0.99161383587005025,\n 0.98477154502010411,\n 0.96443974913743336,\n 0.98817273518918969,\n 0.99028567182958582,\n 0.99191370114143429,\n 0.98076511846021885,\n 0.98456405279713133,\n 0.99479530701737873,\n 0.99157830953436377,\n 0.98346921383909525,\n 0.99035450361696564,\n 0.98673198289355868,\n 0.99268643416778268,\n 0.98449684449921482,\n 0.99127486002147058,\n 0.99468490073357574,\n 0.98654634696333099,\n 0.96307385433379011,\n 0.98215231533731817,\n 0.98981912918538217,\n 0.97973876859132636,\n 0.98966909670211389,\n 0.96403944765022886,\n 0.99365966349329438,\n 0.99114747376375389,\n 0.9821783647085961,\n 0.96905550255447925,\n 0.9817634362981511,\n 0.98912219790009515,\n 0.98995273011290519,\n 0.99270772329034274,\n 0.99467211118109566,\n 0.98956234665325205,\n 0.97793671912249946,\n 0.99511460931336593,\n 0.99066618590166444,\n 0.9768537495086963,\n 0.99804537958965733,\n 0.98811843569810809,\n 0.97785682636932336,\n 0.99169052305758965,\n 0.98042157015127085,\n 0.97937276217347247,\n 0.98356062113341858,\n 0.98504949887365467,\n 0.98803985283524531,\n 0.99055903054798133,\n 0.97165429945461235,\n 0.98286190998052503,\n 0.99260053649537006,\n 0.98971019024087126,\n 0.99754682603608236,\n 0.98599908350898946,\n 0.99478536510201687,\n 0.98250794642725681,\n 0.9670813282016395,\n 0.99450117066145804,\n 0.99055903054798133,\n 0.97779129885582639,\n 0.99040751423997297,\n 0.97534772032505912,\n 0.97776768547718163,\n 0.99750300675844139,\n 0.98785177610686636,\n 0.98736938161081578,\n 0.98730336143013253,\n 0.98922018383632826,\n 0.992132753834013,\n 0.99118460290579469,\n 0.98424362621492378,\n 0.991954507844651,\n 0.99023707701351649,\n 0.98814472009732801,\n 0.99141935480338861,\n 0.97915907944795366,\n 0.98186900043199565,\n 0.97418503712451088,\n 0.9955197688734807,\n 0.98271414158649317,\n 0.98577906140917371,\n 0.99384647785580826,\n 0.98719154973159595,\n 0.98936244670942142,\n 0.98592133013557925,\n 0.98696503665339241,\n 0.98795833427772128,\n 0.991411046418051,\n 0.99055917457662368,\n 0.98853813953939607,\n 0.96992959374717713,\n 0.98938086122585256,\n 0.98852137067365164,\n 0.99051873847027405,\n 0.96801577764139846,\n 0.97391720738312593,\n 0.9953460271575475,\n 0.98794306512675711,\n 0.96063799778008041,\n 0.99061154025798359,\n 0.979133379365419,\n 0.9961383054649019,\n 0.98889829348426528,\n 0.9866710628903258,\n 0.99225375474800104,\n 0.99530362768732838,\n 0.98269804481553458,\n 0.98280665900319197,\n 0.99174000778919824,\n 0.98753563580401882,\n 0.97612438495843878,\n 0.99245715321968486,\n 0.9929884684705802,\n 0.96640714303979902,\n 0.94903745004252649,\n 0.98889709654690106,\n 0.97300662041524355,\n 0.99390657336345745,\n 0.99274045422979251,\n 0.98940748128421785,\n 0.99137037720469567,\n 0.98939884048223725,\n 0.99016148148345751,\n 0.98467959366933822,\n 0.97418752260582364,\n 0.98728622954545986,\n 0.98943603619210596,\n 0.99820478965397452,\n 0.96628797999838334,\n 0.99490046347599881,\n 0.98665159153957738,\n 0.95475468644862549,\n 0.99013217818377963,\n 0.99198940463231122,\n 0.99217594186039626,\n 0.98305726462666521,\n 0.98953080696390305,\n 0.99021774257506723,\n 0.98834370800715821,\n 0.98365832154435617,\n 0.98921565885107077,\n 0.99034194407465959,\n 0.95288473460533007,\n 0.98378638668791363,\n 0.98740481861211826,\n 0.99080720828375901,\n 0.98456405279713133,\n 0.98928714967763121,\n 0.98580273824233777,\n 0.95680216798534679,\n 0.9777538076100768,\n 0.98808302481267751,\n 0.98946974741650151,\n 0.99323613169553104,\n 0.99112595673229253,\n 0.98515082991813785,\n 0.96765983021165303,\n 0.9666523388599727,\n 0.99530178960032922,\n 0.99317142118431567,\n 0.99205601001437071,\n 0.98036070248435359,\n 0.9944485132641564,\n 0.98169311020335404,\n 0.98591414908768571,\n 0.99213333109845991,\n 0.98531926777100365,\n 0.96445033272012715,\n 0.97911826375592537,\n 0.9914985202240626,\n 0.994805962895281,\n 0.98975454932296902,\n 0.99116300437268789,\n 0.99376580191429509,\n 0.98949569766506318,\n 0.97981350361477171,\n 0.96723513091968871,\n 0.98270306768994697,\n 0.98907890552419286,\n 0.96691153623973602,\n 0.97321947338577641,\n 0.0,\n 0.98133269325634809,\n 0.98283590570558488,\n 0.98584379724383941,\n 0.97843092024362299,\n 0.98107062647062904,\n 0.99060658487307529,\n 0.98294294374008884,\n 0.99473312320537044,\n 0.98747620075564624,\n 0.98289936295106917,\n 0.98127327967710121,\n 0.98431749701394611,\n 0.98726730650186278,\n 0.98673983017232825,\n 0.95998962981547287,\n 0.99150426820922932,\n 0.98810413594362534,\n 0.99030650630460093,\n 0.97454978549035076,\n 0.95809121954769982,\n 0.98230040342207747,\n 0.98961101851913402,\n 0.99264379299546823,\n 0.98379035799558312,\n 0.98285284391290573,\n 0.98760609320387704,\n 0.98236659147317329,\n 0.98282208370193647,\n 0.98364314940608877,\n 0.99386702970932783,\n 0.99437784308699273,\n 0.98600918472374777,\n 0.97959063268812496,\n 0.98293928949969145,\n 0.98799658592202888,\n 0.99442804679684593,\n 0.99349409120676202,\n 0.98347916716064177,\n 0.98695493972982584,\n 0.99160769672239513,\n 0.98058676348577301,\n 0.99665242295164636,\n 0.98400787590327421,\n 0.99314200489192328,\n 0.9892080166069106,\n 0.98501231492414121,\n 0.98316816583635103,\n 0.9627880662178967,\n 0.97277532389153865,\n 0.98982478644182337,\n 0.97365007993347208,\n 0.99614212164577987,\n 0.98762496097531816,\n 0.99108549385867395,\n 0.98916398819476659,\n 0.97574947255581623,\n 0.98518150276846861,\n 0.98854440969281443,\n 0.99161582020475192,\n 0.99352839284933825,\n 0.98236705737074603,\n 0.97443970051615458,\n 0.99048844607472708,\n 0.99463970140889435,\n 0.96217422429719124,\n 0.980900847845113,\n 0.98042011317945332,\n 0.99631727686254823,\n 0.99116535692667307,\n 0.98026363947197326,\n 0.99248296650484125,\n 0.9782524883450967,\n 0.98241594610108285,\n 0.9901546120856326,\n 0.99065027182785426,\n 0.97785935219191844,\n 0.99154047999927708,\n 0.99370901645543164,\n 0.98507416871488729,\n 0.98869928553519193,\n 0.98954689703393228,\n 0.96603640862189888,\n 0.98980894641708184,\n 0.97616502613662792,\n 0.98493635804004798,\n 0.99092814386217221,\n 0.98845368238179065,\n 0.99602387977876283,\n 0.97686841111621736,\n 0.9889258922421551,\n 0.9813206822817897,\n 0.99039712747837527,\n 0.99455592416934724,\n 0.99660922029996224,\n 0.99413751855183552,\n 0.98841269266608522,\n 0.98588206943931966,\n 0.98590058918107826,\n 0.98466997825272318,\n 0.95714015404142272,\n 0.96752236695678284,\n 0.98679604979438862,\n 0.99299422245367308,\n 0.98499085419933641,\n 0.99447397304800078,\n 0.99019648981799469,\n 0.9881911153440841,\n 0.95809121954769993,\n 0.99278695690224938,\n 0.99021633917313168,\n 0.96785701592021378,\n 0.99682826149384474,\n 0.97343671085076844,\n 0.98340724431310611,\n 0.99662329090995416,\n 0.99273828515604201,\n 0.99238926434550889,\n 0.98901119839996787,\n 0.97524763404987536,\n 0.98979139660996085,\n 0.99190182718917175,\n 0.98708285688843711,\n 0.99424558205554481,\n 0.99214156427849554,\n 0.99406017469222441,\n 0.99868677482581514,\n 0.98913835203056411,\n 0.98810320663904161,\n 0.98913313182229945,\n 0.99238365241474702,\n 0.99530747620644255,\n 0.97075557344597574,\n 0.98560493325660781,\n 0.98083359582737428,\n 0.97712353543126529,\n 0.97984066412248827,\n 0.97557097552201888,\n 0.99284697002306022,\n 0.96579542237865945,\n 0.99961961594361415,\n 0.96983954531867855,\n 0.99283929994343545,\n 0.98368320474357818,\n 0.99108085207381713,\n 0.98472331689456771,\n 0.99357632608395519,\n 0.98648415331027228,\n 0.99280918528073347,\n 0.98089839102640386,\n 0.99519822515469503,\n 0.99601574756263145,\n 0.97248768912513928,\n 0.97371020434452815,\n 0.98583964931208445,\n 0.98387832264940134,\n 0.98064298562892482,\n 0.99385703001816239,\n 0.99037236512532678,\n 0.99374414489393681,\n 0.99377486441670737,\n 0.99017906576400605,\n 0.99073558811596918,\n 0.98899845135604025,\n 0.98468206720481233,\n 0.99132046289596387,\n 0.99681694443092994,\n 0.98436964531757765,\n 0.9859609513485964,\n 0.99259916215170496,\n 0.98785395593710845,\n 0.97356035768163429,\n 0.98923380260411109,\n 0.98752312044318658,\n 0.99156316348508777,\n 0.98883593406537096,\n 0.97425659132232212,\n 0.99277207176115256,\n 0.98491596280312232,\n 0.96604525621479054,\n 0.99580675165139443,\n 0.99486531641092257,\n 0.98814472009732801,\n 0.95928725659742253,\n 0.99348533232091696,\n 0.97865549826474829,\n 0.98801018918148553,\n 0.97908068971489859,\n 0.98177851293647977,\n 0.99374463388598822,\n 0.95297843102356738,\n 0.98627544684909729,\n 0.98516990155078787,\n 0.98258354093352163,\n 0.9909975852549594,\n 0.96699678367054387,\n 0.99008532647397041,\n 0.98795559853282955,\n 0.98620012040256566,\n 0.98934672333530149,\n 0.98174838185753965,\n 0.97572236411628654,\n 0.98929725514007427,\n 0.96873898287623916,\n 0.97160115728060026,\n 0.99499622236647256,\n 0.97816589202976489,\n 0.98493172645385818,\n 0.9970224364701209,\n 0.98883907506560964]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 220, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "def recommendSim(user, song, nbRecommendations):\n    song = select_user.song_id\n#     get similarity score for this song \n    print ('User liked this song: ')\n    song_freq.filter(col('song_id')==song).show()\n    \n#   get row index for that song \n    find_song = top_songs.filter(col('song_id')==song)\n\n#     if in the top 500 songs \n    if find_song:\n        song_index = find_song.select('id').collect()[0].id\n        song_sim = sim_df.select(str(song_index+1), 'id')\n        recommendations = song_sim.join(top_songs, song_sim.id == top_songs.id).select(top_songs.song_id, top_songs.song_name, top_songs.singer).orderBy(str(song_index+1), ascending=False).limit(nbRecommendations)\n        \n        recommendations.show(truncate=False)\n        \n        \n# else: fit into the matrix, add to dataframe, get a new similarity matrix, get last row\n    else:\n        top_songs.select('song_id', 'singer', 'song_type','freq').limit(nbRecommendation).show(truncate=False)\n        \n\n    ", 
            "metadata": {}, 
            "execution_count": 119, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "SyntaxError", 
                    "output_type": "error", 
                    "evalue": "unexpected EOF while parsing (<ipython-input-119-2a8afb842d6a>, line 23)", 
                    "traceback": [
                        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-119-2a8afb842d6a>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
                    ]
                }
            ]
        }, 
        {
            "source": "def recommendSong(user, nbRecommendations):\n    pos_user = select_pos.filter(col('uid')==user)\n    neg_user = select_neg.filter(col('uid')==user)\n\n#     if shown more than one preference\n    if (not pos_user) and (not neg_user):\n        return recommendCF(model, user, nbRecommendations)\n    \n#     if shown only one preference \n    elif pos_user: \n        song = pos_user.select('song_id').collect()[0].song_id\n        return recommendSim(user, song, nbRecommendations)\n    \n#     if never liked any song from the platform, recommend top 10 songs\n    else:\n        top_songs.select('song_id', 'singer', 'song_type','freq').limit(nbRecommendation).show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 120, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "simArray = simVec.toLocalMatrix().toArray()[0]\ntemp = song_unique.withcolumn('similarity', sc.parallelize(simArray))\n\nnbRecommendation = 10\ntemp.orderBy('similarity').orderBy('similarity', ascending=False).limit(nbRecommendation).show(truncate=False)\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sim_df.count()", 
            "metadata": {}, 
            "execution_count": 45, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "500"
                    }, 
                    "metadata": {}, 
                    "execution_count": 45, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# have a temporary song bank sim_df, save \nif not reload:\n    sim_df.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500_sim.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 52, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_freq = song_freq.na.fill('0', subset=['singer','song_name'])\nsinger_arr = [str(i.singer) for i in song_freq.select('singer').collect()]\nsong_arr = [str(i.song_name) for i in song_freq.select('song_name').collect()]\ntype_arr = [str(i.song_type) for i in song_freq.select('song_type').collect()]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# TODO:\n- upon recommended songs, generate 100 songs per user - keep the song_id \n- try classification methods for fine tune to get top 20 \n- take into considerations of more features: song_name, singer, song_type\n- predict the class: 0 - 5 score (training with normalized frequency 1-5 and download automatically 5)\n- evalute both model's top 20 recommendation, rms error \n\nFor collaborative filter<br>\n- visualize the frequency distribution - try normalization \n- ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "learning about distributed linear algebra \nchallenge: \n- matrix stored in distributed container, moving everything to memory is impossible?", 
            "cell_type": "markdown"
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}