{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "## Table of contents\nPart 2, 4, 5 heavely referes to the [Movie Recommender tutorial](https://datascience.ibm.com/exchange/public/entry/view/99b857815e69353c04d95daefb3b91fa) on DSX. \n1. [Load the data](#1.-Load-the-data) <br>\n    1.1 Load song table<br>\n    1.2 Load user table <br>\n    1.3 Load user play frequency table<br>\n2. [Spark performance basics](#2.-Spark-performance-basics)<br>\n3. [Explore the data with Spark APIs](#3.-Explore-the-data-with-Spark-APIs)<br>\n    3.1 [Clean song table](#3.1-Clean-song-table) <br>\n    3.2 [Clean play table and user table](#3.2-Clean-play-table-and-user-table) <br>\n4. [Visualize the data](#4.-Visualize-the-data)<br>\n5. [Build the recommender system](#5.-Build-the-recommender-system)<br>\n    5.1 Setup training and test set<br>\n    5.2 Train collaborative filtering<br>\n    5.3 Tune parameters <br>\n    5.4 Evaluate recommendation results<br>\n6. [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>\n    6.1 [Setup vectors](#6.1-setup-vectors)<br>\n    6.2 [Compute similarity matrix](#6.2-Compute-similarity-matrix)<br>\n    6.3 [Item-based recommender for selected users](#6.3-Item-based-recommender-for-selected-users) <br>\n    6.4 [Integrate recommenders](#6.4-Integrate-recommenders)\n    \n7. [Future work](#7.-Future-work)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, count, struct, row_number, when, isnan, log,lit\nfrom pyspark.sql.functions import round as cround\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 136, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 1. Load the data\n\nOnly for the first time. <br>\nCleaned data will be saved and could be imported in future. <<br>\n\nJump to other sections: \n- [Build the recommender system](#5.-Build-the-recommender-system)<br>\n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "spark = SparkSession.builder.getOrCreate()\nuser_table_raw = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'valid_user_highfreq.csv'))\n\nuser_table_raw.take(5)\n\nsong_table= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'song_table.csv'))\nprint(song_table.take(5))\n\nsong_freq = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'user_song_freq.csv'))\nprint(song_freq.take(5))\n\ndownload_table = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'clean_download.csv'))\nprint (download_table.take(5))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "user_db = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', '3_1uid.csv'))\nprint (user_db.select('uid').distinct().count())", 
            "metadata": {}, 
            "execution_count": 7, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "264715\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2. Spark performance basics", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "print (sc.defaultParallelism)\nprint (sc.getConf().toDebugString())\nprint (\"Number of partitions for the song_freq DataFrame: \" + str(song_freq.rdd.getNumPartitions()))", 
            "metadata": {}, 
            "execution_count": 8, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "4\nhive.metastore.warehouse.dir=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/work/spark-warehouse\nspark.app.id=app-20171021174259-0155-332c25ad-3f8b-4fff-be46-e709b7342dff\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.host=10.143.133.71\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.driver.port=35869\nspark.eventLog.dir=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.id=driver\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.history.fs.logDirectory=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0022:7089\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v65/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.catalogImplementation=hive\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\nNumber of partitions for the song_freq DataFrame: 8\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 3. Explore the data with Spark APIs", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "\nuser_table_raw.show(truncate=False)\nprint (\"Number of users: \", user_table_raw.count())\nprint (\"Number of different users: \" + str(user_table_raw.select('uid').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 36, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---+----------+---------+\n|_c0|Unnamed: 0|uid      |\n+---+----------+---------+\n|1  |1         |154563989|\n|2  |2         |154806874|\n|3  |3         |154777984|\n|4  |4         |154801899|\n|5  |5         |154522980|\n|6  |6         |154466362|\n|7  |7         |154467953|\n|8  |8         |158752252|\n|9  |9         |154559964|\n|10 |10        |154542883|\n|11 |11        |154828695|\n|12 |12        |154723056|\n|13 |13        |154751052|\n|14 |14        |154630129|\n|15 |15        |154684841|\n|16 |16        |154799108|\n|17 |17        |154786598|\n|18 |18        |154561771|\n|19 |19        |154508382|\n|20 |20        |154710857|\n+---+----------+---------+\nonly showing top 20 rows\n\nNumber of users:  264714\nNumber of different users: 264714\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_table.show(truncate=False)\nprint (\"Number of songs: \", song_table.count())\nprint (\"Number of different songs: \" + str(song_table.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 21, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------+---------+-------------------------------+------+-----------+\n|song_id |song_type|song_name                      |singer|song_length|\n+--------+---------+-------------------------------+------+-----------+\n|602239  |null     |\u859b\u51ef\u742a                            |0     |null       |\n|160911  |null     |\u8521\u4f9d\u6797&\u5468\u6770\u4f26                        |0     |null       |\n|1033156 |null     |\u6c6a\u82cf\u6cf7                            |0     |null       |\n|294622  |null     |DJ\u821e\u66f2                           |0     |null       |\n|517174  |null     |\u68a6\u9e3d                             |0     |null       |\n|6606144 |null     |\u6768\u5c0f\u66fc&\u51b7\u6f20                         |0     |null       |\n|6432663 |null     |\u5c0f\u4e54                             |0     |null       |\n|6587633 |null     |\u97e9\u5b87                             |0     |null       |\n|6587662 |null     |\u97e9\u5b87                             |0     |null       |\n|158182  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|1037626 |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|995380  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|529630  |null     |\u8bd5\u97f3\u789f                            |0     |null       |\n|3006068 |null     |\u7f51\u7edc\u6b4c\u624b                           |0     |null       |\n|4111186 |null     |\u5e7d\u5e7d                             |0     |null       |\n|1488120 |null     |Declan Masterson               |0     |null       |\n|21461735|null     |Lil Uzi Vert&Quavo&Travis Scott|0     |null       |\n|22399766|null     |\u5f20\u6770                             |0     |null       |\n|60057   |null     |\u7518\u840d                             |0     |null       |\n|116273  |null     |null                           |0     |null       |\n+--------+---------+-------------------------------+------+-----------+\nonly showing top 20 rows\n\nNumber of songs:  5047315\nNumber of different songs: 1559990\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_table.printSchema()", 
            "metadata": {}, 
            "execution_count": 5, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- song_length: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 3.1 Clean song table \n##### Need to clean song_table:\n- remove invalid song_id \n- get single entry for each song_id (most common song)\n- drop song_length column because 1) large variance 2) not very relevant", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# song_table.createOrReplaceTempView('song_table')\nsong_table.createOrReplaceTempView('song_table')\nsong_table_valid = spark.sql(\"select *  from song_table where song_id > 0 and song_id is not null\")\n\nprint (\"Number of songs: \", song_table_valid.count())\nprint (\"Number of different songs: \" + str(song_table_valid.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 9, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of songs:  3230980\nNumber of different songs: 1559987\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "# get most common non-zero song_type\ntype_counts = song_table_valid.groupBy(['song_id', 'song_type'])\\\n    .count().alias('cnt')\\\n    .where(col('song_type') != '0')\n\nmax_type = (type_counts\n    .groupBy('song_id')\n    .agg(F.max(struct(col('count'), col('song_type'))).alias('max'))\n    .select(col('song_id'), col('max.song_type')))\n\n# get most common not null song_name \nname_counts = song_table_valid.groupBy(['song_id', 'song_name'])\\\n    .count().alias('cnt')\\\n    .where(col('song_name').isNotNull())\n    \nmax_name = (name_counts.groupBy('song_id')\n            .agg(F.max(struct(col('count'), col('song_name'))).alias('max'))\n            .select(col('song_id'),col('max.song_name')))\n\n\n# get most common not null singer \nsinger_counts = song_table_valid.groupBy(['song_id', 'singer']).count().alias('cnt').where(col('singer').isNotNull())\nw = Window().partitionBy('song_id').orderBy(col('count').desc())\nmax_singer = (singer_counts\n              .withColumn('rn', row_number().over(w))\n              .where(col('rn')==1)\n              .select('song_id', 'singer'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "note: tried pandas - takes a long time for me; both Window or struct should work ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "print (max_type.select('song_id').distinct().count())\nprint (max_name.select('song_id').distinct().count())\nprint (max_singer.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 11, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "231216\n1559532\n1547516\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "songs = song_table_valid.select('song_id').distinct().alias('songs')\nsong_unique = songs\\\n    .join(max_type, 'song_id','left')\\\n    .join(max_name, 'song_id','left')\\\n    .join(max_singer, 'song_id','left')\\\n    .select('song_id', 'song_type','song_name', 'singer')\\\n\nsong_unique = song_unique.na.fill('0', subset=['song_type'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.show()\nprint (song_unique.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 22, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------+---------+--------------------+----------------+\n| song_id|song_type|           song_name|          singer|\n+--------+---------+--------------------+----------------+\n|  100140|        0|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|\n|10015022|        0|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab|\n| 1003644|        0|Save The One, Sav...|  T.M.Revolution|\n| 1004266|        0|        Broken heart|             \u9ec4\u4e49\u8fbe|\n| 1006370|        0|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|\n| 1006422|        0|                  \u6d63\u7eb1|              \u6731\u6d01|\n|10065669|        1|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|\n|10071852|        0|Dirt Road Anthem ...|  Country Nation|\n|10087323|        0|Helden sterben ei...| Michael Wendler|\n| 1009129|        1|    Dietro L'Incanto|Ludovico Einaudi|\n|  100964|        1|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77|\n| 1010103|        0|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|\n|10101536|        0|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|\n|  101021|        0|           I Believe|             Era|\n|10102878|        0|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|\n|10107689|        0|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|\n|  101122|        0|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|\n|  101272|        0|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|\n|10139662|        1|                  \u843d\u82b1|             \u674e\u5c0f\u7490|\n|10141277|        0|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|\n+--------+---------+--------------------+----------------+\nonly showing top 20 rows\n\n1559987\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.groupBy('song_type').count().sort(col('count').desc()).show()", 
            "metadata": {}, 
            "execution_count": 13, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+-------+\n|song_type|  count|\n+---------+-------+\n|        0|1328771|\n|        1| 155797|\n|        2|  65042|\n|        3|  10053|\n|       73|     22|\n|       90|     18|\n|       89|     12|\n|       91|     12|\n|       48|      7|\n|       88|      6|\n|       43|      6|\n|       66|      6|\n|       60|      6|\n|       26|      6|\n|       30|      6|\n|       27|      5|\n|       41|      5|\n|       33|      5|\n|       82|      5|\n|       32|      4|\n+---------+-------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_song.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 3.2 Clean play table and user table\n- remove extremely high frequency (0.9999 quantile at approximately 1000 play frequency)\n- remove play history without uid \n\nNumber of valid users:  264708 <br>\nNumber of invalid users:  186", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq = song_freq.withColumn('freq', song_freq['freq'].cast('integer'))\nsong_freq = song_freq.filter((col('uid').isNotNull()) & (col('uid') > '0') &\n                             (col('song_id').isNotNull()) & (col('song_id')>'0'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.sort(col('freq').desc()).show()", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 7, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+------+--------+-----+\n|      uid|device| song_id| freq|\n+---------+------+--------+-----+\n|167982849|    ar| 4554016|73609|\n|  1791497|    ar| 3401476|51858|\n|   751824|    ar| 9950164|46970|\n|  1685126|    ar|15249349|41265|\n|  1685126|    ar| 9950164|39207|\n| 37025504|    ar| 9950164|35198|\n|  1791497|    ar|15198178|30975|\n| 37025504|    ar|15249349|29830|\n|  1791497|    ar| 6468891|26765|\n|  1791497|    ar|  442265|23907|\n|  1685126|    ar| 5237384|22949|\n|  1791497|    ar| 9950164|22849|\n| 22730453|    ar| 7005106|22294|\n| 22730453|    ar| 5965686|22289|\n|   497685|    ar| 9950164|21574|\n|  1062806|    ar| 9950164|20929|\n|  1791497|    ar| 5245130|19813|\n| 37025504|    ar| 5237384|19799|\n|  1685126|    ar| 6468891|19709|\n|  1791497|    ar|  125802|19697|\n+---------+------+--------+-----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "cutoff_freq = song_freq.approxQuantile('freq', [0.99], 0.005)\nprint (cutoff_freq)", 
            "metadata": {}, 
            "execution_count": 16, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[54.0]\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "cutoff_freq = 1000", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 17, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "extreme_freq = song_freq.filter(col('freq') >= cutoff_freq)\n# get valid song played frequency\nvalid_freq = song_freq.filter(col('freq') < cutoff_freq)\n\n# get valid users \noutlier_user = extreme_freq.select('uid').distinct()\noutlier_user.createOrReplaceTempView('filter_view')\nvalid_user = user_table_raw.where('uid not in (select uid from filter_view)')", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "print (\"Number of valid users: \", valid_user.select('uid').distinct().count())\nprint (\"Number of invalid users: \", outlier_user.select('uid').distinct().count())\n# Number of valid users:  264708\n# Number of invalid users:  186", 
            "metadata": {}, 
            "execution_count": 19, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of valid users:  264708\nNumber of invalid users:  186\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "# too confusing to add this feature\n# song_length = song_table.groupBy(['song_id', 'length']).avg(col('song_length')).alias('avg_length').where((col('song_length').isNotNull()) & (col('song_length') > '200') & (col('song_length') <'720'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "download_table.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 18, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---+-----------+------+----------+------------------------------+---------+\n|_c0|uid        |device|song_id   |song_name                     |paid_flag|\n+---+-----------+------+----------+------------------------------+---------+\n|0  |null       |ip    |6945370.0 |null                          |null     |\n|1  |1685126.0  |ar    |170455.0  |\u987a\u6d41\u3001\u9006\u6d41                         |null     |\n|2  |736305.0   |ar    |23380344.0|\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)                   |null     |\n|3  |168042561.0|ar    |6292506.0 |\u5e1d\u90fd                            |null     |\n|4  |1749320.0  |ar    |21473237.0|\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1                      |null     |\n|5  |155948236.0|ar    |93388.0   |\u4e9a\u62c9\u4f2f\u8df3\u821e\u5973\u90ce                       |null     |\n|6  |167794453.0|ar    |497722.0  |\u85d5\u65ad\u4e1d\u8fde                          |null     |\n|7  |168505311.0|ip    |4188142.0 |null                          |null     |\n|8  |168031064.0|ar    |4243838.0 |\u7231\u60c5\u7801\u5934(2651,cn \u5929\u5730\u4eba\u97f3\u4e50\u7f51)          |null     |\n|9  |167626177.0|ar    |1080516.0 |\u65e0\u6cd5\u539f\u8c05(\u7535\u89c6\u5267\u300a\u56de\u5bb6\u7684\u8bf1\u60d1\u300b\u4e3b\u9898\u66f2)           |null     |\n|10 |736305.0   |ar    |344885.0  |Because Of You                |null     |\n|11 |736305.0   |ar    |119699.0  |\u522b\u519b\u8425                           |null     |\n|12 |1685126.0  |ar    |3042675.0 |\u65f6\u5149\u673a\u524d\u594f-\u4e94\u6708\u5929_\u4e94\u6708\u5929(\u94c3\u58f0)             |null     |\n|13 |167691874.0|ar    |3287563.0 |\u5176\u5b9e\u90fd\u6ca1\u6709                         |null     |\n|14 |736305.0   |ar    |6792401.0 |Dream It Possible             |null     |\n|15 |167973828.0|ar    |10901925.0|Overtime (Vicetone Remix Edit)|null     |\n|16 |736305.0   |ar    |9925786.0 |Dream It Possible(34\u79d2\u94c3\u58f0\u7248)     |null     |\n|17 |167784467.0|ar    |8050943.0 |\u8036\u5229\u4e9a\u5973\u90ce                         |null     |\n|18 |168007277.0|ip    |4373829.0 |null                          |null     |\n|19 |167782564.0|ar    |325517.0  |\u68a6\u9a7c\u94c3                           |null     |\n+---+-----------+------+----------+------------------------------+---------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 11, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+------+--------+----+\n|uid      |device|song_id |freq|\n+---------+------+--------+----+\n|751824   |ar    |6483029 |385 |\n|168156920|ip    |6792060 |5   |\n|497685   |ar    |7207401 |26  |\n|1062806  |ar    |6841262 |50  |\n|168195436|ar    |12808784|22  |\n|1685126  |ar    |59582   |26  |\n|168286187|ar    |4188404 |2   |\n|37025504 |ar    |481552  |733 |\n|168478031|ar    |9822502 |4   |\n|168406030|ar    |909773  |7   |\n|168410987|ar    |5425869 |10  |\n|168511270|ar    |6817428 |68  |\n|168115240|ar    |23665227|2   |\n|168396372|ar    |4276822 |10  |\n|168417737|ar    |5383328 |19  |\n|1062806  |ar    |20870989|73  |\n|168373631|ar    |7202991 |60  |\n|168335848|ip    |4112638 |15  |\n|37025504 |ar    |1108956 |76  |\n|168453430|ar    |1705363 |1   |\n+---------+------+--------+----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "download_table.select('uid').distinct().count()", 
            "metadata": {}, 
            "execution_count": 143, 
            "outputs": [
                {
                    "execution_count": 143, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "242243"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "valid_user.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    \nvalid_freq.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_freq.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 20, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 4. Visualize the data\nnote: this dataset is too big for visualization here \nUsing Seaborn and matplotlib", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install seaborn", 
            "metadata": {}, 
            "execution_count": 319, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): seaborn in /usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/site-packages\r\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "import seaborn as sns\n%matplotlib inline", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 320, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "import pandas as pd\nfreqPandas = valid_freq.toPandas()\ndlPandas = download_table.toPandas()\nsns.lmplot(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {}, 
            "execution_count": 321, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-321-33bed86bf87b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfreqPandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdlPandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlmplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'song_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlPandas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         \"\"\"\n\u001b[0;32m   1575\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[1;31m##########################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o109.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:258)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:254)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:254)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:275)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n"
                    ], 
                    "evalue": "An error occurred while calling o109.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:258)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:254)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:254)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:275)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                    "ename": "Py4JJavaError", 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "plot matrix for song played by user, with frequency as the hue ", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 5. Build the recommender system\n(Cited from IBM bluemix Data Science Experience (DSX) document) <br>\n\n\"\nCollaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who have similar preference on the same item will also have similar opinions on items that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users versus all songs. Most cells in the matrix will be empty. An empty cell means the user hasn't played the song yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for song factors and solving for user factors.\n\"\n\n\nChallenge in this recommender system: <br>\n1. Small number of play history that could be shown by sparsity of the utility matrix\n2. Limited features for songs - mixed language\n\nSolution: <br>\n1. Hybrid", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# restarting kernel\nreload = True", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 5, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# load saved files if starting from middle\nif reload: \n    song_unique = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_song.csv'))\n    song_unique.take(5)\n    \n    valid_freq = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_freq.csv'))\n    valid_freq.take(5)\n    \n    valid_user = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    valid_user.take(5)\n    \n    download_table = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'clean_download.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 6, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.printSchema()\nsong_unique.count()", 
            "metadata": {}, 
            "execution_count": 7, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\n", 
                    "name": "stdout"
                }, 
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1559987"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "valid_freq = valid_freq.withColumn('uid', valid_freq['uid'].cast('integer'))\nvalid_freq = valid_freq.withColumn('song_id', valid_freq['song_id'].cast('integer'))\nvalid_freq = valid_freq.withColumn('freq', valid_freq['freq'].cast('double'))\n\nvalid_user = valid_user.withColumn('uid', valid_user['uid'].cast('integer'))\n\nsong_unique = song_unique.withColumn('song_type', song_unique['song_type'].cast('integer'))\nsong_unique = song_unique.withColumn('song_id', song_unique['song_id'].cast('integer'))\n\nvalid_download = download_table.withColumn('uid', download_table['uid'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\n\n# valid_freq = valid_freq.withColumn('label',log(10.0, valid_freq.freq))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 8, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "song_unique.createOrReplaceTempView('song_unique')\nvalid_freq.createOrReplaceTempView('valid_freq')\nsong_freq = spark.sql(\"\"\"\n    select s.song_type, s.song_id, s.song_name, s.singer, COALESCE(f.cnt,0) as freq\n    from song_unique as s\n    left join\n        (select song_id, count(*) as cnt from valid_freq group by song_id) as f\n        on f.song_id = s.song_id\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 9, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# take log of the frequency and scale to -1 to 100 representing degree of preference \n\nvalid_freq2 = valid_freq.withColumn('label',cround(log(10.0, valid_freq.freq)/3.0*10,0))\nvalid_freq2 = valid_freq2.replace(0, -1, subset=['label'])\n# should be None \nvalid_freq2 = valid_freq2.na.fill(0, subset=['label'])\n\n# print(valid_freq2.groupBy().max('label').show())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# combine with download table\n\nvalid_download = valid_download.withColumn('download', lit(1))\nvalid_download = valid_download.drop('device')\nvalid_download = valid_download.drop('song_name')\n\nvalid_score = valid_freq2.join(valid_download, ['uid','song_id'], 'left_outer')\nvalid_score = valid_score.na.fill(0, subset=['download'])\n\nvalid_score = valid_score.withColumn('label', when(valid_score.download==1, lit(10)).otherwise(valid_score.label))\n\nvalid_score = valid_score.replace(-0.5, -1, subset=['label'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 11, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Jump to other sections: \n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "valid_freq.printSchema()\nvalid_user.printSchema()\nsong_unique.printSchema()\nvalid_download.printSchema()", 
            "metadata": {}, 
            "execution_count": 7, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n# check sparsity \ncounts_freq = valid_freq.count()\nprint ('Number of song frequency entried: ', counts_freq)\n# songs played/( songs x users)\npercentage = (counts_freq)*1.0/1559987/264708\nprint ('Percentage of song played: ', percentage)", 
            "metadata": {}, 
            "execution_count": 18, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of song frequency entried:  32801295\nPercentage of song played:  7.943336195316835e-05\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n\nmoreThanOnce = valid_freq.groupBy('song_id').count().alias('cnt').where(col('count') > 1).select('song_id').count()\nprint ('Number of songs played more than once: ', moreThanOnce)\n# significantly less, only recommend songs played more than once\n# utility matrix will be less sparse\n# number of song frequency entries will be reduce by 686993(only played once) -32114102, which is 2%", 
            "metadata": {}, 
            "execution_count": 28, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of songs played more than once:  872994\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "print ('Percentage of filled matrix: ', round((32801295-686993)*100.0/(872994*264708),4))\n# vs. ~0.0073% filled ", 
            "metadata": {}, 
            "execution_count": 42, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Percentage of filled matrix:  0.0139\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.printSchema()\nsong_freq.count()", 
            "metadata": {}, 
            "execution_count": 26, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- freq: long (nullable = false)\n\n", 
                    "name": "stdout"
                }, 
                {
                    "execution_count": 26, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1559987"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##### checking\n\nprint (song_freq.select('song_id').count())\nprint (song_freq.select('song_id').where(col('freq')>1).count())\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 17, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "1559987\n872994\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.1 Setup training and test set\n\nsetup: 80% training and 20% test set ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "(trainingFreq, testFreq) = valid_score.randomSplit([80.0, 20.0])\n\ntrainingFreq.printSchema()\n# utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns]).show()\ntestFreq.printSchema()", 
            "metadata": {}, 
            "execution_count": 12, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.2 Setup collaborative filtering model\n\nAccoding to DSX document again:\n\"\nA NaN result is due to [SPARK-14489](https://issues.apache.org/jira/browse/SPARK-14489) and because the model can't predict values for users for which there's no data.\"", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "model = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\").fit(trainingFreq)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 13, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "predictions = model.transform(testFreq)\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))", 
            "metadata": {}, 
            "execution_count": 10, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "The root mean squared error for our model is: 2.917168466920176\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.3 Tune parameters\n\nCross validation and grid search to tune for hyperparameters\n\nALS algorithm :\n```python\n    class pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )\n```\n\nThe ALS hyperparameters are:\n- `rank` = the number of latent factors in the model\n- `maxIter` = the maximum number of iterations \n- `regParam` = the regularization parameter", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# takes way too long, killed at second run\n\n(trainingScore, validationScore) = valid_score.randomSplit([90.0, 10.0])\n\nals = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n\n# paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 3, 5, 7]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,2,3]).addGrid(als.regParam, [0.05, 0.1]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = crossval.fit(trainingScore)\npredictions = cvModel.transform(validationScore)\n\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.drop())))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "print ('Best rank is: ', cvModel.bestModel.rank)\nprint ('Best regularizer is: ', cvModel.bestModel.params)\n# evaluate with 0 for null prediction\n# print (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))\n# The root mean squared error for our model is: 3.1921429126212857", 
            "metadata": {}, 
            "execution_count": 136, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Best rank is:  1\nBest regularizer is:  []\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "valid_score.printSchema()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "predictions.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'predictions_CF_1015.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 5.4 Evaluate recommendation results\n\n- MSE is low -> recommender seems to be pretty good\n- However, looking at recommended songs for individual users, it seems to recommending weird songs\n- Therefore, need alternative for inactive users for a more explainable model\n\nDetailed evaluation as following:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "utility_matrix_small = valid_user.crossJoin(song_freq.select('song_id')).select('uid','song_id')\nutility_matrix_small = utility_matrix_small.join(valid_score, ['uid', 'song_id'], 'left_outer').select('uid', 'song_id', 'label')\n\nutility_matrix = utility_matrix_small.na.fill('0', subset=['label'])\n\n# Replace predicted NaN values with the average frequency and evaluate the model\n# avgScore = utility_matrix.select('label').groupBy().avg().first()[0]\n# print (\"The average score in the dataset is: \" + str(avgScore))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 27, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "print (\"Recommendations for user 169262317:\")\n# recommendCF(model, 169262317, 10)\n# print \"Recommendations for user 471:\"\n\nprint (\"Recommendations for user 12333:\")\nrecommendCF(model, 12333, 10)", 
            "metadata": {}, 
            "execution_count": 18, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Recommendations for user 169262317:\nRecommendations for user 12333:\n+--------+---------------+-------+----------+\n|song_id |song_name      |singer |prediction|\n+--------+---------------+-------+----------+\n|23918297|Vol04          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918205|Vol118         |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23883915|Vol72          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918294|Vol47          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23883912|Vol51          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23892451|Vol75          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23892457|Vol53          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23917769|\u3010\u547c\u53eb\u4e00\u53ea\u55b5\u3011\u654f\u611f\u7684\u4eba\u5bb9\u6613\u53d7\u4f24|\u58eb\u5175\u5c0f\u7ad9\u97f3\u4e50\u53f0|18.256443 |\n|23883943|Vol98          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918229|Vol13          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |16.647385 |\n+--------+---------------+-------+----------+\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Further evaluation", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# total user:264708 \n# select 12333\nselect_valid = valid_score.createOrReplaceTempView('select_valid')\nselected = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_valid\n    group by uid\n    having count(1) <= 1\n\n    order by 2 asc \n\"\"\")\n# inactive_user.count()\n# treshold at 1: 68268 -> around a quarter \n# treshold at 5: 154568 -> more than half \n# treshold at 2: 98849 -> around one third", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 21, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "selected.count()", 
            "metadata": {}, 
            "execution_count": 22, 
            "outputs": [
                {
                    "execution_count": 22, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "68268"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "# choose 2 inactive user\nprint (\"Recommendations for user 167579971:\")\nrecommendCF(model, 167579971, 10)\n\nprint (\"Recommendations for user 168975045:\")\nrecommendCF(model, 168975045, 10)\n\n# 169042674 0\n# 167579971 10\n# 169042674 0\n# 167640755 10\n\n# valid_score.select().where(col('uid')==169042674).show()\n# problem is it's not in the user list", 
            "metadata": {}, 
            "execution_count": 23, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Recommendations for user 167579971:\n+-------+---------+------+----------+\n|song_id|song_name|singer|prediction|\n+-------+---------+------+----------+\n+-------+---------+------+----------+\n\nRecommendations for user 168975045:\n+-------+------------------------------+--------------+----------+\n|song_id|song_name                     |singer        |prediction|\n+-------+------------------------------+--------------+----------+\n|2608099|\u83b2\u82b1\u751f\u5927\u5e08\u4e8b\u4e1a\u6210\u5c31\u7948\u7977\u6587                  |\u5c0f\u6148            |8.930752  |\n|6366855|Long Shot(33\u79d2\u94c3\u58f0\u7248)             |Kelly Clarkson|8.8875885 |\n|3139770|\u96e8\u304c\u964d\u308b\u65e5\u306b\u306f(Japanese Version)     |Beast         |8.791963  |\n|8112784|\u5206\u98de                            |\u6a0a\u6850\u821f           |8.699762  |\n|6073973|Style - \u6700\u70ab\u6c11\u65cf\u98ce \u7ad9\u53f0(\u6c5f\u5357)          |\u7f51\u7edc\u6b4c\u624b          |8.699762  |\n|5319753|\u6d2a\u6e56\u6c34\u6d6a\u6253\u6d6a\u00b7\u5c0f\u66f2\u597d\u5531\u53e3\u96be\u5f00                |\u7d22\u5b9d\u8389           |8.673834  |\n|4762871|Party(\u60c5\u4e4b\u8c03 \u4e03\u6708\u5a5a\u793c \u4e0b \u4e2d\u56fd\u98ce \u5f6d\u4fca Remix)|DJ\u821e\u66f2          |8.637085  |\n|4221900|\u671d\u9633\u6c9f \u770b\u89c1\u4e86\u65b0\u88ab\u5b50\u6211\u5b9e\u5728\u96be\u8fc7\u9ad8\u6d01             |\u8c6b\u5267            |8.6367445 |\n|6053716|\u641e\u7b11\u72ec\u767d\u8bf4\u5531\u94c3\u58f0                      |\u7f51\u7edc\u6b4c\u624b          |8.616981  |\n|6118321|\u5ff5\u4eb2\u6069(34\u79d2\u94c3\u58f0\u7248)                   |\u9648\u767e\u5f3a           |8.543522  |\n+-------+------------------------------+--------------+----------+\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 6. Hybrid recommender system\n\nsupplemnt the collaborative filtering with item-item recommender <br>\nfor user with only 1 song played history (or two):<br>\n- recommend songs based on cosine similarity to the song/songs played by the user \n- (not implemented) for user with 0 songs player: recommend top k songs  \n\n- tries implementing on all 800k songs, failed after countless trial\n- eventually turning to subsample ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "from pyspark.mllib.linalg import Vectors\n# Package for distributed linear algebra DIMSUM\n# Dimension Independent Matrix Square using MapReduce\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.types \\\nimport ArrayType, StringType, DoubleType, StructType, StructField\n\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.mllib.linalg.distributed \\\nimport IndexedRowMatrix,IndexedRow, RowMatrix, BlockMatrix,CoordinateMatrix\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 24, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 6.1 Setup feature vector\n\nGet feature vectors for top 500 songs and save it for future use; it's okay to run ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq = song_freq.where(col('freq')>1).orderBy('freq', ascending=False)\n\ntop_songs = song_freq.orderBy('freq', ascending =False).limit(500)\ntop_songs = top_songs.na.fill('0', subset=['singer','song_name'])\n\ntop_songs = top_songs.withColumn('id', monotonically_increasing_id()+1)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 25, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "if not reload:\n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# convert to VectorAssembler \nfrom pyspark.ml.feature import VectorAssembler\n\ndef extract(row):\n    return (row.song_id, row.song_type, ) + tuple(row.name_vec.toArray().tolist()) + tuple(row.singer_vec.toArray().tolist())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 26, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "song_freq.count()", 
            "metadata": {}, 
            "execution_count": 16, 
            "outputs": [
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "872994"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "reload = False\nfdate = '1022'", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 28, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# get a dataframe of vector\nif not reload:\n    \n    song_str2arr = top_songs.withColumn('singer_arr', split(col(\"singer\"), \" \").cast(ArrayType(StringType())).alias(\"singer_arr\"))\n    w2v= Word2Vec(vectorSize=3, minCount=0, inputCol=\"singer_arr\", outputCol=\"singer_vec\")\n    top_song_vec = w2v.fit(song_str2arr).transform(song_str2arr)\n\n    top_song_vec = top_song_vec.withColumn('name_arr', split(col(\"song_name\"), \" \").cast(ArrayType(StringType())).alias(\"name_arr\"))\n    w2v = Word2Vec(vectorSize=5, minCount=0, inputCol=\"name_arr\", outputCol=\"name_vec\")\n    top_song_vec = w2v.fit(top_song_vec).transform(top_song_vec)\n    \n    top_vec_temp = top_song_vec.select('song_type','name_vec','singer_vec', 'song_id')\n    # save as dataframe \n    top_vec_df = top_vec_temp.rdd.map(extract).toDF()\n    top_vec_df = top_vec_df.withColumn(\"id\", monotonically_increasing_id()+1)\n    top_vec_df = top_vec_df.withColumn('_1', top_vec_df['_1'].cast('double'))\n    \n    target = 'top_500_'+fdate +'.csv'\n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', target))\n    \n    target = 'top_500_vec_'+fdate +'.csv'\n    top_vec_df.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', target))\n    \nelse:\n    top_vec_df = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_vec.csv'))    \n    \n    sim_df_ori = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_simi.csv'))    \n    \n    sim_df = sim_df.withColumn('id', sim_df.id+1).orderBy('id')\n    sim_df.select('id').show()\n    ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 30, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# sim_df = spark.read\\\n#   .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n#   .option('header', 'true')\\\n#   .option(\"inferschema\", \"true\")\\\n#   .load(bmos.url('musicrecommendation', 'top_500_simi.csv')) ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 229, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Jump to other sections: \n- [Recommend for selected users](#6.3.-Recommend-for-selected-users)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "top_vec_df.printSchema()", 
            "metadata": {}, 
            "execution_count": 31, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: long (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- _10: double (nullable = true)\n |-- id: long (nullable = false)\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "top_songs.show()", 
            "metadata": {}, 
            "execution_count": 32, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+---------+--------+--------------------+-----------+------+---+\n|song_type| song_id|           song_name|     singer|  freq| id|\n+---------+--------+--------------------+-----------+------+---+\n|        2|15249349|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|    \u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|  1|\n|        2| 9950164|               \u521a\u597d\u9047\u89c1\u4f60|        \u674e\u7389\u521a|102218|  2|\n|        2| 5237384|                \u9006\u6d41\u6210\u6cb3|        \u91d1\u5357\u73b2| 71834|  3|\n|        2| 6468891|                  \u6f14\u5458|        \u859b\u4e4b\u8c26| 68042|  4|\n|        2|15807836|\u4e09\u751f\u4e09\u4e16-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843...|         \u5f20\u6770| 58433|  5|\n|        2| 5114569|          \u6ca1\u6709\u4f60\u966a\u4f34\u771f\u7684\u597d\u5b64\u5355|         \u68a6\u7136| 53102|  6|\n|        2| 3287564|                 \u5c0f\u65f6\u5019|        \u82cf\u6253\u7eff| 52403|  7|\n|        2|16400733|\u601d\u6155-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|        \u90c1\u53ef\u552f| 48343|  8|\n|        2| 6657692|             \u8d70\u7740\u8d70\u7740\u5c31\u6563\u4e86|        \u5e84\u5fc3\u598d| 47438|  9|\n|        2| 3620537|              \u4f60\u8fd8\u8981\u6211\u600e\u6837|        \u859b\u4e4b\u8c26| 43961| 10|\n|        2| 7149583|                \u544a\u767d\u6c14\u7403|        \u5468\u6770\u4f26| 41887| 11|\n|        2| 6749207|               Faded|Alan Walker| 38523| 12|\n|        2| 3971731|               \u4ee5\u540e\u7684\u4ee5\u540e|        \u5e84\u5fc3\u598d| 38138| 13|\n|        1|23498554|                \u52a8\u7269\u4e16\u754c|        \u859b\u4e4b\u8c26| 37840| 14|\n|        2|23082492|                  \u9ad8\u5c1a|        \u859b\u4e4b\u8c26| 37762| 15|\n|        2| 4147754|  \u4e11\u516b\u602a-(\u7535\u89c6\u5267\u300a\u5982\u679c\u6211\u7231\u4f60\u300b\u63d2\u66f2)|        \u859b\u4e4b\u8c26| 34504| 16|\n|        2|13273544|                \u52c9\u4e3a\u5176\u96be|         \u738b\u5195| 33689| 17|\n|        1|16827761|                  \u6210\u90fd|         \u8d75\u96f7| 33348| 18|\n|        2| 6449273|                  \u7ec5\u58eb|        \u859b\u4e4b\u8c26| 32905| 19|\n|        1| 7176392|                  \u6210\u90fd|         \u8d75\u96f7| 31211| 20|\n+---------+--------+--------------------+-----------+------+---+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "top_vec_df.count()", 
            "metadata": {}, 
            "execution_count": 33, 
            "outputs": [
                {
                    "execution_count": 33, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "500"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 6.2 Compute similarity matrix\n#### using pyspark's distributed matrix\n\nunderstanding the goal: is to get single row of similarities when needed <br>\nproblem: \n\np1: columnSimilarities: need to transpose the matrix -> using toBlockMatrix().transpose() results in absurdly large matrix<br>\nSolved: manually transpose at DataFrame<br>\n\np2: distributed container does not allow random access to computed similarity matrix <br>\nTrial and Error:\n- to RDD? takes a long time, and very likely, does not have enough memory for a local matrix\n- do matrix multiplication for BlockMatrix? takes a long time\n- broadcast to different workers, couldn't do patten match from csr_matrix \n- reduced to 872994", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# manuely transpose the dataframe - 20s\ntop_vec_rdd = top_vec_df.rdd\ndata = []\nfor i in range(1,10):\n    data.append(top_vec_rdd.map(lambda row: row[i]).collect())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 68, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "# manurally construct IndexedRow\n# indexedT = IndexedRow(0, Vectors.dense(data[0]))\n\ntoIndexedRows = [IndexedRow(i, data[i]) for i in range(9)]\nindexedRows = sc.parallelize(toIndexedRows)\nmat = IndexedRowMatrix(indexedRows)\n\nrowMat = mat.toRowMatrix()\n\n# usualy takes long, but when u'r luck: 40s\nsimThres = rowMat.columnSimilarities(0.05)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 72, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "simRdd = simThres.toBlockMatrix().toIndexedRowMatrix().rows", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 75, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "simRdd.count()", 
            "metadata": {}, 
            "execution_count": 77, 
            "outputs": [
                {
                    "execution_count": 77, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "500"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "# convert to matrix \n# this shows RDD is empty error on the second run \n\n# mat = IndexedRowMatrix(data.map(lambda row:IndexedRow(monotonically_increasing_id(), Vectors(list(row)))\n# song_mat_temp = IndexedRowMatrix(song_vec_mod.rdd.map(lambda row: IndexedRow(row['id'], Vectors.dense(row[:9]))))\n\n# it will return an rdd of indexed row, need to extract \ndef extractRows(row):\n#     count = count+1\n    return (row.index, ) + tuple(row.vector.toArray().tolist())\n\n# save as dataframe \n# count = 0\n\ncolumnName = [str(i+1) for i in range(500)]\ncolumnName = ['id'] + columnName\nsim_df = simRdd.map(extractRows).toDF(columnName)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 78, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "print(simThres.numCols(), simThres.numRows())", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 79, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "500 500\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 6.3 Item-based recommender for selected users ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# filter users: listened to only one song before, but more than once\n\nselect_score= valid_score.createOrReplaceTempView('select_score')\ninactive_user = spark.sql(\"\"\"\n    select uid, count(1) as cnt\n    from select_score\n    group by uid\n    having cnt = 1\n    order by 2 asc \n\"\"\")\n\ninactive_user = inactive_user.createOrReplaceTempView('inactive_user')\n\n# played only one song, liked\nselect_pos = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label > -1\n\"\"\")\n\n# played only one song, disliked\n\nselect_neg = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label = -1\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 91, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "select_pos.join(song_freq, 'song_id').orderBy(song_freq.freq, ascending=False).show()", 
            "metadata": {}, 
            "execution_count": 112, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------+---------+----+-----+---------+--------------------+-------+------+\n| song_id|      uid|freq|label|song_type|           song_name| singer|  freq|\n+--------+---------+----+-----+---------+--------------------+-------+------+\n|15249349|168618926| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168751165| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168615186|15.0|  4.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168906950| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168776661|40.0|  5.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167403468|18.0|  4.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168626195| 9.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168589748| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167639000| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168351444| 4.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168906776|14.0|  4.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168968743| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168669167| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167628491| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167833719| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167862545| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168613458| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168879755| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168700381| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168957443| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n+--------+---------+----+-----+---------+--------------------+-------+------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "select_pos.printSchema()", 
            "metadata": {}, 
            "execution_count": 92, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "print(select_pos.count())\nprint(select_neg.count())", 
            "metadata": {}, 
            "execution_count": 93, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "18813\n49455\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "import pandas as pd\nsim_pd = sim_df.toPandas()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 189, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 6.4 Integrate recommenders\n\npreparation: \n- trained model for CF\n- top songs DF (top_songs) and similary matrix (sim_df) for Item-based recommender\n- top songs DF for Top K recommender ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# requires sim_df matrix similaries for all songs \n\ndef getSongSimilarity(song_index):\n    \"\"\"\n    param: song_index \n    return: similarity df of this song to all other songs \n    \"\"\"\n    df1 = sim_df.filter(col('id')==song_index).toPandas().transpose().drop(['id'])\n    df1.insert(0, 'id', range(1, 501))\n    df2 = sim_df.select(str(song_index), 'id').toPandas()\n    dfComb = df1.join(df2.set_index('id'), on='id')\n    dfComb['max'] = dfComb[[0, str(song_index)]].max(axis=1)\n    dfComb = dfComb.drop([0, str(song_index)], axis = 1)\n\n    song_df = spark.createDataFrame(dfComb)\n    return song_df", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 83, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "def recommendSim(user, song, nbRecommendations):\n#     song = select_user.song_id\n#     get similarity score for this song \n    print ('User liked this song: ')\n    song_freq.filter(col('song_id')==song).show()\n    \n#   get row index for that song \n    find_song = top_songs.filter(col('song_id')==song)\n\n#     if in the top 500 songs \n    if find_song.count():\n        print('Found the song user liked in top 500, recommend with similarity matrix...')\n        song_index = find_song.select('id').collect()[0].id\n        song_sim = sim_df.select(str(song_index+1), 'id')\n        predictions = getSongSimilarity(song_index).orderBy('max', ascending=False)\n        \n        recommendations = predictions.join(top_songs, top_songs.id == predictions.id)\\\n            .select(top_songs.song_id, top_songs.song_name, top_songs.singer, top_songs.song_type,predictions.max)\\\n            .orderBy(\"max\", ascending=False).limit(nbRecommendations)\n\n        recommendations.show(truncate=False)\n        \n# else: fit into the matrix, add to dataframe, get a new similarity matrix, get last row\n    else:\n        print('Did not find the song user liked in top 500, recommend top k songs...')\n        top_songs.select('song_id', 'song_name', 'singer', 'song_type','freq').limit(nbRecommendations).show(truncate=False)\n        ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 134, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_freq.song_name, song_freq.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 119, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "def recommendSong(user, nbRecommendations):\n    pos_user = select_pos.filter(col('uid')==user)\n    neg_user = select_neg.filter(col('uid')==user)\n\n#     if shown more than one preference\n    if (not pos_user.count()) and (not neg_user.count()):\n#         there should be a trained model \n        print('Active user, calling CF... ')\n        return recommendCF(model, user, nbRecommendations)\n    \n#     if shown only one preference \n    elif pos_user.count(): \n        print('Inactive user with shown preference, calling Item-based... ')\n        song = pos_user.select('song_id').collect()[0].song_id\n        return recommendSim(user, song, nbRecommendations)\n    \n#     if never liked any song from the platform, recommend top 10 songs\n    else:\n        print('Inctive user with no preference, return top K songs... ')\n        top_songs.select('song_id', 'song_name', 'singer', 'song_type','freq').limit(nbRecommendations).show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 120, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "user = 60683192\npos_user = select_pos.filter(col('uid')==user).rdd\nneg_user = select_neg.filter(col('uid')==user).rdd\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 107, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "user = 60683192\nrecommendSong(user, 10)", 
            "metadata": {}, 
            "execution_count": 115, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Inactive user with shown preference, calling Item-based... \nUser liked this song: \n+---------+-------+---------+------+----+\n|song_type|song_id|song_name|singer|freq|\n+---------+-------+---------+------+----+\n|        2|6694268|    \u7231\u4f60\u9519\u9519\u9519|\u8def\u7ae5&\u674e\u6cd3\u6ee2| 276|\n+---------+-------+---------+------+----+\n\nDid not find the song user liked in top 500, recommend top k songs...\n+--------+-------+---------+------+\n|song_id |singer |song_type|freq  |\n+--------+-------+---------+------+\n|15249349|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|2        |109798|\n|9950164 |\u674e\u7389\u521a    |2        |102218|\n|5237384 |\u91d1\u5357\u73b2    |2        |71834 |\n|6468891 |\u859b\u4e4b\u8c26    |2        |68042 |\n|15807836|\u5f20\u6770     |2        |58433 |\n|5114569 |\u68a6\u7136     |2        |53102 |\n|3287564 |\u82cf\u6253\u7eff    |2        |52403 |\n|16400733|\u90c1\u53ef\u552f    |2        |48343 |\n|6657692 |\u5e84\u5fc3\u598d    |2        |47438 |\n|3620537 |\u859b\u4e4b\u8c26    |2        |43961 |\n+--------+-------+---------+------+\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "## test on single user - recommend songs with similarity matrix vs. CF\n# using similarity matrix: not very distinguishable for Chinese word embedding (high similarity for all songs ingeneral)\n# yet, it seems to indicate some semantic connection in terms of the emotion,\n# also, recommending songs with same singer name sand song type\n\nuser = 12333\nfavorite = valid_score.filter(col('uid') == 12333).join(top_songs, 'song_id').orderBy('label', ascending=False)\n# favorite.show(truncate=False)\n\n# get index of favorite song which is in top 500 list \n# future work: generate a new similarity matrix with the song not in top 500 list\nsong_id = favorite.select('song_id').collect()[0].song_id\n\nrecommendSim(user,song_id, 10)\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 135, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "User liked this song: \n+---------+-------+---------+------+-----+\n|song_type|song_id|song_name|singer| freq|\n+---------+-------+---------+------+-----+\n|        2|5237384|     \u9006\u6d41\u6210\u6cb3|   \u91d1\u5357\u73b2|71834|\n+---------+-------+---------+------+-----+\n\nFound the song user liked in top 500, recommend with similarity matrix...\n+--------+---------------------+-------------------+---------+------------------+\n|song_id |song_name            |singer             |song_type|max               |\n+--------+---------------------+-------------------+---------+------------------+\n|23489524|\u9a84\u50b2\u7684\u5c11\u5e74-(\u771f\u4eba\u79c0\u300a\u9ad8\u80fd\u5c11\u5e74\u56e2\u300b\u4e3b\u9898\u66f2)|\u738b\u4fca\u51ef&\u5f20\u4e00\u5c71&\u5218\u660a\u7136&\u738b\u5927\u9646&\u8463\u5b50\u5065|2        |0.9993650308278718|\n|945320  |\u4e0d\u5206\u624b\u7684\u604b\u7231               |\u6c6a\u82cf\u6cf7                |2        |0.997381329032554 |\n|23861347|\u6211\u5bb3\u6015                  |\u859b\u4e4b\u8c26                |2        |0.9973311522721656|\n|4147754 |\u4e11\u516b\u602a-(\u7535\u89c6\u5267\u300a\u5982\u679c\u6211\u7231\u4f60\u300b\u63d2\u66f2)   |\u859b\u4e4b\u8c26                |2        |0.9972721633361123|\n|3213921 |\u65b9\u5706\u51e0\u91cc                 |\u859b\u4e4b\u8c26                |2        |0.9972138126256004|\n|23646084|\u66a7\u6627                   |\u859b\u4e4b\u8c26                |2        |0.9971979015774947|\n|90147   |\u65e0\u5fc3\u7761\u7720                 |\u5f20\u56fd\u8363                |2        |0.9971423050280094|\n|1063033 |\u7ea2\u65e5-(\u7535\u89c6\u5267\u300a\u4ed6\u6765\u81ea\u5929\u5802\u300b\u4e3b\u9898\u66f2)   |\u674e\u514b\u52e4                |2        |0.9969903018263635|\n|5268033 |\u5f85\u6211\u957f\u53d1\u53ca\u8170 (\u7535\u89c6\u5267\u300a\u7f8e\u4eba\u5236\u9020\u300b\u4e3b\u9898\u66f2)|\u5c1a\u96ef\u5a55                |2        |0.9967313814484539|\n|320442  |\u72ec\u89d2\u620f                  |\u8bb8\u8339\u82b8                |2        |0.9966631117186647|\n+--------+---------------------+-------------------+---------+------------------+\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "# recommend for the same user with CF - seems to be less interesting and\nuser = 12333\nrecommendSong(user, 10)", 
            "metadata": {}, 
            "execution_count": 117, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Active user, calling CF... \n+--------+---------------+-------+----------+\n|song_id |song_name      |singer |prediction|\n+--------+---------------+-------+----------+\n|23883943|Vol98          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918294|Vol47          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23883912|Vol51          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23892451|Vol75          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918205|Vol118         |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918297|Vol04          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23892457|Vol53          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23917769|\u3010\u547c\u53eb\u4e00\u53ea\u55b5\u3011\u654f\u611f\u7684\u4eba\u5bb9\u6613\u53d7\u4f24|\u58eb\u5175\u5c0f\u7ad9\u97f3\u4e50\u53f0|18.256443 |\n|23883915|Vol72          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |18.256443 |\n|23918229|Vol13          |\u7eaf\u8272\u5fe7\u4f24\u7535\u53f0 |16.647385 |\n+--------+---------------+-------+----------+\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 7. Future work\n- Recommender for inactive users with song preference not in top 500\n- Better word embedding (Chinese or mixed language) for Song name and Singer\n- Collect more information about the song to improve explainability and effectiveness of recommendation \n- Streaming \n- User interface \n", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark21", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "pygments_lexer": "ipython3", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 1
}