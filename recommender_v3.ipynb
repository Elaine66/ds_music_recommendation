{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "## Table of contents\n1. [Load the data](#1.-Load-the-data) <br>\n    1.1 Load song table<br>\n    1.2 Load user table <br>\n    1.3 Load user play frequency table<br>\n2. [Spark performance basics](#2.-Spark-performance-basics)<br>\n3. [Explore the data with Spark APIs](#3.-Explore-the-data-with-Spark-APIs)<br>\n    3.1 [Clean song table](#3.1-Clean-song-table) <br>\n    3.2 [Clean play table and user table](#3.2-Clean-play-table-and-user-table) <br>\n4. [Visualize the data](#4.-Visualize-the-data)<br>\n5. [Build the recommender system](#5.-Build-the-recommender-system)<br>\n    5.1 Setup training and test set<br>\n    5.2 Train collaborative filtering<br>\n    5.3 Tune parameters <br>\n    5.4 Evaluate recommendation results<br>\n6. [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>\n    6.1 Setup vectors<br>\n    6.2 Compute similarity matrix<br>\n    6.3 Recommend for Selected user <br>\n    \n7. Summary and next steps", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, count, struct, row_number, when, isnan, log,lit\nfrom pyspark.sql.functions import round as cround\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 1. Load the data\n\nOnly for the first time. <br>\nCleaned data will be saved and could be imported in future. <<br>\n\nJump to other sections: \n- [Build the recommender system](#5.-Build-the-recommender-system)<br>\n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "cell_type": "markdown"
        }, 
        {
            "source": "spark = SparkSession.builder.getOrCreate()\nuser_table_raw = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'valid_user_highfreq.csv'))\n\nuser_table_raw.take(5)\n\nsong_table= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'song_table.csv'))\nprint(song_table.take(5))\n\nsong_freq = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'user_song_freq.csv'))\nprint(song_freq.take(5))\n\ndownload_table = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', 'clean_download.csv'))\nprint (download_table.take(5))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "user_db = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('musicrecommendation', '3_1uid.csv'))\nprint (user_db.select('uid').distinct().count())", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "264715\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 2. Spark performance basics", 
            "cell_type": "markdown"
        }, 
        {
            "source": "print (sc.defaultParallelism)\nprint (sc.getConf().toDebugString())\nprint (\"Number of partitions for the song_freq DataFrame: \" + str(song_freq.rdd.getNumPartitions()))", 
            "metadata": {}, 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "4\nhive.metastore.warehouse.dir=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf05-764985d1937dab-a222de131660/notebook/work/spark-warehouse\nspark.app.id=app-20171021174259-0155-332c25ad-3f8b-4fff-be46-e709b7342dff\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.host=10.143.133.71\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.driver.port=35869\nspark.eventLog.dir=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.id=driver\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.history.fs.logDirectory=/gpfs/fs01/user/sf05-764985d1937dab-a222de131660/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0022:7089\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v65/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.catalogImplementation=hive\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\nNumber of partitions for the song_freq DataFrame: 8\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## 3. Explore the data with Spark APIs", 
            "cell_type": "markdown"
        }, 
        {
            "source": "user_table_raw.show(truncate=False)\nprint (\"Number of users: \", user_table_raw.count())\nprint (\"Number of different users: \" + str(user_table_raw.select('uid').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+----------+---------+\n|_c0|Unnamed: 0|uid      |\n+---+----------+---------+\n|1  |1         |154563989|\n|2  |2         |154806874|\n|3  |3         |154777984|\n|4  |4         |154801899|\n|5  |5         |154522980|\n|6  |6         |154466362|\n|7  |7         |154467953|\n|8  |8         |158752252|\n|9  |9         |154559964|\n|10 |10        |154542883|\n|11 |11        |154828695|\n|12 |12        |154723056|\n|13 |13        |154751052|\n|14 |14        |154630129|\n|15 |15        |154684841|\n|16 |16        |154799108|\n|17 |17        |154786598|\n|18 |18        |154561771|\n|19 |19        |154508382|\n|20 |20        |154710857|\n+---+----------+---------+\nonly showing top 20 rows\n\nNumber of users:  264714\nNumber of different users: 264714\n"
                }
            ]
        }, 
        {
            "source": "song_table.show(truncate=False)\nprint (\"Number of songs: \", song_table.count())\nprint (\"Number of different songs: \" + str(song_table.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+-------------------------------+------+-----------+\n|song_id |song_type|song_name                      |singer|song_length|\n+--------+---------+-------------------------------+------+-----------+\n|602239  |null     |\u859b\u51ef\u742a                            |0     |null       |\n|160911  |null     |\u8521\u4f9d\u6797&\u5468\u6770\u4f26                        |0     |null       |\n|1033156 |null     |\u6c6a\u82cf\u6cf7                            |0     |null       |\n|294622  |null     |DJ\u821e\u66f2                           |0     |null       |\n|517174  |null     |\u68a6\u9e3d                             |0     |null       |\n|6606144 |null     |\u6768\u5c0f\u66fc&\u51b7\u6f20                         |0     |null       |\n|6432663 |null     |\u5c0f\u4e54                             |0     |null       |\n|6587633 |null     |\u97e9\u5b87                             |0     |null       |\n|6587662 |null     |\u97e9\u5b87                             |0     |null       |\n|158182  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|1037626 |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|995380  |null     |\u5f20\u5b66\u53cb                            |0     |null       |\n|529630  |null     |\u8bd5\u97f3\u789f                            |0     |null       |\n|3006068 |null     |\u7f51\u7edc\u6b4c\u624b                           |0     |null       |\n|4111186 |null     |\u5e7d\u5e7d                             |0     |null       |\n|1488120 |null     |Declan Masterson               |0     |null       |\n|21461735|null     |Lil Uzi Vert&Quavo&Travis Scott|0     |null       |\n|22399766|null     |\u5f20\u6770                             |0     |null       |\n|60057   |null     |\u7518\u840d                             |0     |null       |\n|116273  |null     |null                           |0     |null       |\n+--------+---------+-------------------------------+------+-----------+\nonly showing top 20 rows\n\nNumber of songs:  5047315\nNumber of different songs: 1559990\n"
                }
            ]
        }, 
        {
            "source": "song_table.printSchema()", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- song_length: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 3.1 Clean song table \n##### Need to clean song_table:\n- remove invalid song_id \n- get single entry for each song_id (most common song)\n- drop song_length column because 1) large variance 2) not very relevant", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# song_table.createOrReplaceTempView('song_table')\nsong_table.createOrReplaceTempView('song_table')\nsong_table_valid = spark.sql(\"select *  from song_table where song_id > 0 and song_id is not null\")\n\nprint (\"Number of songs: \", song_table_valid.count())\nprint (\"Number of different songs: \" + str(song_table_valid.select('song_id').distinct().count()))", 
            "metadata": {}, 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs:  3230980\nNumber of different songs: 1559987\n"
                }
            ]
        }, 
        {
            "source": "# get most common non-zero song_type\ntype_counts = song_table_valid.groupBy(['song_id', 'song_type'])\\\n    .count().alias('cnt')\\\n    .where(col('song_type') != '0')\n\nmax_type = (type_counts\n    .groupBy('song_id')\n    .agg(F.max(struct(col('count'), col('song_type'))).alias('max'))\n    .select(col('song_id'), col('max.song_type')))\n\n# get most common not null song_name \nname_counts = song_table_valid.groupBy(['song_id', 'song_name'])\\\n    .count().alias('cnt')\\\n    .where(col('song_name').isNotNull())\n    \nmax_name = (name_counts.groupBy('song_id')\n            .agg(F.max(struct(col('count'), col('song_name'))).alias('max'))\n            .select(col('song_id'),col('max.song_name')))\n\n\n# get most common not null singer \nsinger_counts = song_table_valid.groupBy(['song_id', 'singer']).count().alias('cnt').where(col('singer').isNotNull())\nw = Window().partitionBy('song_id').orderBy(col('count').desc())\nmax_singer = (singer_counts\n              .withColumn('rn', row_number().over(w))\n              .where(col('rn')==1)\n              .select('song_id', 'singer'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "note: tried pandas - takes a long time for me; both Window or struct should work ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "print (max_type.select('song_id').distinct().count())\nprint (max_name.select('song_id').distinct().count())\nprint (max_singer.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "231216\n1559532\n1547516\n"
                }
            ]
        }, 
        {
            "source": "songs = song_table_valid.select('song_id').distinct().alias('songs')\nsong_unique = songs\\\n    .join(max_type, 'song_id','left')\\\n    .join(max_name, 'song_id','left')\\\n    .join(max_singer, 'song_id','left')\\\n    .select('song_id', 'song_type','song_name', 'singer')\\\n\nsong_unique = song_unique.na.fill('0', subset=['song_type'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.show()\nprint (song_unique.select('song_id').distinct().count())", 
            "metadata": {}, 
            "execution_count": 22, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+--------------------+----------------+\n| song_id|song_type|           song_name|          singer|\n+--------+---------+--------------------+----------------+\n|  100140|        0|             \u5929\u5916\u5929\u4e0a\u5929\u65e0\u6daf|             \u9648\u6d01\u4e3d|\n|10015022|        0|                \u6700\u540e\u4e00\u6b21|             \u859b\u6653\u67ab|\n| 1003644|        0|Save The One, Sav...|  T.M.Revolution|\n| 1004266|        0|        Broken heart|             \u9ec4\u4e49\u8fbe|\n| 1006370|        0|           \u4e8c\u5341\u56db\u5f0f\u592a\u6781\u62f3\u97f3\u4e50|             \u7eaf\u97f3\u4e50|\n| 1006422|        0|                  \u6d63\u7eb1|              \u6731\u6d01|\n|10065669|        1|                \u82b1\u623f\u59d1\u5a18|              \u5d14\u5065|\n|10071852|        0|Dirt Road Anthem ...|  Country Nation|\n|10087323|        0|Helden sterben ei...| Michael Wendler|\n| 1009129|        1|    Dietro L'Incanto|Ludovico Einaudi|\n|  100964|        1|                  \u95ee\u60c5|             \u9ec4\u601d\u5a77|\n| 1010103|        0|Sunday Sunshine \uff0f...|          \u3044\u3068\u3046\u304b\u306a\u3053|\n|10101536|        0|          \u6253\u51fb\u4e50\u66f2 \u7235\u58eb\u9f13\u72ec\u594f| Various Artists|\n|  101021|        0|           I Believe|             Era|\n|10102878|        0|               \u4e1c\u5317\u795e\u9ea62|            MC\u5531\u5c06|\n|10107689|        0|           \u4f60\u6ca1\u9519,\u662f\u6211\u7231\u591a\u4e86|            MC\u5bd2\u9f99|\n|  101122|        0|           \u9633\u5149\u7167\u8000\u6211\u7684\u7834\u8863\u88f3|            \u5149\u5934\u674e\u8fdb|\n|  101272|        0|                \u4e0d\u8981\u60f9\u6211|              \u6731\u8335|\n|10139662|        1|                  \u843d\u82b1|             \u674e\u5c0f\u7490|\n|10141277|        0|           Baby Tree|           \u8096\u5c71&\u53ef\u8d1d|\n+--------+---------+--------------------+----------------+\nonly showing top 20 rows\n\n1559987\n"
                }
            ]
        }, 
        {
            "source": "song_unique.groupBy('song_type').count().sort(col('count').desc()).show()", 
            "metadata": {}, 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+-------+\n|song_type|  count|\n+---------+-------+\n|        0|1328771|\n|        1| 155797|\n|        2|  65042|\n|        3|  10053|\n|       73|     22|\n|       90|     18|\n|       89|     12|\n|       91|     12|\n|       48|      7|\n|       88|      6|\n|       43|      6|\n|       66|      6|\n|       60|      6|\n|       26|      6|\n|       30|      6|\n|       27|      5|\n|       41|      5|\n|       33|      5|\n|       82|      5|\n|       32|      4|\n+---------+-------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "song_unique.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_song.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 3.2 Clean play table and user table\n- remove extremely high frequency (0.9999 quantile at approximately 1000 play frequency)\n- remove play history without uid \n\nNumber of valid users:  264708 <br>\nNumber of invalid users:  186", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_freq = song_freq.withColumn('freq', song_freq['freq'].cast('integer'))\nsong_freq = song_freq.filter((col('uid').isNotNull()) & (col('uid') > '0') &\n                             (col('song_id').isNotNull()) & (col('song_id')>'0'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_freq.sort(col('freq').desc()).show()", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+-----+\n|      uid|device| song_id| freq|\n+---------+------+--------+-----+\n|167982849|    ar| 4554016|73609|\n|  1791497|    ar| 3401476|51858|\n|   751824|    ar| 9950164|46970|\n|  1685126|    ar|15249349|41265|\n|  1685126|    ar| 9950164|39207|\n| 37025504|    ar| 9950164|35198|\n|  1791497|    ar|15198178|30975|\n| 37025504|    ar|15249349|29830|\n|  1791497|    ar| 6468891|26765|\n|  1791497|    ar|  442265|23907|\n|  1685126|    ar| 5237384|22949|\n|  1791497|    ar| 9950164|22849|\n| 22730453|    ar| 7005106|22294|\n| 22730453|    ar| 5965686|22289|\n|   497685|    ar| 9950164|21574|\n|  1062806|    ar| 9950164|20929|\n|  1791497|    ar| 5245130|19813|\n| 37025504|    ar| 5237384|19799|\n|  1685126|    ar| 6468891|19709|\n|  1791497|    ar|  125802|19697|\n+---------+------+--------+-----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "cutoff_freq = song_freq.approxQuantile('freq', [0.99], 0.005)\nprint (cutoff_freq)", 
            "metadata": {}, 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[54.0]\n"
                }
            ]
        }, 
        {
            "source": "cutoff_freq = 1000", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "extreme_freq = song_freq.filter(col('freq') >= cutoff_freq)\n# get valid song played frequency\nvalid_freq = song_freq.filter(col('freq') < cutoff_freq)\n\n# get valid users \noutlier_user = extreme_freq.select('uid').distinct()\noutlier_user.createOrReplaceTempView('filter_view')\nvalid_user = user_table_raw.where('uid not in (select uid from filter_view)')", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print (\"Number of valid users: \", valid_user.select('uid').distinct().count())\nprint (\"Number of invalid users: \", outlier_user.select('uid').distinct().count())\n# Number of valid users:  264708\n# Number of invalid users:  186", 
            "metadata": {}, 
            "execution_count": 19, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of valid users:  264708\nNumber of invalid users:  186\n"
                }
            ]
        }, 
        {
            "source": "# too confusing to add this feature\n# song_length = song_table.groupBy(['song_id', 'length']).avg(col('song_length')).alias('avg_length').where((col('song_length').isNotNull()) & (col('song_length') > '200') & (col('song_length') <'720'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "download_table.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+-----------+------+----------+------------------------------+---------+\n|_c0|uid        |device|song_id   |song_name                     |paid_flag|\n+---+-----------+------+----------+------------------------------+---------+\n|0  |null       |ip    |6945370.0 |null                          |null     |\n|1  |1685126.0  |ar    |170455.0  |\u987a\u6d41\u3001\u9006\u6d41                         |null     |\n|2  |736305.0   |ar    |23380344.0|\u4e00\u4eba\u6211\u558a\u53e6\u7c7b(\u4f24\u611f\u7248)                   |null     |\n|3  |168042561.0|ar    |6292506.0 |\u5e1d\u90fd                            |null     |\n|4  |1749320.0  |ar    |21473237.0|\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1                      |null     |\n|5  |155948236.0|ar    |93388.0   |\u4e9a\u62c9\u4f2f\u8df3\u821e\u5973\u90ce                       |null     |\n|6  |167794453.0|ar    |497722.0  |\u85d5\u65ad\u4e1d\u8fde                          |null     |\n|7  |168505311.0|ip    |4188142.0 |null                          |null     |\n|8  |168031064.0|ar    |4243838.0 |\u7231\u60c5\u7801\u5934(2651,cn \u5929\u5730\u4eba\u97f3\u4e50\u7f51)          |null     |\n|9  |167626177.0|ar    |1080516.0 |\u65e0\u6cd5\u539f\u8c05(\u7535\u89c6\u5267\u300a\u56de\u5bb6\u7684\u8bf1\u60d1\u300b\u4e3b\u9898\u66f2)           |null     |\n|10 |736305.0   |ar    |344885.0  |Because Of You                |null     |\n|11 |736305.0   |ar    |119699.0  |\u522b\u519b\u8425                           |null     |\n|12 |1685126.0  |ar    |3042675.0 |\u65f6\u5149\u673a\u524d\u594f-\u4e94\u6708\u5929_\u4e94\u6708\u5929(\u94c3\u58f0)             |null     |\n|13 |167691874.0|ar    |3287563.0 |\u5176\u5b9e\u90fd\u6ca1\u6709                         |null     |\n|14 |736305.0   |ar    |6792401.0 |Dream It Possible             |null     |\n|15 |167973828.0|ar    |10901925.0|Overtime (Vicetone Remix Edit)|null     |\n|16 |736305.0   |ar    |9925786.0 |Dream It Possible(34\u79d2\u94c3\u58f0\u7248)     |null     |\n|17 |167784467.0|ar    |8050943.0 |\u8036\u5229\u4e9a\u5973\u90ce                         |null     |\n|18 |168007277.0|ip    |4373829.0 |null                          |null     |\n|19 |167782564.0|ar    |325517.0  |\u68a6\u9a7c\u94c3                           |null     |\n+---+-----------+------+----------+------------------------------+---------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "song_freq.show(truncate=False)", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+------+--------+----+\n|uid      |device|song_id |freq|\n+---------+------+--------+----+\n|751824   |ar    |6483029 |385 |\n|168156920|ip    |6792060 |5   |\n|497685   |ar    |7207401 |26  |\n|1062806  |ar    |6841262 |50  |\n|168195436|ar    |12808784|22  |\n|1685126  |ar    |59582   |26  |\n|168286187|ar    |4188404 |2   |\n|37025504 |ar    |481552  |733 |\n|168478031|ar    |9822502 |4   |\n|168406030|ar    |909773  |7   |\n|168410987|ar    |5425869 |10  |\n|168511270|ar    |6817428 |68  |\n|168115240|ar    |23665227|2   |\n|168396372|ar    |4276822 |10  |\n|168417737|ar    |5383328 |19  |\n|1062806  |ar    |20870989|73  |\n|168373631|ar    |7202991 |60  |\n|168335848|ip    |4112638 |15  |\n|37025504 |ar    |1108956 |76  |\n|168453430|ar    |1705363 |1   |\n+---------+------+--------+----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "download_table.select('uid').distinct().count()", 
            "metadata": {}, 
            "execution_count": 143, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "242243"
                    }, 
                    "metadata": {}, 
                    "execution_count": 143, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "valid_user.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    \nvalid_freq.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'cleaned_freq.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 4. Visualize the data\n###  TODO\nUsing Seaborn and matplotlib", 
            "cell_type": "markdown"
        }, 
        {
            "source": "!pip install seaborn", 
            "metadata": {}, 
            "execution_count": 164, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): seaborn in /usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages\r\n"
                }
            ]
        }, 
        {
            "source": "import seaborn as sns\n%matplotlib inline", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "import pandas as pd\nfreqPandas = valid_freq.toPandas()\ndlPandas = download_table.toPandas()\nsns.lmplot(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "plot matrix for song played by user, with frequency as the hue ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "min(freqPandas['uid'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sns.palplot(sns.diverging_palette(10, 133, sep=80, n=10))\n# user_id: max 100047599, min 99983627\n# song_id: max 2147483647, min 2\nsns.(x='uid', y='song_id', data = dlPandas, fit_reg=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 5. Build the recommender system\n(Cited from IBM bluemix Data Science Experience (DSX) document) <br>\n\n\"\nCollaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who have similar preference on the same item will also have similar opinions on items that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users versus all songs. Most cells in the matrix will be empty. An empty cell means the user hasn't played the song yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for song factors and solving for user factors.\n\"\n\n\nChallenge in this recommender system: <br>\n1. Small number of play history that could be shown by sparsity of the utility matrix\n2. Limited features for songs - mixed language\n\nSolution: <br>\n1. Hybrid", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# restarting kernel\nreload = True", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# load saved files if starting from middle\nif reload: \n    song_unique = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_song.csv'))\n    song_unique.take(5)\n    \n    valid_freq = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_freq.csv'))\n    valid_freq.take(5)\n    \n    valid_user = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'cleaned_user.csv'))\n    valid_user.take(5)\n    \n    download_table = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(bmos.url('musicrecommendation', 'clean_download.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.printSchema()\nsong_unique.count()", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: string (nullable = true)\n |-- song_type: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\n"
                }, 
                {
                    "data": {
                        "text/plain": "1559987"
                    }, 
                    "metadata": {}, 
                    "execution_count": 5, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "valid_freq = valid_freq.withColumn('uid', valid_freq['uid'].cast('integer'))\nvalid_freq = valid_freq.withColumn('song_id', valid_freq['song_id'].cast('integer'))\nvalid_freq = valid_freq.withColumn('freq', valid_freq['freq'].cast('double'))\n\nvalid_user = valid_user.withColumn('uid', valid_user['uid'].cast('integer'))\n\nsong_unique = song_unique.withColumn('song_type', song_unique['song_type'].cast('integer'))\nsong_unique = song_unique.withColumn('song_id', song_unique['song_id'].cast('integer'))\n\nvalid_download = download_table.withColumn('uid', download_table['uid'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\nvalid_download = valid_download.withColumn('song_id', valid_download['song_id'].cast('integer'))\n\n# valid_freq = valid_freq.withColumn('label',log(10.0, valid_freq.freq))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_unique.createOrReplaceTempView('song_unique')\nvalid_freq.createOrReplaceTempView('valid_freq')\nsong_freq = spark.sql(\"\"\"\n    select s.song_type, s.song_id, s.song_name, s.singer, COALESCE(f.cnt,0) as freq\n    from song_unique as s\n    left join\n        (select song_id, count(*) as cnt from valid_freq group by song_id) as f\n        on f.song_id = s.song_id\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# take log of the frequency and scale to -1 to 100 representing degree of preference \n\nvalid_freq2 = valid_freq.withColumn('label',cround(log(10.0, valid_freq.freq)/3.0*10,0))\nvalid_freq2 = valid_freq2.replace(0, -1, subset=['label'])\n# should be None \nvalid_freq2 = valid_freq2.na.fill(0, subset=['label'])\n\n# print(valid_freq2.groupBy().max('label').show())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# combine with download table\n\nvalid_download = valid_download.withColumn('download', lit(1))\nvalid_download = valid_download.drop('device')\nvalid_download = valid_download.drop('song_name')\n\nvalid_score = valid_freq2.join(valid_download, ['uid','song_id'], 'left_outer')\nvalid_score = valid_score.na.fill(0, subset=['download'])\n\nvalid_score = valid_score.withColumn('label', when(valid_score.download==1, lit(10)).otherwise(valid_score.label))\n\nvalid_score = valid_score.replace(-0.5, -1, subset=['label'])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Jump to other sections: \n- [Hybrid recommender system](#6.-Hybrid-recommender-system)<br>", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_unique.printSchema()\nvalid_user.printSchema()\nvalid_freq.printSchema()\ndownload_table.printSchema()", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: string (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: string (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "valid_freq.printSchema()\nvalid_user.printSchema()\nsong_unique.printSchema()\nvalid_download.printSchema()", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- uid: integer (nullable = true)\n\nroot\n |-- song_id: integer (nullable = true)\n |-- song_type: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n |-- uid: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "##### checking\n# check sparsity \ncounts_freq = valid_freq.count()\nprint ('Number of song frequency entried: ', counts_freq)\n# songs played/( songs x users)\npercentage = (counts_freq)*1.0/1559987/264708\nprint ('Percentage of song played: ', percentage)", 
            "metadata": {}, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of song frequency entried:  32801295\nPercentage of song played:  7.943336195316835e-05\n"
                }
            ]
        }, 
        {
            "source": "##### checking\n\nmoreThanOnce = valid_freq.groupBy('song_id').count().alias('cnt').where(col('count') > 1).select('song_id').count()\nprint ('Number of songs played more than once: ', moreThanOnce)\n# significantly less, only recommend songs played more than once\n# utility matrix will be less sparse\n# number of song frequency entries will be reduce by 686993(only played once) -32114102, which is 2%", 
            "metadata": {}, 
            "execution_count": 28, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Number of songs played more than once:  872994\n"
                }
            ]
        }, 
        {
            "source": "print ('Percentage of filled matrix: ', round((32801295-686993)*100.0/(872994*264708),4))\n# vs. ~0.0073% filled ", 
            "metadata": {}, 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Percentage of filled matrix:  0.0139\n"
                }
            ]
        }, 
        {
            "source": "song_freq.printSchema()\nsong_freq.count()", 
            "metadata": {}, 
            "execution_count": 26, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = true)\n |-- singer: string (nullable = true)\n |-- freq: long (nullable = false)\n\n"
                }, 
                {
                    "data": {
                        "text/plain": "1559987"
                    }, 
                    "metadata": {}, 
                    "execution_count": 26, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "##### checking\n\nprint (song_freq.select('song_id').count())\nprint (song_freq.select('song_id').where(col('freq')>1).count())\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "1559987\n872994\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.1 Setup training and test set\n\nsetup: 80% training and 20% test set ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "(trainingFreq, testFreq) = valid_score.randomSplit([80.0, 20.0])\n\ntrainingFreq.printSchema()\n# utility_matrix_small.select([count(when(isnan(c), c)).alias(c) for c in utility_matrix_small.columns]).show()\ntestFreq.printSchema()", 
            "metadata": {}, 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\nroot\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- device: string (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n |-- _c0: string (nullable = true)\n |-- paid_flag: string (nullable = true)\n |-- download: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.2 Setup collaborative filtering model\n\nAccoding to DSX document again:\n\"\nA NaN result is due to [SPARK-14489](https://issues.apache.org/jira/browse/SPARK-14489) and because the model can't predict values for users for which there's no data.\"", 
            "cell_type": "markdown"
        }, 
        {
            "source": "model = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\").fit(trainingFreq)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "predictions = model.transform(testFreq)\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))", 
            "metadata": {}, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The root mean squared error for our model is: 2.917168466920176\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 5.3 Tune parameters\n\nCross validation and grid search to tune for hyperparameters\n\nALS algorithm :\n```python\n    class pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )\n```\n\nThe ALS hyperparameters are:\n- `rank` = the number of latent factors in the model\n- `maxIter` = the maximum number of iterations \n- `regParam` = the regularization parameter", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# takes way too long, killed at second run\n\n(trainingScore, validationScore) = valid_score.randomSplit([90.0, 10.0])\n\nals = ALS(userCol=\"uid\", itemCol=\"song_id\", ratingCol=\"label\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n\n# paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 3, 5, 7]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,2,3]).addGrid(als.regParam, [0.05, 0.1]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = crossval.fit(trainingScore)\npredictions = cvModel.transform(validationScore)\n\nprint (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.drop())))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print ('Best rank is: ', cvModel.bestModel.rank)\nprint ('Best regularizer is: ', cvModel.bestModel.params)\n# evaluate with 0 for null prediction\n# print (\"The root mean squared error for our model is: \" + str(evaluator.evaluate(predictions.na.fill(0))))\n# The root mean squared error for our model is: 3.1921429126212857", 
            "metadata": {}, 
            "execution_count": 136, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Best rank is:  1\nBest regularizer is:  []\n"
                }
            ]
        }, 
        {
            "source": "valid_score.printSchema()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "predictions.coalesce(1).write\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .save(bmos.url('musicrecommendation', 'predictions_CF_1015.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 5.4 Evaluate recommendation results\n\n- MSE is low -> recommender seems to be pretty good\n- However, looking at recommended songs for individual users, it seems to recommending weird songs\n- Therefore, need alternative for inactive users for a more explainable model\n\nDetailed evaluation as following:", 
            "cell_type": "markdown"
        }, 
        {
            "source": "utility_matrix_small = valid_user.crossJoin(song_freq.select('song_id')).select('uid','song_id')\nutility_matrix_small = utility_matrix_small.join(valid_score, ['uid', 'song_id'], 'left_outer').select('uid', 'song_id', 'label')\n\nutility_matrix = utility_matrix_small.na.fill('0', subset=['label'])\n\n# Replace predicted NaN values with the average frequency and evaluate the model\n# avgScore = utility_matrix.select('label').groupBy().avg().first()[0]\n# print (\"The average score in the dataset is: \" + str(avgScore))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "predictions.createOrReplaceTempView('pred_subset')\npred_first20 = spark.sql('select uid, song_id, label,download, prediction from pred_subset order by uid limit 100').show()", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------+-----+--------+-----------+\n|  uid| song_id|label|download| prediction|\n+-----+--------+-----+--------+-----------+\n|12333|21596231|  1.0|       0|  2.4781432|\n|12333|   55219|  3.0|       0|  1.4091263|\n|12333|  708667|  4.0|       0|  3.9749548|\n|12333| 2725093|  3.0|       0|  4.3128133|\n|12333| 5114569|  6.0|       0|   4.895308|\n|36816| 6906526| -1.0|       0|   4.813818|\n|60183| 3627946| -1.0|       0|-0.13519855|\n|60183|  116329| -1.0|       0| -0.2599472|\n|60183|23610522| -1.0|       0| -0.7154152|\n|60183|20866010|  2.0|       0| 0.11216657|\n|60183| 1242385|  1.0|       0|-0.47428238|\n|60183|  134967|  1.0|       0| 0.13887726|\n|60183|21234365| -1.0|       0|-0.37378824|\n|60183| 6227103| -1.0|       0| 0.34997967|\n|60183|  710442| -1.0|       0|-0.09907829|\n|60183| 6128890| -1.0|       0|-0.11929984|\n|60183|  223973|  1.0|       0| 0.30912238|\n|60183|  981246| -1.0|       0| -0.6802237|\n|60183|23641903| -1.0|       0| 0.11194147|\n|60183|  706554| -1.0|       0| 0.29457986|\n+-----+--------+-----+--------+-----------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 44, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print (\"Recommendations for user 169262317:\")\n# recommendCF(model, 169262317, 10)\n# print \"Recommendations for user 471:\"\n# recommendCF(cvModel, 471, 10)\n# print \"Recommendations for user 496:\"\n# recommendCF(cvModel, 496, 10)\n\nprint (\"Recommendations for user 12333:\")\nrecommendCF(model, 12333, 10)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# total user:264708 \n# select 12333\nselect_valid = valid_score.createOrReplaceTempView('select_valid')\nselected = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_valid\n    where uid = 12333\n    group by uid\n    order by 2 asc \n\"\"\")\n# inactive_user.count()\n# treshold at 1: 68268 -> around a quarter \n# treshold at 5: 154568 -> more than half \n# treshold at 2: 98849 -> around one third", 
            "metadata": {}, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "98849"
                    }, 
                    "metadata": {}, 
                    "execution_count": 40, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "selected.show()\n\nprint (\"Recommendations for user 12333:\")\nrecommendCF(model, 12333, 10)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "inactive_user.show()", 
            "metadata": {}, 
            "execution_count": 37, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+---+\n|      uid|cnt|\n+---------+---+\n|168984483|  1|\n|168275548|  1|\n|168572441|  1|\n|168532375|  1|\n|168510789|  1|\n|168713236|  1|\n|168686527|  1|\n|168683346|  1|\n|167801928|  1|\n|168173268|  1|\n|168889990|  1|\n|168779601|  1|\n|169042674|  1|\n|168266657|  1|\n|167640755|  1|\n|168029402|  1|\n|168767795|  1|\n|167579971|  1|\n|168902557|  1|\n|168975045|  1|\n+---------+---+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "print (\"Recommendations for user 167579971:\")\nrecommendCF(model, 167579971, 10)\n\nprint (\"Recommendations for user 168975045:\")\nrecommendCF(model, 168975045, 10)\n\n# 169042674 0\n# 167579971 10\n# 169042674 0\n# 167640755 10\n\n\n# valid_score.select().where(col('uid')==169042674).show()\n# problem is it's not in the user list", 
            "metadata": {}, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Recommendations for user 167579971:\n+--------+-----------------+----------------+----------+\n|song_id |song_name        |singer          |prediction|\n+--------+-----------------+----------------+----------+\n|11109921|\u514b\u535c\u52d2 (Cover)      |\u90d1\u5c0f\u5b87             |3.387147  |\n|5009810 |\u8302\u540d\u8fd9\u573a\u96e8            |\u79e6\u9f50              |3.3330908 |\n|22399046|\u9f13\u4e0e\u82b1              |\u8427\u5fc6\u60c5Alex         |3.2971313 |\n|7190353 |\u5ff5\u5ff5\u4e0d\u5fd8\u7684\u59d1\u5a18(1\u520602\u79d2\u94c3\u58f0\u7248)|\u963f\u6743              |3.035895  |\n|413840  |Ich verzeih' Dir |Veronika Fischer|2.9503236 |\n|6110029 |\u660e\u5929(37\u79d2\u94c3\u58f0\u7248)       |\u8427\u4e9a\u8f69             |2.6847723 |\n|22827582|Come Together    |Michael Jackson |2.626977  |\n|2697950 |\u5728\u96e8\u4e2d\u6f2b\u6b65            |\u4ed8\u5a1c              |2.5599701 |\n|366974  |\u96ea\u67d3\u7684\u98ce\u91c7            |\u9648\u8bfa              |2.257006  |\n|6922682 |\u8bf7\u4f60\u50cf\u6211\u8fd9\u6837\u505a-\u8ddf\u6211\u6765-(\u7ea2\u679c\u679c)|\u513f\u7ae5\u6545\u4e8b            |2.2517805 |\n+--------+-----------------+----------------+----------+\n\nRecommendations for user 168975045:\n+--------+-------------------------+----------------+----------+\n|song_id |song_name                |singer          |prediction|\n+--------+-------------------------+----------------+----------+\n|81330   |\u5411\u65e5\u8475\u7684\u82b1\u5b63                   |\u4fdd\u5251\u950b             |12.676041 |\n|1240655 |Go In, Go Hard           |Wretch 32       |10.050616 |\n|1240642 |Bright Lights            |\u7535\u5f71\u539f\u58f0            |10.050616 |\n|15145086|\u5fae\u7b11\u7684\u5411\u65e5\u8475(\u4f34\u594f\u7248)              |BEJ48           |9.34867   |\n|13165006|??? ???                  |\u88f4\u79c0\u667a             |9.190221  |\n|13967998|And...???                |\u767d\u667a\u82f1             |9.190221  |\n|859831  |free storm               |\u88f4\u6da9\u742a             |9.002642  |\n|413840  |Ich verzeih' Dir         |Veronika Fischer|8.934969  |\n|6199318 |\u8d24\u826f(39\u79d2\u94c3\u58f0\u7248)               |\u82cf\u9633              |8.810483  |\n|5725764 |\u571f\u8c6a\u624b\u673a\u94c3\u58f0 \u5de7\u5999\u5236\u4f5c Www.Y2002.Com|\u7f51\u7edc\u6b4c\u624b            |8.637566  |\n+--------+-------------------------+----------------+----------+\n\n"
                }
            ]
        }, 
        {
            "source": "dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n# Create a Spark DataFrame with the movies that have already been rated by this user\nsongsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n# Apply the recommender system to the data set without the already rated movies to predict ratings\npredictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n# Join with the movies DataFrame to get the movies titles and genres\nrecommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction)\n\nrecommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## 6. Hybrid recommender system\n\nsupplemnt the collaborative filtering with item-item recommender <br>\nfor user with only 1 song played history (or two):<br>\n- recommend songs based on cosine similarity to the song/songs played by the user \n- (not implemented) for user with 0 songs player: recommend top k songs  \n\n- tries implementing on all 800k songs, failed after countless trial\n- eventually turning to subsample ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from pyspark.mllib.linalg import Vectors\n# Package for distributed linear algebra DIMSUM\n# Dimension Independent Matrix Square using MapReduce\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.types \\\nimport ArrayType, StringType, DoubleType, StructType, StructField\n\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.mllib.linalg.distributed \\\nimport IndexedRowMatrix,IndexedRow, RowMatrix, BlockMatrix,CoordinateMatrix\n", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### 6.1 Setup feature vector\n\nGet feature vectors for top 500 songs and save it for future use; it's okay to run ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "song_freq = song_freq.where(col('freq')>1).orderBy('freq', ascending=False)\n\ntop_songs = song_freq.orderBy('freq', ascending =False).limit(500)\ntop_songs = top_songs.na.fill('0', subset=['singer','song_name'])\n\ntop_songs = top_songs.withColumn('id', monotonically_increasing_id()+1)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "if not reload:\n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500.csv'))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# convert to VectorAssembler \nfrom pyspark.ml.feature import VectorAssembler\n\ndef extract(row):\n    return (row.song_id, row.song_type, ) + tuple(row.name_vec.toArray().tolist()) + tuple(row.singer_vec.toArray().tolist())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "song_freq.count()", 
            "metadata": {}, 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "872994"
                    }, 
                    "metadata": {}, 
                    "execution_count": 16, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "reload", 
            "metadata": {}, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "True"
                    }, 
                    "metadata": {}, 
                    "execution_count": 17, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# get a dataframe of vector\nif not reload:\n    \n    song_str2arr = top_songs.withColumn('singer_arr', split(col(\"singer\"), \" \").cast(ArrayType(StringType())).alias(\"singer_arr\"))\n    w2v= Word2Vec(vectorSize=3, minCount=0, inputCol=\"singer_arr\", outputCol=\"singer_vec\")\n    top_song_vec = w2v.fit(song_str2arr).transform(song_str2arr)\n\n    top_song_vec = top_song_vec.withColumn('name_arr', split(col(\"song_name\"), \" \").cast(ArrayType(StringType())).alias(\"name_arr\"))\n    w2v = Word2Vec(vectorSize=5, minCount=0, inputCol=\"name_arr\", outputCol=\"name_vec\")\n    top_song_vec = w2v.fit(top_song_vec).transform(top_song_vec)\n    \n    top_vec_temp = top_song_vec.select('song_type','name_vec','singer_vec', 'song_id')\n    # save as dataframe \n    top_vec_df = top_vec_temp.rdd.map(extract).toDF()\n    top_vec_df = top_vec_df.withColumn(\"id\", monotonically_increasing_id())\n    top_vec_df = top_vec_df.withColumn('_1', top_vec_df['_1'].cast('double'))\n    \n    top_songs.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500.csv'))\n\n    top_vec_df.coalesce(1).write\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .save(bmos.url('musicrecommendation', 'top_500_vec.csv'))\n    \n\nelse:\n    top_vec_df = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_vec.csv'))    \n    \n    sim_df_ori = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .option(\"inferschema\", \"true\")\\\n      .load(bmos.url('musicrecommendation', 'top_500_simi.csv'))    \n    \n    sim_df = sim_df.withColumn('id', sim_df.id+1).orderBy('id')\n    sim_df.select('id').show()\n    ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sim_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferschema\", \"true\")\\\n  .load(bmos.url('musicrecommendation', 'top_500_simi.csv')) ", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 229, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Jump to other sections: \n- [Recommend for selected users](#6.3.-Recommend-for-selected-users)", 
            "cell_type": "markdown"
        }, 
        {
            "source": "top_vec_df.printSchema()", 
            "metadata": {}, 
            "execution_count": 25, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: integer (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- _10: double (nullable = true)\n |-- id: integer (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "top_songs.show()\n1    0.000000\n2    0.000000\n3    0.000000\n4    0.000000\n5    0.000000\n6    0.991990\n7    0.996428\n8    0.990109\n9    0.991140\n10   0.983688\n11   0.989649\n12   0.996449\n13   0.992721\n14   0.961659\n15   0.985563\n16   0.981806\n17   0.990653\n18   0.956802\n19   0.984632\n20   0.956802", 
            "metadata": {}, 
            "execution_count": 19, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+--------+--------------------+-----------+------+\n|song_type| song_id|           song_name|     singer|  freq|\n+---------+--------+--------------------+-----------+------+\n|        2|15249349|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|    \u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|        2| 9950164|               \u521a\u597d\u9047\u89c1\u4f60|        \u674e\u7389\u521a|102218|\n|        2| 5237384|                \u9006\u6d41\u6210\u6cb3|        \u91d1\u5357\u73b2| 71834|\n|        2| 6468891|                  \u6f14\u5458|        \u859b\u4e4b\u8c26| 68042|\n|        2|15807836|\u4e09\u751f\u4e09\u4e16-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843...|         \u5f20\u6770| 58433|\n|        2| 5114569|          \u6ca1\u6709\u4f60\u966a\u4f34\u771f\u7684\u597d\u5b64\u5355|         \u68a6\u7136| 53102|\n|        2| 3287564|                 \u5c0f\u65f6\u5019|        \u82cf\u6253\u7eff| 52403|\n|        2|16400733|\u601d\u6155-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|        \u90c1\u53ef\u552f| 48343|\n|        2| 6657692|             \u8d70\u7740\u8d70\u7740\u5c31\u6563\u4e86|        \u5e84\u5fc3\u598d| 47438|\n|        2| 3620537|              \u4f60\u8fd8\u8981\u6211\u600e\u6837|        \u859b\u4e4b\u8c26| 43961|\n|        2| 7149583|                \u544a\u767d\u6c14\u7403|        \u5468\u6770\u4f26| 41887|\n|        2| 6749207|               Faded|Alan Walker| 38523|\n|        2| 3971731|               \u4ee5\u540e\u7684\u4ee5\u540e|        \u5e84\u5fc3\u598d| 38138|\n|        1|23498554|                \u52a8\u7269\u4e16\u754c|        \u859b\u4e4b\u8c26| 37840|\n|        2|23082492|                  \u9ad8\u5c1a|        \u859b\u4e4b\u8c26| 37762|\n|        2| 4147754|  \u4e11\u516b\u602a-(\u7535\u89c6\u5267\u300a\u5982\u679c\u6211\u7231\u4f60\u300b\u63d2\u66f2)|        \u859b\u4e4b\u8c26| 34504|\n|        2|13273544|                \u52c9\u4e3a\u5176\u96be|         \u738b\u5195| 33689|\n|        1|16827761|                  \u6210\u90fd|         \u8d75\u96f7| 33348|\n|        2| 6449273|                  \u7ec5\u58eb|        \u859b\u4e4b\u8c26| 32905|\n|        1| 7176392|                  \u6210\u90fd|         \u8d75\u96f7| 31211|\n+---------+--------+--------------------+-----------+------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "top_songs.filter(col('id') == 315).show()", 
            "metadata": {}, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+-------+---------+------+----+---+\n|song_type|song_id|song_name|singer|freq| id|\n+---------+-------+---------+------+----+---+\n|        2| 203139|      \u84dd\u83b2\u82b1|    \u8bb8\u5dcd|7257|315|\n+---------+-------+---------+------+----+---+\n\n"
                }
            ]
        }, 
        {
            "source": "top_vec_df.printSchema()", 
            "metadata": {}, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- _1: double (nullable = true)\n |-- _2: double (nullable = true)\n |-- _3: double (nullable = true)\n |-- _4: double (nullable = true)\n |-- _5: double (nullable = true)\n |-- _6: double (nullable = true)\n |-- _7: double (nullable = true)\n |-- _8: double (nullable = true)\n |-- _9: double (nullable = true)\n |-- id: long (nullable = false)\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 6.2 Compute similarity matrix\n#### using pyspark's distributed matrix\n\nunderstanding the goal: is to get single row of similarities when needed <br>\nproblem: \n\np1: columnSimilarities: need to transpose the matrix -> using toBlockMatrix().transpose() results in absurdly large matrix<br>\nSolved: manually transpose at DataFrame<br>\n\np2: distributed container does not allow random access to computed similarity matrix <br>\nTrial and Error:\n- to RDD? takes a long time, and very likely, does not have enough memory for a local matrix\n- do matrix multiplication for BlockMatrix? takes a long time\n- broadcast to different workers, couldn't do patten match from csr_matrix \n- reduced to 872994", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# manuely transpose the dataframe - 20s\ntop_vec_rdd = top_vec_df.rdd\ndata = []\nfor i in range(9):\n    data.append(top_vec_rdd.map(lambda row: row[i]).collect())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# manurally construct IndexedRow\n# indexedT = IndexedRow(0, Vectors.dense(data[0]))\n\ntoIndexedRows = [IndexedRow(i, data[i]) for i in range(9)]\nindexedRows = sc.parallelize(toIndexedRows)\nmat = IndexedRowMatrix(indexedRows)\n\nrowMat = mat.toRowMatrix()\n\n# usualy takes long, but when u'r luck: 40s\nsimThres = rowMat.columnSimilarities(0.05)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "top_song_vec.printSchema()", 
            "metadata": {}, 
            "execution_count": 45, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- song_type: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- song_name: string (nullable = false)\n |-- singer: string (nullable = false)\n |-- freq: long (nullable = false)\n |-- id: long (nullable = false)\n |-- singer_arr: array (nullable = false)\n |    |-- element: string (containsNull = true)\n |-- singer_vec: vector (nullable = true)\n |-- name_arr: array (nullable = false)\n |    |-- element: string (containsNull = true)\n |-- name_vec: vector (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "source": "# convert to matrix \n# this shows RDD is empty error on the second run \n\n# mat = IndexedRowMatrix(data.map(lambda row:IndexedRow(monotonically_increasing_id(), Vectors(list(row)))\n# song_mat_temp = IndexedRowMatrix(song_vec_mod.rdd.map(lambda row: IndexedRow(row['id'], Vectors.dense(row[:9]))))\n\n# it will return an rdd of indexed row, need to extract \ndef extractRows(row):\n#     count = count+1\n    return (row.index, ) + tuple(row.vector.toArray().tolist())\n\n# save as dataframe \n# count = 0\n\ncolumnName = [str(i+1) for i in range(500)]\ncolumnName = ['id'] + columnName\nsim_df = simRdd.map(extractRows).toDF(columnName)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print(simThres.numCols(), simThres.numRows())", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 47, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "500 500\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### 6.3 Recommend for selected users ", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# filter users: listened to only one song before, but more than once\n\nselect_score= valid_score.createOrReplaceTempView('select_score')\ninactive_user = spark.sql(\"\"\"\n    select distinct uid, count(1) as cnt\n    from select_score\n    group by uid\n    having cnt = 1\n    order by 2 asc \n\"\"\")\n\ninactive_user = inactive_user.createOrReplaceTempView('inactive_user')\n\nselect_pos = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label > -1\n\"\"\")\n\nselect_neg = spark.sql(\"\"\"\n    select uid, song_id, freq, label \n    from select_score \n    where uid in (\n    select uid from inactive_user)\n    and label = -1\n\"\"\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 242, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "select_pos.join(song_freq, 'song_id').orderBy(song_freq.freq, ascending = False).show()", 
            "metadata": {}, 
            "execution_count": 134, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+---------+----+-----+---------+--------------------+-------+------+\n| song_id|      uid|freq|label|song_type|           song_name| singer|  freq|\n+--------+---------+----+-----+---------+--------------------+-------+------+\n|15249349|168965732| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168848103| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168597540| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168028168| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168826351|10.0| 10.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168258482| 4.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168916514| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167764401| 3.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167639000| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168149928| 0.0|  0.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168139018|24.0|  5.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167896511| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168930543| 2.0| 10.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168204433| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168335651| 4.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167900079| 9.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168364536| 5.0|  2.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|167972265| 6.0|  3.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168914581| 2.0|  1.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n|15249349|168906776|14.0|  4.0|        2|\u51c9\u51c9-(\u7535\u89c6\u5267\u300a\u4e09\u751f\u4e09\u4e16\u5341\u91cc\u6843\u82b1\u300b...|\u5f20\u78a7\u6668&\u6768\u5b97\u7eac|109798|\n+--------+---------+----+-----+---------+--------------------+-------+------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "select_pos.printSchema()", 
            "metadata": {}, 
            "execution_count": 57, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- uid: integer (nullable = true)\n |-- song_id: integer (nullable = true)\n |-- freq: double (nullable = true)\n |-- label: double (nullable = false)\n\n"
                }
            ]
        }, 
        {
            "source": "select_pos.count()\nselect_neg.count()", 
            "metadata": {}, 
            "execution_count": 55, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "49455"
                    }, 
                    "metadata": {}, 
                    "execution_count": 55, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "def recommendCF(model, user, nbRecommendations):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = song_unique.select(\"song_id\").distinct().withColumn(\"uid\", lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    songsAlreadyRated = valid_score.filter(valid_score.uid == user).select(\"song_id\", \"uid\")\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(songsAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"song_id\", \"prediction\")\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(song_unique, predictions.song_id == song_unique.song_id).select(predictions.song_id, song_unique.song_name, song_unique.singer, predictions.prediction).orderBy(\"prediction\", ascending=False)\n\n    recommendations.show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "import pandas as pd\nsim_pd = sim_df.toPandas()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 189, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# requires sim_df matrix similaries for all songs \n\ndef getSongSimilarity(song_index):\n    \"\"\"\n    param: song_index \n    return: similarity df of this song to all other songs \n    \"\"\"\n    df1 = sim_df.filter(col('id')==song_index).toPandas().transpose().drop(['id'])\n    df1.insert(0, 'id', range(1, 501))\n    df2 = sim_df.select(str(song_index), 'id').toPandas()\n    dfComb = df1.join(df2.set_index('id'), on='id')\n    dfComb['max'] = dfComb[[0, str(song_index)]].max(axis=1)\n    dfComb = dfComb.drop([0, str(song_index)], axis = 1)\n\n    song_df = spark.createDataFrame(dfComb)\n    return song_df", 
            "metadata": {}, 
            "execution_count": 308, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "def recommendSim(user, song, nbRecommendations):\n#     song = select_user.song_id\n#     get similarity score for this song \n    print ('User liked this song: ')\n#     song_freq.filter(col('song_id')==song).show()\n    \n#   get row index for that song \n    find_song = top_songs.filter(col('song_id')==song)\n\n#     if in the top 500 songs \n    if find_song:\n        song_index = find_song.select('id').collect()[0].id\n        song_sim = sim_df.select(str(song_index+1), 'id')\n        prediction = getSongSimilarity(song_index).orderBy('max', ascending=False)\n        \n        recommendations = predictions.join(top_songs, top_songs.id == predictions.id)\\\n            .select(top_songs.song_id, top_songs.song_name, top_songs.singer, predictions.max)\\\n            .orderBy(\"max\", ascending=False).limit(nbRecommendations)\n\n        recommendations.show(truncate=False)\n        \n        \n# else: fit into the matrix, add to dataframe, get a new similarity matrix, get last row\n    else:\n        top_songs.select('song_id', 'singer', 'song_type','freq').limit(nbRecommendation).show(truncate=False)\n        ", 
            "metadata": {}, 
            "execution_count": 305, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "def recommendSong(user, nbRecommendations):\n    pos_user = select_pos.filter(col('uid')==user)\n    neg_user = select_neg.filter(col('uid')==user)\n\n#     if shown more than one preference\n    if (not pos_user) and (not neg_user):\n        return recommendCF(model, user, nbRecommendations)\n    \n#     if shown only one preference \n    elif pos_user: \n        song = pos_user.select('song_id').collect()[0].song_id\n        return recommendSim(user, song, nbRecommendations)\n    \n#     if never liked any song from the platform, recommend top 10 songs\n    else:\n        top_songs.select('song_id', 'singer', 'song_type','freq').limit(nbRecommendation).show(truncate=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 120, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# # test on single user\n# user = 12333\nfavorite = valid_score.filter(col('uid') == 12333).join(top_songs, 'song_id').orderBy('label', ascending=False)\nfavorite.show(truncate=False)\n\n# get index of favorite song which is in top 500 list \n# future work: generate a new similarity matrix with the song not in top 500 list\nsong_index = favorite.select('id').limit(1).collect()[0]\n\nrecommendSim(user, song_index , 10)\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 311, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+------------------+---------+-----------------+------------------+\n|song_id |song_name         |song_type|singer           |max               |\n+--------+------------------+---------+-----------------+------------------+\n|157767  |\u7b49\u4f60\u7b49\u5230\u6211\u5fc3\u75db           |2        |\u5f20\u5b66\u53cb              |0.9985164466289682|\n|6477086 |\u5c0f\u6c34\u679c               |2        |\u7b77\u5b50\u5144\u5f1f             |0.9982372430988816|\n|7196022 |\u65f6\u5149\u7b14\u58a8-(\u7535\u89c6\u5267\u300a\u9752\u4e91\u5fd7\u300b\u7247\u5c3e\u66f2)|2        |\u5f20\u78a7\u6668              |0.997938413881416 |\n|23497506|\u753b\u5fc3(Live)          |2        |\u5f20\u9753\u9896              |0.9978128617846123|\n|7153193 |\u522b\u628a\u75bc\u4f60\u7684\u4eba\u5f04\u4e22\u4e86         |2        |\u96e8\u5b97\u6797              |0.9977944865013622|\n|7186112 |\u5c06\u519b\u4ee4               |2        |\u5434\u514b\u7fa4&\u62d6\u978b&SING\u7ec4\u5408&\u53f6\u6653\u7ca4|0.9976869503427881|\n|5746692 |Try               |2        |Colbie Caillat   |0.9976558123419578|\n|157908  |\u543b\u522b                |2        |\u5f20\u5b66\u53cb              |0.9975941893360246|\n|9919225 |\u6211\u8981\u4f60-(\u7535\u5f71\u300a\u9a74\u5f97\u6c34\u300b\u4e3b\u9898\u66f2)  |2        |\u4efb\u7d20\u6c50              |0.9974928271853185|\n|500897  |\u8fc7\u706b                |2        |\u5f20\u4fe1\u54f2              |0.9974926040740845|\n+--------+------------------+---------+-----------------+------------------+\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "##  Future work:\n- upon recommended songs, generate 100 songs per user - keep the song_id \n- try classification methods for fine tune to get top 20 \n- take into considerations of more features: song_name, singer, song_type\n- predict the class: 0 - 5 score (training with normalized frequency 1-5 and download automatically 5)\n- evalute both model's top 20 recommendation, rms error \n\nFor collaborative filter<br>\n- visualize the frequency distribution - try normalization ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "learning about distributed linear algebra \nchallenge: \n- matrix stored in distributed container, moving everything to memory is impossible?", 
            "cell_type": "markdown"
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}